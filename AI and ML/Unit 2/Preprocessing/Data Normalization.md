# Normalization

Normalizing the data is an important step before applying any learning algorithm. If the data isn't normalized, it could be positioned in the higher-dimension space in a way such that the Euclidian distance (or any other [distance metric](/AI and ML/Unit 2/Distance Metrics.md)) might not be a good metric.

Moreover, algorithms such as [k-NN](/AI and ML/Unit 2/Supervised Learning/Nearest Neighbour.md) have irregular and non-linear decision boundaries, which is a sign of overfitting. Normalization is applied to ensure smooth decision boundaries and reducing the risk of overfitting the data.

## Min-Max Normalization

*Min-Max* normalization aims to scale all the axes to fit the data in a range $[0,1]^{|\mathcal D|}$ , so that all features have equal importance.

The formula to scale a single axis to the range $[0,1]$ is the following.

$$\large
	x' = \frac{x - x_\min}{x_\max - x_\min}
$$

## Standard Normalization

A.k.a. *normal normalization*, the standard normalization supposes that the data is generated by a single gaussian and is reshaped in a way to have zero mean and unit variance on all axis.

The formula to standardize a single axis is the following.

$$\large
	x' = \frac{x - \mu}{\sigma}
$$

This type of normalization is equivalent to centring and decorrelating the features with [PCA](/AI and ML/Unit 2/Preprocessing/Principal Component Analysis.md).

## Feature Normalization

In reality, features carry different weights, meaning that some features are more important than other features. Some features could be even categorised as *irrelevant features*.

The are two options to normalize the features.

1. **Classify good and irrelevant features**

Assume that the distribution is composed of good features (useful to classification) and irrelevant features (useless to classification).

Let $\mathcal S_{gt}, \mathcal S_{ir}$ be two sets containing the indices of good features (ground truth) and irrelevant features. Then, the Euclidian metric can be defined in the following way.

$$\large
	d(x,v) = \sqrt{
		\sum_{i \in \mathcal S_{gt} } (x_i - v_i)^2 +
		\sum_{j \in \mathcal S_{ir} } (x_j - v_j)^2
	}
$$

Learning to recognize which are the irrelevant features and removing them could help increase the accuracy of the algorithm.

2. **Feature weighting**

The Euclidian distance treats all features equally, with the same importance, but each axis could carry a different meaning with a different importance (wrt to the others).

The Euclidian distance can be redefined as a weighted sum of the magnitude difference on all axis: let $w = [\seq w D]$ be a vector containing the weights for each axis, then the weighted Euclidian distance $d(\cdot, \cdot)$ is defined as follows.

$$\large
	d(x, v) = \sqrt{ \sum_{i=1}^D w_i (v_i - x_i)^2 }
$$

## Manifold

*TK*