{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Applied Computer Science and Artificial Intelligence","text":"<p>This is a collection of notes I work on after classes by myself, to set the concepts in my mind and to have a repository full of inter-linked information.</p> <p>Every class has a README file, to make the navigation easier. These files should link every note in the respective class in the order they were written, feel free to browse for yourself.</p> <p>Some of the courses </p>"},{"location":"#course-index","title":"Course Index","text":"<p>First Year - First Semester</p> <ul> <li>Calculus 1 - Unit 1</li> <li>Computer Architecture - Unit 1</li> <li>Linear Algebra</li> <li>Programming 1 - Unit 1</li> <li>Programming 1 - Unit 2</li> </ul> <p>First Year - Second Semester</p> <ul> <li>Algorithms</li> <li>Calculus 1 - Unit 2</li> <li>Computer Architecture - Unit 2</li> <li>Physics</li> <li>Programming 2</li> </ul> <p>Second Year - First Semester</p> <ul> <li>Calculus 2</li> <li>Data Management and Analysis - Unit 1</li> <li>Probability</li> <li>Systems and Networking - Unit 1</li> <li>Systems and Networking - Unit 2</li> </ul> <p>Second Year - Second Semester</p> <ul> <li>Artificial Intelligence and Machine Learning - Unit 1</li> <li>Artificial Intelligence and Machine Learning - Unit 2</li> <li>Computer Vision and Natural Language Processing</li> <li>Data Management and Analysis - Unit 2</li> <li>Statistics</li> </ul> <p>Other</p> <ul> <li>Workbench</li> <li>Latex Definitions</li> </ul>"},{"location":"#obsidian","title":"Obsidian","text":"<p>:warning: The whole repository is edited through Obsidian, a powerful note-taking software. Notes are written in markdown and  LaTeX; the best way to ensure full compatibility of the notes is cloning the repository and opening them through Obsidian.</p> <p>My end-goal is to have a net of information where I can link an argument from one class to another, hopefully something that will look like this.</p> <p></p>"},{"location":"#knowledge-graph","title":"Knowledge Graph","text":"<p>This is the state of the knowledge graph, updated on <code>march 26th, 2023</code>.</p> <ul> <li>The nodes are markdown notes</li> <li>The edges are links between notes</li> </ul> <p></p> <p>Every colour is a root folder, a list will follow.</p>"},{"location":"#used-plugins","title":"Used Plugins","text":"<p>A part of the <code>.obsidian</code> folder is uploaded in the repository, so plugins should be downloaded automatically. Regardless, here is a list of plugins I'm using to edit the notes.</p> Name Description Home Admonition Add callouts to notes GitHub Advanced Tables Advanced support for markdown tables GitHub Editor Syntax Highlight TK GitHub Emoji Toolbar TK GitHub File Tree Alternative Transform Obsidian's file explorer into a file tree GitHub Hider Hide some UI components GitHub Minimal Theme Settings Add more customization to Minimal Theme GitHub"},{"location":"Latex%20Definitions/","title":"Latex Definitions","text":"<p>A collection of latex macros I use</p> \\[\\large \\displaylines{     \\lceil \\text{Wrap} \\rfloor \\\\     \\lceil \\cdot \\rfloor } \\] <pre><code>$\\def \\wrap#1{{ \\lceil {#1} \\rfloor }}$\n</code></pre> \\[\\large \\displaylines{     \\text{[Probability]} \\\\     \\mathbb P(\\cdot) } \\] <pre><code>$\\def \\P#1{{ \\mathbb{P} \\left(#1\\right) }}$\n</code></pre> \\[\\large \\displaylines{     \\text{[Expectation]} \\\\     \\mathbb E(\\cdot) } \\] <pre><code>$\\def \\E#1{{ \\mathbb{E} \\left(#1\\right) }}$\n</code></pre> \\[\\large \\displaylines{     \\text{[Random Variable]} \\\\     \\mathbb X(\\cdot) } \\] <pre><code>$\\def \\X#1{{ \\mathbb{X} \\left(#1\\right) }}$\n</code></pre> \\[\\large \\displaylines{     \\text{[Variance]} \\\\     \\text{Var}(\\cdot) } \\] <pre><code>$\\def \\Var#1{{ \\text{Var} \\left(#1\\right) }}$\n</code></pre> \\[\\large \\displaylines{     \\text{[Covariance]} \\\\     \\text{Cov}(\\cdot) } \\] <pre><code>$\\def \\Cov#1{{ \\text{Cov} \\left(#1\\right) }}$\n</code></pre> \\[\\large \\displaylines{     \\text{[Indicator]} \\\\     \\mathbb{1}_{A} (\\cdot) } \\] <pre><code>$\\def \\Ind#1#2{{ \\mathbb{1}_{#1} \\left( {#2} \\right) }}$\n</code></pre> \\[\\large \\displaylines{     \\text{[Bold]} \\\\     \\mathbb{A,B,C} } \\] <pre><code>$\\def \\bb#1{{ \\mathbb{#1} }}$\n</code></pre> \\[\\large \\displaylines{     \\text{[Calligraphy]} \\\\     \\mathcal{A,B,C} } \\] <pre><code>$\\def \\cal#1{{ \\mathcal{#1} }}$\n</code></pre> \\[\\large \\displaylines{     \\text{[Normal Distribution]} \\\\     \\mathcal N (\\cdot, \\cdot) } \\] <pre><code>$\\def \\ND#1#2{{ \\mathcal N \\left( {#1},{#2} \\right) }}$\n</code></pre> \\[\\large \\displaylines{     \\text{[Sequence]} \\\\      A_1, A_2, \\ldots, A_B } \\] <pre><code>$\\def \\seq#1#2{{ {#1}_1, {#1}_2, \\ldots, {#1}_{#2} }}$\n</code></pre> \\[\\large \\displaylines{     \\text{[Sequence - Functions]} \\\\      A_1(B), A_2(B), \\ldots, A_C(B) } \\] <pre><code>$\\def \\seqf#1#2#3{{ {#1}_1({#2}_1), {#1}_2({#2}_2), \\ldots, {#1}_{#3}({#2}_{#3}) }}$\n</code></pre> \\[\\large \\displaylines{     \\text{[Sequence - w/Operator]} \\\\      B_1 \\odot B_2 \\odot \\ldots \\odot B_C } \\] <pre><code>$\\def \\seqop#1#2#3{{ {#2}_1 #1 {#2}_2 #1 \\cdots #1 {#2}_{#3} }}$\n</code></pre> \\[\\large \\displaylines{     \\text{[Independence]} \\\\     \\unicode{x2AEB} } \\] <pre><code>$\\def \\indep{{ \\mathrel\\unicode{x2AEB} }}$\n</code></pre> \\[\\large \\displaylines{     \\text{[Propositional Logic Model]} \\\\     \\mathcal M(\\cdot) } \\] <pre><code>$\\def \\M#1{{ \\mathcal M({#1}) }}$\n</code></pre>"},{"location":"TODO/","title":"TODO","text":""},{"location":"TODO/#systems-networking-u1","title":"Systems &amp; Networking U1","text":"<ul> <li> Machine Language (Lecture 3, p~25)</li> <li> Direct Memory Access (Lecture 3, P91)</li> <li> Atomic Instructions (Lecture 4, p188)</li> <li> Process Communication (Lecture 6, p100)</li> <li> Estimating CPU Time of a Job (Lecture 8, p17)</li> </ul>"},{"location":"Workbench/","title":"Workbench","text":"<p>Just a big TODO list of notes</p> <p></p>"},{"location":"Workbench/#ai-ml-u1","title":"AI ML U1","text":""},{"location":"Workbench/#agents","title":"Agents","text":"<p>Goal based agent</p> <p>Consequentialism</p> <p>Instead of storing all possible percepts, the best solution is to start from a subset of it and then include others while learning.</p>"},{"location":"Workbench/#automata","title":"Automata","text":"<ul> <li> <p> Finite-State LTS means that it has finite states and actions, Finite-State Automata means that it has to end at some point.</p> </li> <li> <p> Every non-deterministic automata can be represented as a deterministic automata, even though the states and actions may increase.</p> </li> <li> <p> An FSA is deterministic if the outcome of an action is determined, non-deterministic if the outcome isn't determined (i.e. deterministic if for each state every action leads to one state only, non-deterministic otherwise).</p> </li> </ul>"},{"location":"Workbench/#searching","title":"Searching","text":"<ul> <li> <p> Graph search vs tree-like search</p> </li> <li> <p> Breadth-first search is complete (if there's a solution it will return it, otherwise it won't)</p> </li> </ul>"},{"location":"Workbench/#logic-based-agents","title":"Logic-based Agents","text":"<ul> <li> Entailment / Models</li> </ul>"},{"location":"Workbench/#ai-ml-u2","title":"AI ML U2","text":""},{"location":"Workbench/#training","title":"Training","text":"<p>A model can't be verified on the same dataset it has been trained on, because the model is kind of guaranteed to reach the expected output. To verify the correctness of a model, a dataset with different data is required, so that the model will try to guess the output on data it has never seen.</p>"},{"location":"Workbench/#pearson-correlation-coefficient","title":"Pearson Correlation Coefficient","text":"<p>The formula calculates the arithmetic means of both x and y, then calculates how much the set of the points change wrt to the mean point. Then, it sums all contributions to calculate the correlation between x and y. </p>"},{"location":"Workbench/#inductive-bias","title":"Inductive Bias","text":"<p>Different people (and different AI's) have different experiences, which influences the resulting reasoning.</p>"},{"location":"Workbench/#pca","title":"PCA","text":"<ul> <li> <p> Reconstruction Error</p> </li> <li> <p> Another PCA interpretation: find an orthogonal projection that minimises the reconstruction error</p> </li> <li> <p> What if the sample cardinality is smaller than the data dimensionality</p> </li> <li> <p> There is a way to retrieve the Principal Components from SVD</p> </li> </ul>"},{"location":"Workbench/#matrix-calculus","title":"Matrix Calculus","text":"<ul> <li> <p> First derivative of vector to scalar: Gradient (Dx1)</p> </li> <li> <p> Second derivative of vector to scalar: Hessian (DxD, symmetric)</p> </li> <li> <p> First derivative of vector to vector (D-&gt;P): Jacobian (DxP)</p> </li> <li> <p> Second derivative of vector to vector (D-&gt;P): High-order Tensor (DxPxP)</p> </li> <li> <p> The hessian can be seen as the jacobian of the gradient</p> </li> <li> <p> Quadratic Forms: \\(x^T A x\\) (\\(A\\) symmetric) | If we call \\(Ax = b\\), then \\(x^T b\\) is the dot product between \\(x\\) and itself after \\(A\\) has been applied. Geometrically, it represents the shape of the parabola of the matrix</p> </li> <li> <p> Quadratic form can be applied to asymmetric square matrices too: let \\(B = \\frac{1}{2} A + \\frac{1}{2} A^T\\), then \\(B\\) is symmetric and the quadratic form can be applied to \\(B\\). </p> </li> <li> <p> Gradient is linear</p> </li> <li> <p> Lagrange multiplier</p> </li> </ul>"},{"location":"Workbench/#k-mean","title":"K-mean","text":"<ul> <li> Furthest-first Heuristic: first is picked at random, others are picked iteratively based on decreasing distance (updated on each assignment)</li> </ul>"},{"location":"Workbench/#supervised-learning","title":"Supervised Learning","text":"<ul> <li> Empirical Risk Minimization</li> <li> Bias error</li> <li> Bias-Variance tradeoff (dartboard)</li> <li> Feature weighting (choosing dominant vs superficial features)</li> </ul>"},{"location":"Workbench/#k-nn","title":"K-NN","text":"<ul> <li> <p> Advantages and disadvantages</p> </li> <li> <p> Split the data into training (60%), validation (20%), test (20%)</p> </li> <li> <p> The validation data is used to select or fix the hyper-parameter \\(k\\)</p> </li> <li> <p> The training loss function can be defined by classifying the training set itself given a parameter \\(k\\)</p> </li> <li> <p> Generally, the increasing order of errors should be train, validation, test</p> </li> <li> <p> Remember to estimate scaling / regularizing the data only with the training set, and then apply it to validation / test data too</p> </li> </ul>"},{"location":"Workbench/#decision-trees","title":"Decision Trees","text":"<ul> <li> <p> Supervised w/parameters</p> </li> <li> <p> \\(p_k = \\frac{|S_k|}{|S|}\\) probability of picking one label</p> </li> <li> <p> impurity: misclassification in the data during the mode decision (1 - probab of picking the label picked) \\(1 - \\max_{k \\in Y}{p_k}\\)</p> </li> <li> <p> Gini impurity: sort of an entropy of impurities, it's computed as the sum of (prob picking) (prob not picking)</p> </li> <li> <p> Geany Imputiry function: \\(H(S) = \\sum_{k \\in Y} p_k (1 - p_k)\\)</p> </li> <li> <p> Etropy function: \\(- \\sum_{k \\in Y} p_k \\log_2()\\)</p> </li> <li> <p> Entropy is derived from impurity: impurity is a general concept, entropy is impurity implemented</p> </li> <li> <p> Impurity of a tree: recursive weighted sum of the entropy of both sides of the current branch of the tree.</p> </li> </ul> <p></p>"},{"location":"Workbench/#computer-vision-and-nlp","title":"Computer Vision and NLP","text":"<ul> <li> CV images are stored in standard NumPy arrays</li> </ul>"},{"location":"Workbench/#statistics","title":"Statistics","text":"<p>Forchetta: the range of values a variable could assume.</p> <p>sample vs observation</p> <p>Data are a random sample of characteristics </p> <p>Unit profile: connection between an individual and a unit group</p> <p>Profile of a specific unit: combination of all values for a ?</p> <p>Box plot</p> <p>Response Variable Explanatory Variable</p> <p></p>"},{"location":"Workbench/#data-management-and-analysis-u2","title":"Data Management and Analysis U2","text":"<p>Empty, for now...</p> <p></p>"},{"location":"AI%20and%20ML/","title":"Artificial Intelligence and Machine Learning","text":"<p>The classes I've signed up for: AI and machine learning.</p> <p></p>"},{"location":"AI%20and%20ML/#table-of-contents","title":"Table of Contents","text":""},{"location":"AI%20and%20ML/#unit-1","title":"Unit 1","text":"<p>Agents</p> <ul> <li> <p>Agent</p> </li> <li> <p>Task Environment</p> </li> <li> <p>Agent Models</p> </li> </ul> <p>Propositional Logic</p> <ul> <li> <p>Logical Operations</p> </li> <li> <p>Entailment</p> </li> <li> <p>Deductive System</p> </li> </ul>"},{"location":"AI%20and%20ML/#unit-2","title":"Unit 2","text":"<p>Machine Learning</p> <ul> <li>Principal Component Analysis</li> </ul> <p>Unsupervised Learning</p> <ul> <li> <p>Clustering</p> </li> <li> <p>K-means Clustering</p> </li> <li> <p>Gaussian Mixture Model</p> </li> </ul> <p>Supervised Learning</p> <ul> <li>Bayes Optimal Classifier</li> </ul>"},{"location":"AI%20and%20ML/Unit%201/Agents/Agent%20Models/","title":"Agent Models","text":"<p>TK</p>"},{"location":"AI%20and%20ML/Unit%201/Agents/Agent%20Models/#table-driven-agent","title":"Table-Driven Agent","text":"<p>A table-driven agent is represented by a sequence of percepts and an action assigned to every sequence. A pre-set table maps an action to every possible sequence and the agents acts solely on this table:</p> <ul> <li>the sensors read a percept,</li> <li>the action corresponding to the sequence of past percepts (including the one just read) is looked up,</li> <li>the action is performed through the actuators.</li> </ul> <p>Python representation of a table-driven agent.</p> <pre><code># Sequence of read percepts\npercepts: tuple\n# [Percepts sequence -&gt; action] mapping\ntable: dict\n\n# Agent function\ndef table_driven_agent(percept) -&gt; action:\n    append(percepts, percept)\n    action = table[percepts]\n    return action\n</code></pre> <p>[!failure] Table Size</p> <p>The table containing every possible sequence of percepts can't possibly be computed. The number of possible entries is given by</p> \\[\\large   \\sum_{t=1}^T {| \\mathcal P |}^t \\] <p>where \\(T\\) is the agent lifetime.</p>"},{"location":"AI%20and%20ML/Unit%201/Agents/Agent%20Models/#simple-reflex-agent","title":"Simple Reflex Agent","text":"<p>A simple reflex agent is an agent that operates on condition rules. The sensors read percepts which are used to understand the current state of the world, then rules are used to determine an action the actuators have to execute.</p> <p>[!info] Stateless Agent</p> <p>The Simple Reflex Agent is a stateless agent, in the sense that the agent decides on an action by rules which are applied to the current state of the environment, without keeping track of the past.</p> <p></p> <p>Python representation of a simple reflex agent.</p> <pre><code># A structure containing the [condition -&gt; action] rules\nrules: any\n\n# Agent function\ndef simple_reflex_agent(percept) -&gt; action:\n    state = interpret_input(percept)\n    rule = rule_match(state, rules)\n    return rule.action\n</code></pre>"},{"location":"AI%20and%20ML/Unit%201/Agents/Agent%20Models/#model-based-reflex-agent","title":"Model-Based Reflex Agent","text":"<p>The model-based reflex agent is similar to the simple reflex one. It reasons with rules too, but it keeps track of the previous states and actions which help decide in a resulting action.</p> <p>A model-based reflex agent requires:</p> <ul> <li>transition model, a description of how the the next state depends on the current state and action (e.g. automa);</li> <li>sensor model, a description of how the current world state is reflected in the agent's percepts.</li> </ul> <p>[!info] Stateful Agent</p> <p>The Model-Based Reflex Agent is a stateful agent, in the sense that the agent decides on an action based on the current state, which is related to information of past states and actions</p> <p></p> <p>Python representation of a model-based reflex agent.</p> <pre><code># A structure containing the [condition -&gt; action] rules\nrules: any\n# State transition descriptor\ntransition_model: any\n# [Percepts -&gt; environment state] descriptor\nsensor_model: any\n# Current state of the environment\n# (known by the agent)\ncurrent_state: state\n# Most recent action\nlast_action: action\n\n# Agent function\ndef model_reflex_agent(percept) -&gt; action:\n    current_state = update_state(current_state, last_action, percept, transition_model, sensor_model)\n    rule = rule_match(current_state, rules)\n    last_action = rule.action\n    return last_action\n</code></pre>"},{"location":"AI%20and%20ML/Unit%201/Agents/Agent%20Models/#goal-based-agent","title":"Goal-Based Agent","text":"<p>TK</p> <p></p>"},{"location":"AI%20and%20ML/Unit%201/Agents/Agent%20Models/#utility-based-agent","title":"Utility-Based Agent","text":"<p>TK</p> <p></p>"},{"location":"AI%20and%20ML/Unit%201/Agents/Agent%20Models/#learning-agent","title":"Learning Agent","text":"<p>TK</p> <p></p>"},{"location":"AI%20and%20ML/Unit%201/Agents/Agent/","title":"Agent","text":"<p>An agent is any automated thing that interacts dynamically with an environment.</p> <p></p> <p>An agent is composed of - sensors, which are used to read from the environment (the possible readings are called percepts); - some kind of logic, which turns the percepts into actions (by decision making); - actuators, which are used to perform the actions and interact with the environment.</p> <p>[!example] Agents</p> <p>Example of agents are web browsers, robots, automatic vacuum cleaners.</p> <p>The rigorous definition of an agent is the following,</p> \\[\\large     f : \\mathcal P^* \\nrightarrow \\mathcal A, \\] <p>which basically means that an agent is a partial function (maps only a subset of the whole domain) that partially maps the closure of percepts to a set of actions.</p> <p>The agent function is integral part of the agent diagram: percepts are fed in by the sensors and actions are executed by the actuators.</p>"},{"location":"AI%20and%20ML/Unit%201/Agents/Agent/#goal-based-agent","title":"Goal Based Agent","text":"<p>An agent can be designed to act not towards a final goal, but towards getting the most performance out of every step. This is called a goal-based agent, and the criteria which decides the rankings of performance is called performance measure.</p> <p>Performance measures should be built according to what the agent should achieve, and not how it should be achieved.</p> <p>[!example] Chess Analogy</p> <p>A good goal-based chess agent's performance measure could be composed of abstract strategy points. Some moves may give more strategy points than others, and better yet if the performance measure can give even more strategy points to moves that will be useful in the future. This way, the agent's goal is to win the chess game by maximising the awarded strategy points.</p> <p>A bad agent, for example, will always try to eat every piece or to check the king. Doing this isn't the best way to win a chess game.</p>"},{"location":"AI%20and%20ML/Unit%201/Agents/Agent/#peas","title":"PEAS","text":"<p>PEAS is an acronym that indicates the base structure of every goal-based agent. It stands for:</p> <ul> <li>Performance measure</li> <li>Environment</li> <li>Actuators</li> <li>Sensors</li> </ul> <p>[!example] pac-man</p> <p>The pac-man ghosts are agents with the following PEAS.</p> <ul> <li>Performance Measure: could be the distance from pac-man or how close it is from catching it (by some sort of criteria)</li> <li>Environment: the maze</li> <li>Actuators: whatever moves the ghost</li> <li>Sensors: whatever it uses to understand theirs and pac-man's position in the maze and if pac-man is energized</li> </ul>"},{"location":"AI%20and%20ML/Unit%201/Agents/Task%20Environment/","title":"Task Environment","text":"<p>The task environment is the environment that an agent interacts with.</p>"},{"location":"AI%20and%20ML/Unit%201/Agents/Task%20Environment/#characteristics-of-an-environment","title":"Characteristics of an Environment","text":"<p>Various characteristics define an environment, a good agent is developed starting from a correct analysis of the environment.</p>"},{"location":"AI%20and%20ML/Unit%201/Agents/Task%20Environment/#observability","title":"Observability","text":"<ul> <li>Fully observable, the state of the environment can be fully known;</li> <li>Partially observable. the state of only a part the environment can be known;</li> <li>Unobservable, the state of the environment can't be known at all.</li> </ul> <p>The observability of an environment doesn't have to be static and can change.</p> <p>[!example] Real world and a camera</p> <p>A machine can observe the real world (environment) via a camera (sensors): - during the day the environment is partially observable; - during the night the environment is unobservable.</p>"},{"location":"AI%20and%20ML/Unit%201/Agents/Task%20Environment/#multiplicity","title":"Multiplicity","text":"<p>An environment that hosts just a single agent is called a single-agent environment, an environment that hosts multiple agents at the same time is called a multi-agent environment.</p> <p>In a multi-agent environment, the agents can either be competitive or cooperative.</p>"},{"location":"AI%20and%20ML/Unit%201/Agents/Task%20Environment/#stochasticity","title":"Stochasticity","text":"<p>Multiple events can happen in an environment, the stochasticity of an environment is an indicator of how much is known of the probability of the events happening.</p> <ul> <li>Deterministic, actions in a deterministic environment have a certain outcome, it is possible to correctly predict how the environment will evolve in time;</li> <li>Non-deterministic, actions in a non-deterministic environment have outcomes with known probabilities, it may be possible to predict an expectation of how the environment will evolve in time;</li> <li>Stochastic, actions in a stochastic environment have outcomes with unknown probabilities, it is impossible to predict how the environment will evolve in time.</li> </ul> <p>[!example] Pac-man's Maze</p> <p>Pac-man's maze can be both deterministic and non-deterministic, based on the version of the game.</p> <ul> <li>In the first version, there are only points on the ground which will add up to the score and nothing more. This environment is deterministic, because the agent knows how it will evolve (it won't evolve at all).</li> <li>In further versions, fruits may pop up randomly in the maze which will add even more to the score. This environment is non-deterministic, stochastic even, because there's a random component such that the agent won't be able to predict the next state of the environment.</li> </ul>"},{"location":"AI%20and%20ML/Unit%201/Agents/Task%20Environment/#episodic-or-sequential","title":"Episodic or Sequential","text":"<p>An episodic environment is an environment such that the agent's actions have independent consequences, i.e. one action won't affect the next one.</p> <p>[!example] Classification</p> <p>Many classification types are episodic, because classifying a percept doesn't influence the classification of the next percept.</p> <p>A sequential environment is an environment such an agent's action may affect the next one.</p> <p>[!example] Chess</p> <p>A chess board is a sequential environment: single actions have long-term consequences.</p>"},{"location":"AI%20and%20ML/Unit%201/Agents/Task%20Environment/#dynamicity","title":"Dynamicity","text":"<ul> <li>Static, a static environment won't change with time (chess);</li> <li>Dynamic, a dynamic environment will change with time (driving);</li> <li>Semidynamic, a semidynamic environment won't change with time, but the agent's score will (e.g. timed chess).</li> </ul>"},{"location":"AI%20and%20ML/Unit%201/Agents/Task%20Environment/#distrete-or-continuous","title":"Distrete or Continuous","text":"<p>TK</p>"},{"location":"AI%20and%20ML/Unit%201/Agents/Task%20Environment/#actions-effect","title":"Action's Effect","text":"<p>TK (Known / Unknown)</p>"},{"location":"AI%20and%20ML/Unit%201/Constraint%20Satisfaction%20Problem/Backtracking/","title":"Backtracking","text":"<p>Backtracking is the most used algorithm for solving CSP. It is an efficient method for systematically exploring the search space of possible solutions and finding a valid assignment of values to the variables.</p> <p>The backtracking algorithm follows a depth-first search approach, systematically trying different values for the variables and backtracking whenever a partial assignment violates a constraint. It explores the search space by recursively traversing through the variable assignments, making choices at each step.</p> <p>Here's a python (with pseudo-types) overview of the backtracking algorithm.</p> <pre><code># Backtracking:\n# - either return a valid solution (list of values)\n# - or failure\ndef backtracking_search(csp) -&gt; list[val] | None:\n    return backtrack\n\n# Recursive backtracking\ndef backtrack(\n        assignment: dict[var, val],\n        csp: tuple[dict[var, dom], list[con]]\n):\n    # Check if assignment is complete\n    if is_complete(assignment, csp):\n        return assignment\n\n    # Select an unassigned variable\n    variable = select_unassigned_variable(assignment, csp)\n\n    # Iterate through the domain values of the variable\n    for value in order_domain_values(variable, assignment, csp):\n        # Check if the value satisfies the constraints\n        if is_consistent(value, variable, assignment, csp):\n            # Assign the value to the variable\n            assignment[variable] = value\n\n            # Recursive call to continue the search\n            result = backtrack(assignment, csp)\n\n            # Check if a valid assignment is found\n            if result is not None:\n                return result\n\n            # If no valid assignment found,\n            # remove the value from assignment\n            del assignment[variable]\n\n    # End of consistent branch\n    return None\n</code></pre> <p>Backtracking allows for an efficient exploration of the solution space by pruning branches that are guaranteed to lead to invalid solutions. It leverages the concept of depth-first search combined with backtracking to find a valid assignment while minimizing the number of assignments and constraint checks required.</p> <p>Backtracking can be enhanced with various optimization techniques, such as variable and value ordering heuristics, constraint propagation, and forward checking, to improve the efficiency of finding a valid solution.</p>"},{"location":"AI%20and%20ML/Unit%201/Constraint%20Satisfaction%20Problem/Backtracking/#forward-checking","title":"Forward Checking","text":""},{"location":"AI%20and%20ML/Unit%201/Constraint%20Satisfaction%20Problem/Consistency/","title":"Consistency","text":"<p>Consistency is an important concept in CSP that refers to the property of assignments satisfying the constraints imposed by the problem. It ensures that the assigned values adhere to the defined relationships and constraints among the variables.</p> <p>In a CSP, consistency can be evaluated at different levels:</p> <ol> <li>Node consistency, involving one node;</li> <li>Arc consistency, involving two nodes;</li> <li>Path consistency, involving three nodes;</li> <li>\\(k\\)-consistency, which involves \\(k\\) nodes.</li> </ol> <p>[!note] \\(k\\)-consistency</p> <p>\\(k\\)-consistency is a generalization of consistency on \\(k\\) nodes, all the previous consistency properties are specific cases of \\(k\\)-consistency.</p> <p>By enforcing consistency in a CSP, we reduce the search space and improve the efficiency of finding valid solutions. It allows us to make informed decisions during the search process, prune inconsistent assignments early, and guide the search towards more promising areas of the solution space.</p> <p>Various algorithms, such as backtracking with constraint propagation or arc consistency algorithms like AC-3, can be employed to enforce consistency and solve CSPs efficiently. The choice of consistency level depends on the complexity of the problem and the desired trade-off between computational cost and solution optimality.</p>"},{"location":"AI%20and%20ML/Unit%201/Constraint%20Satisfaction%20Problem/Consistency/#node-consistency","title":"Node Consistency","text":"<p>Node consistency ensures that the values assigned to a single variable satisfy the constraints associated with that variable. In other words, it ensures that the assigned value is consistent with the unary constraints specific to that variable.</p>"},{"location":"AI%20and%20ML/Unit%201/Constraint%20Satisfaction%20Problem/Consistency/#arc-consistency","title":"Arc Consistency","text":"<p>Arc consistency extends node consistency to consider binary constraints between pairs of variables. It ensures that for every pair of variables connected by a constraint, each value in the domain of one variable has a compatible value in the domain of the other variable. By enforcing arc consistency, we reduce the search space and eliminate inconsistent assignments early on.</p> <p>Given two variables \\(X_i, X_j \\in \\mathcal X\\), \\(X_i\\) is said to be arc-consistent w.r.t \\(X_j\\) iff</p> <ol> <li>for every value in \\(D_i\\)</li> <li>there exists some value in \\(D_j\\)</li> <li>such that all binary constraints on \\((X_i, X_j)\\) are satisfied.</li> </ol> <p>A variable \\(X_i \\in \\mathcal X\\) is said to be arc-consistent iff</p> <ol> <li>for every binary constraint \\(C \\in \\mathcal C\\) with space \\((X_i, X_j)\\)</li> <li>and every value in \\(D_i\\)</li> <li>there exists some value in \\(D_j\\)</li> <li>such that \\(C\\) is satisfied.</li> </ol> <p>Looking at another perspective, it could also be said that a variable is said to be arc-consistent if is it is arc-consistent w.r.t. every variable.</p> <p>Python implementation of the AC-3 algorithm, that checks whether a node is arc-consistent.</p> <pre><code>def ac3(csp):\n    queue = []\n    for arc in csp.get_all_arcs():\n        queue.append(arc)\n\n    while queue:\n        arc = queue.pop(0)\n        if revise(csp, arc):\n            if len(csp.get_domain(arc[0])) == 0:\n                # Inconsistent assignment\n                return False\n\n            # Add neighboring arcs to the queue\n            for neighbor in csp.get_neighbors(arc[0]):\n                if neighbor != arc[1]:\n                    queue.append((neighbor, arc[0]))\n\n    # Consistent assignment\n    return True\n\n\ndef revise(csp, arc):\n    revised = False\n    for value in csp.get_domain(arc[0]):\n        if not any(satisfies_constraint(value, assignment[arc[1]]) for assignment in csp.get_consistent_assignments(arc)):\n            csp.remove_value_from_domain(arc[0], value)\n            revised = True\n\n    return revised\n</code></pre>"},{"location":"AI%20and%20ML/Unit%201/Constraint%20Satisfaction%20Problem/Consistency/#path-consistency","title":"Path Consistency","text":"<p>Path consistency goes beyond arc consistency and considers the transitive closure of binary constraints. It ensures that if a constraint exists between variable A and B, and a constraint exists between variable B and C, then there must exist a consistent assignment for the variables A, B, and C. Path consistency is more powerful than arc consistency and helps in pruning inconsistent assignments.</p> <p>Given three variables \\(X_i, X_j, X_k \\in \\mathcal X\\), \\(X_i\\) and \\(X_k\\) are said to be path-consistent w.r.t \\(X_j\\) iff</p> <ol> <li>for every assignment \\(\\set{X_i = v_i, X_k = v_k}\\) with \\((v_i, v_k) \\in D_i \\times D_k\\)</li> <li>that is consistent with every binary constraint with space \\((X_i, X_k)\\)</li> <li>there exists some assignment \\(X_j = v_j\\) with \\(v_j \\in D_j\\)</li> <li>such that all binary constraints on \\((X_i, X_j)\\) and \\((X_k, X_j)\\) are satisfied.</li> </ol>"},{"location":"AI%20and%20ML/Unit%201/Constraint%20Satisfaction%20Problem/Consistency/#global-consistency-also-known-as-constraint-propagation","title":"Global Consistency (also known as Constraint Propagation)","text":"<p>Global consistency refers to a higher level of consistency that considers all the constraints in the problem. It ensures that the assigned values satisfy all the constraints simultaneously. Achieving global consistency typically involves using algorithms such as constraint propagation, forward checking, and intelligent variable and value ordering techniques.</p>"},{"location":"AI%20and%20ML/Unit%201/Constraint%20Satisfaction%20Problem/Constraint%20Satisfaction%20Problem/","title":"Constraint Satisfaction Problem","text":"<p>\\(\\def \\seq#1#2{{ {#1}_1, {#1}_2, \\ldots, {#1}_{#2} }}\\)</p>"},{"location":"AI%20and%20ML/Unit%201/Constraint%20Satisfaction%20Problem/Constraint%20Satisfaction%20Problem/#constraint-satisfaction-problem","title":"Constraint Satisfaction Problem","text":"<p>A Constraint Satisfaction Problem (CSP) is a computational problem defined by variables, their domains, and some constraints that must the variables must satisfy. Solving a CSP involves finding values for the variables that satisfy all the given constraints.</p> <p>A CSP can be defined as a tuple with three components: variables, domains, constraints.</p> \\[\\large     \\text{csp} = (\\mathcal X, D, \\mathcal C) \\] <p>Solving a constraint satisfaction problem typically involves using algorithms and techniques, such as backtracking, constraint propagation and search heuristics to efficiently explore the search space and find a valid solution or determine that no solution exists.</p>"},{"location":"AI%20and%20ML/Unit%201/Constraint%20Satisfaction%20Problem/Constraint%20Satisfaction%20Problem/#variable","title":"Variable","text":"<p>Variables represent the unknowns or decision variables in the problem, each variable has a specific role and represents an aspect of the problem that needs to be determined or assigned a value.</p> <p>[!example] Sudoku</p> <p>In sudoku, a variable is a single cell that contains a number (both solved and unsolved).</p> <p>A variable \\(X_i\\) is a symbol which can take value \\(v_i \\in D_i\\), \\(D_i\\) being the set of possible values that \\(v_i\\) can take.</p> \\[\\large     \\mathcal X := \\set{\\seq X n} \\]"},{"location":"AI%20and%20ML/Unit%201/Constraint%20Satisfaction%20Problem/Constraint%20Satisfaction%20Problem/#domain","title":"Domain","text":"<p>A domain defines the allowable values for a variable, which can either be discrete or continuous (both non-dense and dense).</p> \\[\\large     D := \\set{\\seq D n} \\] <p>Domains don't need to be homogeneous between themselves and aren't limited to numerical values only, but they could also be sets of symbols.</p> <p>[!example] Sudoku</p> <p>In sudoku, the domains of all variables (the cells) are the same, i.e. \\(\\set {1,2,\\ldots,9}\\).</p>"},{"location":"AI%20and%20ML/Unit%201/Constraint%20Satisfaction%20Problem/Constraint%20Satisfaction%20Problem/#constraints","title":"Constraints","text":"<p>Constraints represent the restrictions or relationships among the variables. They define the conditions that must be satisfied for a valid solution of the problem.</p> \\[\\large     \\mathcal C := \\set{\\seq C m} \\] <p>Constraints can be unary (involving a single variable), binary (relating two variables), or higher-order (involving more than two variables). A constraint \\(C_k\\) is defined as follows.</p> \\[\\large \\begin{aligned}     C_k     &amp;= (Y_k, R_k) \\\\     &amp;= \\langle         \\set{X_1, X_2},         \\set{(3,1), (3,2), (2,1)}     \\rangle \\\\     &amp;= \\langle         \\set{X_1, X_2},         \\set{X_1 &gt; X_2}     \\rangle \\end{aligned} \\] <ul> <li> <p>\\(Y_k \\subseteq \\mathcal X\\) is the set that contains the attributes involved in the constraint, it is called scope; </p> </li> <li> <p>\\(R_k \\subseteq \\bigtimes_{D_i \\in Y_k} D_i\\) is the set of all tuples of values that attributes in \\(Y_k\\) can assume at the same time, it is called relation. It is the actual constraint that defines the problem to solve.</p> </li> </ul> <p>Constraints can be:</p> <ul> <li>unary, involving a single variable;</li> <li>binary, relating two variables;</li> <li>global, or higher-order, involving more than two variables.</li> </ul> <p>[!example] Sudoku</p> <p>In sudoku, the most obvious constraints are: - all cells in a row must be different; - all cells in a column must be different; - all cells in a box must be different.</p> <p>If a CSP contains no global constraints (i.e. unary and binary only), then it is called a binary CSP. Some global constraints can be decomposed into multiple binary ones, but not all of them.</p> <p>[!example] Sudoku</p> <p>Apparently, the main constraints of a sudoku may seem global (involving 9 variables), but they can be decomposed into multiple binary constraints: every combination of two cells in a row, column or box must be different.</p> <p>Another constraint may be the already initialized cells: we can either restrict their domain to respective value or put a constraint on the variable that they must be equal to their value (latter is a unary constraint).</p> <p>If considering these constraints only, then a sudoku may be considered a binary CSP.</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Deductive%20System/","title":"Deductive System","text":"<p>\\(\\def \\seq#1#2{{ {#1}_1, {#1}_2, \\ldots, {#1}_{#2} }}\\)</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Deductive%20System/#deductive-system","title":"Deductive System","text":"<p>A deductive system is a procedure through which it is possible to derive, infer or deduce sentences (a.k.a. conclusions) by iteratively applying rules on other sentences (premises).</p> <p>A deductive system \\(\\mathcal D\\) is composed by a set of sentences (a.k.a. axioms) and a set of inference rules.</p> <p>In a given deductive system \\(\\mathcal D\\), If a set \\(\\Gamma = \\seq \\phi n\\) of premises can deduce a conclusion \\(\\psi\\), it is said that \\(\\psi\\) is a deductive consequence of \\(\\Gamma\\), denoted by the following.</p> \\[\\large     \\Gamma \\vdash_{\\mathcal D} \\psi \\] <p>Deductive consequence can be checked through propositional logic resolution.</p> <p>[!note] Soundness</p> <p>A deductive system is said to be sound if, for any deductive consequence, an entailment with the same operands holds.</p> \\[\\large   (\\Gamma \\vdash_{\\mathcal D} \\psi)   \\Longrightarrow   (\\Gamma \\vDash \\psi)   \\quad \\forall \\,\\Gamma \\vdash_{\\mathcal D} \\psi \\] <p>By design, every deductive system should be sound.</p> <p>[!note] Completeness</p> <p>A deductive system is said to be complete if, for any entailment in the propositional model, a deductive consequence with the same operands holds.</p> \\[\\large   (\\Gamma \\vDash \\psi)   \\Longrightarrow   (\\Gamma \\vdash_{\\mathcal D} \\psi)   \\quad \\forall \\,\\Gamma \\vDash \\psi \\] <p>A deductive system doesn't necessarily need to be complete, but it's a good property to have.</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Deductive%20System/#inference-rules","title":"Inference rules","text":"<p>An inference rule is a schema in the following form,</p> \\[\\large     \\frac{\\seq \\phi n}{\\psi}     \\quad \\text{or} \\quad     \\frac \\Gamma \\psi \\] <p>where \\(\\Gamma = \\seq \\phi n\\) are the premises and \\(\\psi\\) is the conclusion.</p> <p>Any equivalence \\(\\Gamma \\equiv \\psi\\) can be used as an inference rule to deduce \\(\\psi\\) from \\(\\Gamma\\); moreover, some inference rules are the result of iteratively applying equivalences and other rules, they can be used to make processing faster (e.g. modus pones, modus tollens).</p> \\[\\large \\begin{aligned}     \\text{And-Elimination}&amp;: \\quad     \\frac{\\phi, \\quad \\psi}{\\phi}, \\frac{\\phi, \\quad \\psi}{\\psi} \\\\     \\text{Modus Pones}&amp;: \\quad     \\frac{\\phi, \\quad \\phi \\Rightarrow \\psi}{\\psi} \\\\     \\text{Modus Tollens}&amp;: \\quad     \\frac{\\lnot\\psi, \\quad \\phi \\Rightarrow \\psi}{\\lnot\\phi} \\end{aligned} \\]"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Deductive%20System/#propositional-logic-resolution","title":"Propositional Logic Resolution","text":"<p>To check whether a set of premises \\(\\Gamma\\) entails a conclusion \\(\\psi\\), there are some steps to follow:</p> <ol> <li>Remove any implication or biconditional through equivalences;</li> <li>Convert the premises to a negated normal form;</li> <li>Convert the premises to a conjunctive normal form;</li> <li>Conjunct the premises with the negated conclusion;</li> <li>Try to derive the empty clause through inference rules.</li> </ol> <p>If an empty clause can be found, then \\(\\psi\\) is a deductive consequence of (entailed by) the premises \\(\\Gamma\\); otherwise, if all the possible clauses have been explored without finding an empty clause, \\(\\psi\\) isn't a conclusion of \\(\\Gamma\\).</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Entailment/","title":"Entailment","text":"<p>\\(\\def \\M#1{{ \\mathcal M({#1}) }}\\) \\(\\def \\seq#1#2{{ {#1}_1, {#1}_2, \\ldots, {#1}_{#2} }}\\) \\(\\def \\seqop#1#2#3{{ {#2}_1 #1 {#2}_2 #1 \\cdots #1 {#2}_{#3} }}\\)</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Entailment/#entailment","title":"Entailment","text":"<p>Entailment is logical consequence. </p> <p>Given two sentences \\(\\phi, \\psi\\), it is said that \\(\\phi\\) entails \\(\\psi\\) iff \\(\\M\\phi \\subseteq \\M\\psi\\), i.e. the set of all models of \\(\\phi\\) is a subset of the set of all models of \\(\\psi\\).</p> \\[\\large     \\phi \\vDash \\psi     \\quad \\Longleftrightarrow \\quad     \\M \\phi \\subseteq \\M \\psi \\] <p>Similarly, given a set of sentences \\(\\set\\seq \\phi n\\) and a sentence \\(\\psi\\) such that \\(\\psi\\) is a logical consequence of \\(\\set\\seq \\phi n\\), i.e. \\(\\M{\\seq \\phi n} \\subseteq \\M \\psi\\), it can be  said that \\(\\seq \\phi n\\) are the assumptions and \\(\\psi\\) is the conclusion.</p> \\[\\large     \\set\\seq \\phi n \\vDash \\psi \\] <p>[!tip] Implication vs Entailment</p> <p>At a first glance, implication and entailment might describe the same thing, but they work on different levels:</p> <ul> <li>Implication is a logical operator that returns true if the LHS proposition actually implies the RHS proposition;</li> <li>Entailment isn't an operator, but it is a symbol to describe the relation between propositions (i.e. if LHS true then RHS true, otherwise no claim).</li> </ul> <p>Rephrasing the two concepts, it could even be said that the implication operator returns true if the RHS is entailed by the LHS, false otherwise.</p> <p>Because of the entailment definition, the assumptions are stricter sentences w.r.t. the conclusion: the assumptions rule out more models than the conclusion do.</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Entailment/#tautology","title":"Tautology","text":"<p>Tautology, sometimes called validity, refers to those sentences that are satisfied by all models (i.e. true for all models).</p> <p>Let \\(n\\) be the number of propositional symbols in a sentence \\(\\phi\\), then a sentence is a tautology (or is valid) if \\(|\\M \\phi| = 2^n\\), i.e. the number of all possible models given \\(n\\) propositions. Another way to check if a sentence is a tautology is to reduce the sentence to \\(True\\) using logical equivalences.</p> <p>A tautology is marked by writing the sentence(s) on the RHS of the entailment symbol, with nothing on the LHS.</p> \\[\\large     \\vDash \\seq \\phi n \\] <p>[!example] Example</p> <p>\\(\\phi = \\text{\"To be or not to be\"}\\) isn't a dilemma, it's a tautology (\\(\\vDash \\phi\\)).</p> <p>\\(\\phi = \\beta \\lor \\lnot\\beta\\) can be reduced, by the complement rule, to \\(\\phi = True\\), which shows that \\(\\phi\\) is indeed a tautology. It can also be checked, by a truth table, that \\(|\\M \\phi| = 2\\), which is the number of all possible models</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Entailment/#contradiction","title":"Contradiction","text":"<p>Contradiction is the opposite of tautology, it's a sentence that won't be satisfied by any model (i.e. false for all models).</p> <p>A sentence \\(\\phi\\) is a contradiction if \\(|\\M \\phi| = 0\\), i.e. there are no models of \\(\\phi\\) such that \\(\\phi\\) is true in it.</p> <p>A contradiction is marked by writing the sentence(s) on the RHS of the negated entailment symbol, with nothing on the LHS.</p> \\[\\large     \\nvDash \\seq \\phi n \\] <p>[!example]</p> <p>The AND complement of any proposition is a contradiction.</p> <p>\\(\\phi = \\beta \\land \\lnot\\beta\\), by the complement rule, can be reduced to \\(\\phi = False\\), which shows that there's no model in which \\(\\phi\\) is satisfied and hence \\(\\phi\\) is a contradiction.</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Entailment/#equivalences","title":"Equivalences","text":"<p>Given some sentences \\(\\seq \\phi n, \\psi\\), the following entailments / tautologies / contradictions express the same concept, it could be said (by misuse of the term) that they're equivalent.</p> \\[\\large \\displaylines{     \\seq \\phi n \\vDash \\psi \\\\     \\seqop{\\land} \\phi n \\vDash \\psi \\\\     \\vDash (\\seqop{\\land} \\phi n) \\Rightarrow \\psi \\\\     \\vDash (\\phi_1 \\Rightarrow (         \\cdots \\Rightarrow (\\phi_n \\Rightarrow \\psi) \\cdots     )) \\\\     \\nvDash \\seqop{\\land} \\phi n \\land \\lnot\\psi } \\]"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Logical%20Operations/","title":"Logical Operations","text":"<p>\\(\\def \\M#1{{ \\mathcal M({#1}) }}\\)</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Logical%20Operations/#logical-operations","title":"Logical Operations","text":"<p>In boolean proposition logic, the operators are the following (in descending order of precedence).</p> <ol> <li>Negation \\([\\,\\lnot\\,] \\rightarrow\\) negates a given proposition;</li> <li>And \\([\\,\\land\\,] \\rightarrow\\) true if both the LHS and RHS propositions are true;</li> <li>Or \\([\\,\\lor\\,] \\rightarrow\\) true if either one of or both the LHS and RHS propositions are true;</li> <li>Implication \\([\\,\\Rightarrow\\,] \\rightarrow\\) is true unless the LHS is true and the RHS is false;</li> <li>Biconditional \\([\\,\\Leftrightarrow\\,] \\rightarrow\\) it is true if both the LHS and RHS are equal (i.e. true-true or false-false).</li> </ol> <p>Parenthesis can be used to change the natural order of the operators by first evaluating the operators inside the parenthesis and then the ones outside.</p> <p>[!note] Literal</p> <p>A single propositional symbol, with or without a negation operator, is called a literal.</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Logical%20Operations/#equivalences","title":"Equivalences","text":"<p>A sentence \\(\\phi\\) is said to be equivalent to a sentence \\(\\psi\\) if \\(\\M \\phi = \\M \\psi\\). It could also be said that the two sentences are equivalent if they entail each other, i.e. \\(\\phi \\vDash \\psi\\) and \\(\\psi \\vDash \\phi\\).</p> <p>[!note]</p> <p>It can be proven, by means of a truth table or through other equivalences, that some sentences are equivalent to other sentences.</p> <p>Most (if not all) properties of boolean operators (and, or, not) work under logical operators too, e.g.</p> <ul> <li>Associativity (AND, OR)</li> <li>Commutativity (AND, OR)</li> <li>Distributivity (AND, OR)</li> <li>De-Morgan (AND / OR)</li> <li>Involution (NOT)</li> </ul> <p>The inverse and converse of implications don't hold, but a different kind of \"opposite\" (called contraposition) obtained by first inverting the implication and then conversing it does hold.</p> \\[\\large     \\alpha \\Rightarrow \\beta     \\ \\equiv \\      \\lnot\\beta \\Rightarrow \\lnot\\alpha \\] <p>Implications can be removed: implications are equivalent to the disjunction of the RHS and the negation of the LHS.</p> \\[\\large     \\alpha \\Rightarrow \\beta     \\ \\equiv \\      \\lnot\\alpha \\lor \\beta \\] <p>Biconditionals can be removed: biconditionals tell that two implications are in place, LHS implies RHS and RHS implies LHS.</p> \\[\\large     \\alpha \\Leftrightarrow \\beta     \\ \\equiv \\      (\\alpha \\Rightarrow \\beta) \\land (\\beta \\Rightarrow \\alpha) \\]"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Logical%20Operations/#negated-normal-form","title":"Negated Normal Form","text":"<p>Negated Normal Form (a.k.a. NNF) is a sentence form where all the negations are pushed on the single symbols, rather than on complex sentences, and there are no implications nor equivalences.</p> <p>Any sentence can be converted to NNF, if not already, by using a combination of equivalences on parts of the sentence until the sentence is in NNF.</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Logical%20Operations/#conjunctive-normal-form","title":"Conjunctive Normal Form","text":"<p>Conjunctive Normal Form (a.k.a. CNF) is a sentence form where the sentence is formed by conjunctions (\\(\\land\\)) of disjunctions (\\(\\lor\\)) of literals (either \\(a\\) or \\(\\lnot a\\)).</p> \\[\\large     (\\ell_{1,1} \\lor \\cdots \\lor \\ell_{1,n_1})     \\land \\cdots \\land     (\\ell_{m,1} \\lor \\cdots \\lor \\ell_{m,n_m}) \\] <p>Any sentence can be converted to CNF, if not already, by converting the sentence into NNF and then distributing the disjunctions over the conjunctions.</p> \\[\\large     \\phi \\lor (\\psi \\land \\chi)     \\equiv     (\\phi \\lor \\psi) \\land (\\phi \\lor \\chi) \\]"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Propositional%20Logic/","title":"Propositional Logic","text":"<p>\\(\\def \\M#1{{ \\mathcal M({#1}) }}\\) \\(\\def \\seq#1#2{{ {#1}_1, {#1}_2, \\ldots, {#1}_{#2} }}\\)</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Propositional%20Logic/#propositional-logic","title":"Propositional Logic","text":"<p>Propositional Logic is one of the most important concepts for reasoning with artificial intelligence. It is a whole mathematical model used to describe and infer the real world.</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Propositional%20Logic/#knowledge-base","title":"Knowledge Base","text":"<p>The knowledge base, KB, is the core component of logic, it is a set of sentences. The sentences are not the homonymous ones used by humans, but rather they are a way to assert something about the world. Sentences can be either given as ground truth (axioms) or inferred from other sentences.</p> <p>A knowledge base can be queried in two ways: - tell, adding sentences to the KB; - ask, retrieving sentences from the KB.</p> <p>[!info] Inferring</p> <p>Inferring can happen both when a KB has been told or asked something, it isn't constrained to when it is asked only-.</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Propositional%20Logic/#model","title":"Model","text":"<p>A model refers to an instance of the world where all propositions of the model assume a certain value. In a world where all propositions can assume boolean values, there are a total of \\(2^N\\) models, where \\(N\\) is the number of propositions.</p> <p>If a model satisfies a sentence \\(\\alpha\\), it is said that \\(m\\) satisfies \\(\\alpha\\) or \\(m\\) is a model of \\(\\alpha\\). The notation \\(\\M\\alpha\\) is used to indicate the set of all models of \\(\\alpha\\) (i.e. \\(m \\in \\M\\alpha)\\) is a model of \\(\\alpha\\)).</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Propositional%20Logic/#syntax","title":"Syntax","text":"<p>Syntax defines which sentences are valid sentences and which are invalid, it describes the structure of sentences.</p> <p>Legal sentences (propositional formulae) are discerned in atomic sentences and complex sentences:</p> <ul> <li>atomic sentences are sentences which are composed of a single proposition symbol, a proposition symbol indicates a single proposition (a fact about the world) that can assume a truth value (whose domain is defined by semantics);</li> <li>complex sentences are sentences which are composed of atomic sentences and operators, operators define the relations between the atomic sentences.</li> </ul> <p>[!note] Symbols and Operators</p> <p>Syntax defines the sets of all proposition symbols and operators and the rules such that a sentence is valid or invalid, but it doesn't define how interpret symbols or how to apply operators and their result. That is left up to semantics.</p>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Propositional%20Logic/#semantics","title":"Semantics","text":"<p>Semantics define the rules for determining the truth of a sentence with respect to a particular model. In the simplest logic model, semantics define that a proposition can assume just one of two truth values: true or false.</p> <p>Semantics then define some other rules:</p> <ul> <li>The only pre-existent sentences are \\(\\top\\)  (a.k.a. top) and \\(\\bot\\) (a.k.a. bottom), with values True and False respectively for every model;</li> <li>The truth value of every proposition symbol must be assigned to the model directly, it can't be supposed outside the model;</li> <li>The rules for all operators.</li> </ul>"},{"location":"AI%20and%20ML/Unit%201/Propositional%20Logic/Propositional%20Logic/#evaluation","title":"Evaluation","text":"<p>Two functions are defined to query the values of symbols and sentences: the interpretation function and the valuation function.</p> <p>The interpretation function takes in input a propositional symbol and returns the truth value of that symbol on the underlying model.</p> \\[\\large     \\iota : PL \\rightarrow \\set{T,F} \\] <p>The interpretation functions isn't the same for all models, but rather it is dependant on the model: it returns the truth values for the symbols in the model, i.e. given a model such that \\(\\set{\\alpha=T,\\beta=F}\\), then \\(\\iota(\\alpha) = T\\) and \\(\\iota(\\beta) = F\\).</p> <p>The valuation function (under \\(\\iota\\)) takes in input a sentence, it evaluates the truth value of the sentence and it returns it.</p> \\[\\large     v^\\iota : \\mathcal L_{PL} \\rightarrow \\set{T,F} \\]"},{"location":"AI%20and%20ML/Unit%202/Distance%20Metrics/","title":"Distance Metrics","text":"<p>\\(\\def \\ND#1#2{{ \\mathcal N \\left( {#1},{#2} \\right) }}\\)</p>"},{"location":"AI%20and%20ML/Unit%202/Distance%20Metrics/#distance-metrics","title":"Distance Metrics","text":"<p>Distance metrics are mathematical measures used to quantify the distance or similarity between two data points in a dataset. There are several different distance metrics used, the most common being the Euclidian distance.</p>"},{"location":"AI%20and%20ML/Unit%202/Distance%20Metrics/#l-norms","title":"L-norms","text":"<p>L-norms are a family of distance metrics parametrised on a parameter \\(\\ell\\), which can be any positive real number.</p> <p>Given a vector of \\(n\\) components, L-norms are generalized by the following formula.</p> \\[\\large \\begin{aligned}     || x ||_\\ell &amp;= \\sqrt[l] {|x_1|^\\ell + \\cdots + |x_n|^\\ell} \\\\     &amp;= \\left( \\sum_{i=1}^n |x_i|^\\ell \\right)^\\frac{1}{l} \\end{aligned} \\] <p>The most used L-norms are the following: - Manhattan Distance</p>"},{"location":"AI%20and%20ML/Unit%202/Distance%20Metrics/#l1-norm","title":"L1 Norm","text":"<p>The L1 norm, also called Manhattan distance, is a specific case of L-norm where the parameter \\(\\ell\\) is 1.</p> <p>The norm computes the sum of all the components in the vector.</p> \\[\\large     || x ||_1 = \\sum_{i=1}^n | x_i | \\] <p>[!quote] Manhattan Distance</p> <p>The L1 norm is called Manhattan distance because, if traced on a plot, the summed components would look like the straight streets that separate the blocks in Manhattan.</p>"},{"location":"AI%20and%20ML/Unit%202/Distance%20Metrics/#l2-norm","title":"L2 Norm","text":"<p>The L2 norm, also called Euclidian distance, is a specific case of L-norm where the parameter \\(\\ell\\) is 2.</p> <p>The norm computes the Euclidian length of a vector.</p> \\[\\large     || x ||_2 = \\sqrt{ \\sum_{i=1}^n x_i^2 } \\]"},{"location":"AI%20and%20ML/Unit%202/Distance%20Metrics/#mahalanobis-distance","title":"Mahalanobis Distance","text":"<p>The Mahalanobis distance is a distance metric that measures the distance of a point \\(x\\) from a normal distribution \\(\\ND \\mu \\Sigma\\), it takes into account the correlations of the data and the variances of the variables involved.</p> <p>[!tip] Mahalanobis Distance</p> <p>Computing the Mahalanobis distance would be equal to centring and decorrelating the distribution \\(\\ND \\mu \\Sigma \\rightarrow \\ND {\\vec 0} {I}\\) and computing the Euclidian distance between the new position of the point relative to the distribution and the origin, i.e. the vector's magnitude.</p> <p>The formula for the Mahalanobis distance between a point \\(x\\) and a distribution with mean \\(\\mu\\) and covariance matrix \\(\\Sigma\\) is the following.</p> \\[\\large     D_M(x) = \\sqrt{         (x - \\mu)^T \\Sigma^{-1} (x - \\mu)     } \\] <p>The Mahalanobis distance is useful in various fields, including classification, clustering, and outlier detection. It is particularly helpful when the data is high-dimensional and correlated, as it takes these factors into account to provide a more accurate distance measure.</p>"},{"location":"AI%20and%20ML/Unit%202/Distance%20Metrics/#mahalanobis-distance-formula","title":"Mahalanobis Distance Formula","text":"<p>Here's a break-down of the Mahalanobis distance formula.</p> <ol> <li>Centring the distribution: the distribution is centred by subtracting the mean \\(\\rightarrow \\overline x = x - \\mu\\);</li> <li>Decorrelating the distribution: the distribution is standardized by inverting the covariance between the variables \\(\\rightarrow \\Sigma^{-1} \\overline x\\);</li> <li>Projecting the point: now that the distribution is standardized, i.e. \\(\\mathcal D \\sim \\ND{\\vec 0}{I}\\), the Euclidian distance can be used to compute the distance from the centre of the distribution. The centre is the origin, so the distance is just the magnitude of the vector, which can be computed by computing the square root of the dot product with itself.</li> </ol>"},{"location":"AI%20and%20ML/Unit%202/Distance%20Metrics/#cosine-similarity","title":"Cosine Similarity","text":"<p>TK</p>"},{"location":"AI%20and%20ML/Unit%202/Evaluation%20Metrics/","title":"Evaluation Metrics","text":"<p>When evaluating the performance of a classification model, several metrics can be used to assess its effectiveness. Here are some commonly used evaluation metrics for binary classification.</p>"},{"location":"AI%20and%20ML/Unit%202/Evaluation%20Metrics/#score","title":"Score","text":"<p>The score of a binary classifier refers to the numerical value or probability assigned to each instance that indicates its likelihood of belonging to the positive class. The score can be used as a measure of confidence or certainty in the classification decision made by the model.</p> <p></p>"},{"location":"AI%20and%20ML/Unit%202/Evaluation%20Metrics/#threshold","title":"Threshold","text":"<p>In binary classification, a score threshold is a predefined value used to convert the continuous scores assigned by a binary classifier into a categorical prediction; by comparing the score of an instance to the threshold, the classifier determines whether to classify it as belonging to the positive class or the negative class.</p> <p>When the score of an instance exceeds the threshold, it is classified as positive; otherwise, it is classified as negative. The threshold can be set at different levels depending on the desired trade-off:</p> <ul> <li>A lower threshold results in more positive predictions, potentially increasing the sensitivity but also increasing the risk of false positives;</li> <li>A higher threshold leads to fewer positive predictions, potentially increasing the specificity but also increasing the risk of false negatives.</li> </ul> <p></p>"},{"location":"AI%20and%20ML/Unit%202/Evaluation%20Metrics/#confusion-matrix","title":"Confusion Matrix","text":"<p>A confusion matrix is a useful tool to visualize and evaluate the performance of a classifier. It provides a comprehensive summary of the classifier's predictions and their agreement with the actual class labels.</p> <p></p> <p>A binary classifier confusion matrix is a 2x2 matrix that consists of four elements:</p> <ul> <li>True Positive \\([tp]\\): The number of instances correctly classified as positive.</li> <li>False Positive \\([fp]\\) The number of instances incorrectly classified as positive.</li> <li>True Negative \\([tn]\\): The number of instances correctly classified as negative.</li> <li>False Negative \\([fn]\\): The number of instances incorrectly classified as negative.</li> </ul> <p>The elements on the diagonal represent correct predictions; the elements on the off-diagonal represent wrong predictions.</p> <p>[!info] Optimal Classifier</p> <p>An optimal classifier's confusion matrix will be a diagonal matrix: there won't be wrong predictions, so the off-diagonal elements will always be null.</p>"},{"location":"AI%20and%20ML/Unit%202/Evaluation%20Metrics/#accuracy","title":"Accuracy","text":"<p>Accuracy measures the overall correctness of the classifier's predictions. It quantifies the proportion of instances that are correctly classified out of the total number of instances in the dataset.</p> \\[\\large     \\text{Accuracy} =     \\frac{tp + tn}{tp + tn + fp + fn} \\] <p>The number of correct predictions refers to the count of instances that are correctly classified as either positive or negative, while the total number of predictions represents the total number of instances in the dataset.</p> <p>[!warning] Drawbacks</p> <p>Accuracy provides a straightforward measure of how well the classifier performs in terms of overall correctness. However, it can be misleading in certain scenarios, especially when dealing with imbalanced datasets where the classes are not represented equally. In such cases, a high accuracy value can be achieved by simply predicting the majority class most of the time, while the classifier may struggle to correctly identify the minority class.</p>"},{"location":"AI%20and%20ML/Unit%202/Evaluation%20Metrics/#precision","title":"Precision","text":"<p>Precision is used to assess the quality of the positive predictions made by a classifier. It quantifies the proportion of correctly predicted positive instances out of all instances predicted as positive.</p> \\[\\large     \\text{Precision}     = \\frac{tp}{tp + fp} \\] <ul> <li>A high precision indicates that the classifier has a low rate of false positive predictions, meaning it is selective in labelling instances as positive.</li> <li>A low precision suggests that the classifier has a higher rate of false positive predictions, resulting in a higher number of instances being falsely labelled as positive.</li> </ul> <p>[!tip] Use Case</p> <p>Precision is particularly useful in scenarios where the cost of false positive predictions is high, such as in medical diagnoses or spam email filtering. In these cases, precision helps assess the classifier's ability to avoid false alarms and provide accurate positive predictions.</p>"},{"location":"AI%20and%20ML/Unit%202/Evaluation%20Metrics/#recall","title":"Recall","text":"<p>Recall, also known as sensitivity or true positive rate, is used to measure the classifier's ability to correctly identify positive instances out of all actual positive instances.</p> \\[\\large     \\text{Recall}     = \\frac{tp}{tp + fn} \\] <ul> <li>A high recall indicates that the classifier is effective in capturing most of the positive instances and has a low rate of false negative predictions. It means that the classifier has a good ability to detect positive cases.</li> <li>A low recall suggests that the classifier has a higher rate of false negative predictions, resulting in missing a significant number of actual positive instances.</li> </ul> <p>[!tip] Use Case</p> <p>Recall plays a crucial role in situations where identifying all positive instances is critical, such as in disease detection or fraud detection. In medical diagnoses, for example, a high recall ensures that the classifier minimizes the number of missed positive cases, enabling timely intervention and treatment.</p>"},{"location":"AI%20and%20ML/Unit%202/Evaluation%20Metrics/#f-measure","title":"F-Measure","text":"<p>The F-measure is an evaluation metric that combines both precision and recall into a single score. It provides a balanced assessment of a classifier's performance by considering both the ability to avoid false alarms (i.e. precision) and the ability to capture positive instances (i.e. recall).</p> \\[\\large     F_\\beta =     {         (1 + \\beta^2) \\cdot \\text{Precision} \\cdot \\text{Recall}         \\over         \\beta^2 \\cdot \\text{Precision} + \\text{Recall}     } \\] <p>The parameter \u03b2 controls the weight assigned to precision and recall: - when \\(\\beta = 1\\) it is called the F1-measure, which provides an equal balance between precision and recall; - higher \\(\\beta\\) values gives more weight to recall; - lower \\(\\beta\\) values gives more weight to precision.</p> <p>The F-measure is useful when there is an imbalance between the positive and negative classes in the dataset, it provides a comprehensive evaluation of the classifier's performance by considering both false positives and false negatives.</p>"},{"location":"AI%20and%20ML/Unit%202/Evaluation%20Metrics/#positive-rates","title":"Positive Rates","text":"<p>Positive rates refer to the proportions associated with the positive class predictions made by a classifier.</p> <ul> <li>The True Positive Rate \\([TPR]\\) measures the proportion of actual positive instances correctly classified as positive by the classifier.</li> </ul> \\[\\large     TPR = \\frac{tp}{tp + fn} \\] <p>[!note] True Positive Rate</p> <p>The \\(TPR\\), which is actually the recall, provides insights into how well the classifier captures positive instances and is particularly relevant when the goal is to minimize false negatives.</p> <ul> <li>The False Positive Rate \\([FPR]\\) measures the proportion of negative instances incorrectly classified as positive by the classifier.</li> </ul> \\[\\large     FPR = \\frac{fp}{fp + tn} \\] <p>[!note] False Positive Rate</p> <p>The \\(FPR\\) helps assess the classifier's tendency to produce false alarms by misclassifying negative instances as positive.</p>"},{"location":"AI%20and%20ML/Unit%202/Evaluation%20Metrics/#receiver-operating-characteristic-curve","title":"Receiver Operating Characteristic Curve","text":"<p>The Receiver Operating Characteristic curve is a graphical representation that illustrates the performance of a classifier by plotting the True Positive Rate against the False Positive Rate at various classification thresholds.</p> <p>The curve is created by calculating the TPR and FPR for different threshold settings and plotting them on a graph. An optimal classifier would have an ROC curve that reaches the top-left corner of the graph, indicating high sensitivity and low false positive rate.</p> <p></p> <p>The area under the ROC curve (AUC) is commonly used as a metric to quantify the overall performance of the classifier. A higher AUC value indicates better classifier performance in distinguishing between positive and negative instances.</p> <p>The ROC curve and AUC are useful tools for assessing the discriminative power of a binary classifier and comparing the performance of different models. They provide a comprehensive evaluation of the classifier's ability to balance true positives and false positives across different threshold values, allowing for an informed decision on the threshold selection based on the specific requirements of the problem at hand.</p>"},{"location":"AI%20and%20ML/Unit%202/Impurity/","title":"Impurity","text":"<p>\\(\\def \\P#1{{ \\mathbb{P} \\left(#1\\right) }}\\) \\(\\def \\E#1{{ \\mathbb{E} \\left(#1\\right) }}\\)</p>"},{"location":"AI%20and%20ML/Unit%202/Impurity/#impurity","title":"Impurity","text":"<p>Impurity, in a Machine Learning context, is a numerical value that represent how accurate a prediction-model is. It usually refers to models that use decision-making algorithms such as decision trees, and it is analogous to a loss or cost function.</p> <p>[!tip] Classification</p> <p>A good classification model should should aim to lower the impurity value.</p> <p>Usually, impurity functions are denoted by \\(H(S)\\), where \\(S\\) is a dataset.</p> <p>Let \\(S_k \\subseteq S\\) be subsets divided by label,</p> \\[\\large     S_k = \\set{(x,y) \\in S \\ | \\ y = k}, \\quad     \\bigcup_k S_k = S \\] <p>then, define \\(p_k\\) to be the probability of picking a point of label \\(k\\) from \\(S\\).</p> \\[\\large     p_k = \\frac{|S_k|}{|S|} \\]"},{"location":"AI%20and%20ML/Unit%202/Impurity/#misclassification-impurity","title":"Misclassification Impurity","text":"<p>The misclassification impurity measures the probability of mislabelling the label with the highest cardinality.</p> \\[\\large     H(S) = 1 - \\max_k(p_k) \\] <p>[!example] Example</p> <p>Given a set \\(S\\) with 4 points, of which 3 have label \\(y_1\\) and 1 has label \\(y_2\\), then the misclassification impurity of \\(S\\) is $$\\large   H(S) = 1 - \\max{p_k}   = 1 - \\max\\set{ \\frac{3}{4}, \\frac{1}{4} }   = \\frac{1}{4} = 25\\% $$</p>"},{"location":"AI%20and%20ML/Unit%202/Impurity/#gini-impurity","title":"Gini Impurity","text":"<p>The Gini impurity is an advanced form of misclassification impurity, but it involves all classes instead of considering just the one with the highest cardinality.</p> \\[\\large     G(S) = \\sum_{k=1}^K p_k (1 - p_k) \\] <p>The Gini impurity represents the expectation of randomly classifying a point with the wrong label.</p> \\[\\large \\begin{aligned}     \\E{K \\ne k}     &amp;= \\E{1 - \\P{K = k}} \\\\     &amp;= \\E{1 - p_k} \\\\     &amp;= \\sum_{k=1}^K (1 - p_k) p_k \\\\     &amp;= G(S) \\end{aligned} \\]"},{"location":"AI%20and%20ML/Unit%202/Impurity/#entropy","title":"Entropy","text":"<p>Entropy is defined as the amount of randomness, or chaos, that is present in a set. The formula that defines entropy is the following.</p> \\[\\large     H(X) \\doteq - \\sum_{x \\in X} p_x \\log_2 p_x \\] <p>[!abstract] Problem generalization</p> <p>The entropy function is denoted as \\(H(X)\\), and not \\(H(S)\\), because we generalize the problem and suppose that \\(X\\) is a random variable instead of a set of points and labels, but the only thing that changes is the notation.</p> <p>Like Gini impurity, entropy computes some kind of expectation, but using a function \\(h(x)\\) that measures the excitement of a single class. For convenience, \\(h(x)\\) must abide to some properties.</p> <p>The excitement of a class being picked should be inversely proportional to the likelihood of it happening:</p> <ul> <li>if a class is almost certain to be picked, then the excitement should be very small, \\(\\P X \\rightarrow 1 \\Longrightarrow h(x) \\rightarrow 0\\);</li> <li>if a class is almost impossible to be picked, then the excitement should be very big, \\(\\P X \\rightarrow 0 \\Longrightarrow h(x) \\rightarrow \\infty\\).</li> </ul> <p>If two pick events are independent, then we should be able to sum the excitement of both pickings to get the excitement of the joint picking: \\(h(x,y) = h(x) + h(y)\\).</p> <p>A function that satisfies all the previous properties is the \\(\\log\\) function of the inverse of the probability of picking the class, which is equal to the negative \\(log\\) of the probability. </p> \\[\\large     h(x) = \\log\\frac{1}{p_x} = - \\log p_x \\] <p>Hence, the entropy function computes the expectation of the excitement of each class.</p> \\[\\large \\begin{aligned}     \\E{h(x)} &amp;= \\sum_{x \\in X} p_x \\, h(x) \\\\     &amp;= - \\sum_{x \\in X} p_x \\log_2 p_x \\\\     &amp;= H(X) \\end{aligned} \\] <p>[!tip] Common cases for Entropy</p> <p>Here are listed some common cases for entropy:</p> <ul> <li>If there is just one class, then \\(H(X) = 0\\), i.e. there is no excitement because the only class present is always going to be picked.</li> <li>If there are \\(N\\) classes equiprobable, then all the classes will have the same excitement. The value of the entropy will be equal to the excitement of all classes, and can be computed by the formula \\(H(X) = \\log_2(N)\\).</li> </ul>"},{"location":"AI%20and%20ML/Unit%202/Impurity/#relative-entropy","title":"Relative Entropy","text":"<p>Given two distributions \\(P,Q\\), the cross entropy \\(H(P,Q)\\) is defined as follows.</p> \\[\\large     H(P,Q) = -\\sum_{x \\in X} p_x \\log q_x \\] <p>The cross-entropy represents the expected value of the excitement of the classes in \\(Q\\) while using the weights of \\(P\\).</p> <p>[!note] Note</p> <p>Note that, usually, \\(H(P,Q) \\neq H(Q,P) and k\\)</p> <p>Given two distributions \\(P,Q\\)</p>"},{"location":"AI%20and%20ML/Unit%202/Machine%20Learning/","title":"Machine Learning","text":"<p>Machine Learning is a branch of Artificial Intelligence that teaches machines to learn and improve their performance on specific tasks by recognizing patterns in data. Compared to traditional algorithmic approaches, ML can analyse complex situations, adapt to new data and provide greater accuracy and efficiency.</p> <p>The whole Machine Learning system is based on finding common patterns in the data, called training set, which then will be used to make a prediction on new input data.</p> <p>[!info] Output Prediction</p> <p>The output prediction on the input data can take values of various types, based on the application of the ML algorithms; for example:</p> <ul> <li> <p>Discrete labels \\(\\rightarrow Y \\subset \\mathbb N\\)</p> </li> <li> <p>Continuous labels \\(\\rightarrow Y \\subseteq \\mathbb R\\)</p> </li> <li> <p>Hyper-dimensional vectors, e.g. from the same space of the input data \\(\\rightarrow Y \\subset \\mathbb R^d\\)</p> </li> </ul> <p></p>"},{"location":"AI%20and%20ML/Unit%202/Machine%20Learning/#learning","title":"Learning","text":"<p>Suppose that there exists a loss function, such that it has null value if the prediction is correct and grows the more the prediction is wrong. Learning is defined as lowering the loss (or cost) function when predicting both training and testing data.</p> \\[\\large     \\mathcal L( \\underbrace{\\hat y}_\\text{pred.}, \\underbrace{y}_\\text{gt} )     \\quad \\text{where} \\quad     \\hat y = h_0(x) \\]"},{"location":"AI%20and%20ML/Unit%202/Machine%20Learning/#regression-loss-functions","title":"Regression Loss Functions","text":"<ul> <li> <p>Squared error: \\(\\mathcal L(\\hat y, y) = (\\hat y - y)^2\\)</p> </li> <li> <p>Absolute error: \\(\\mathcal L(\\hat y, y) = |\\hat y - y|\\)</p> </li> </ul>"},{"location":"AI%20and%20ML/Unit%202/Machine%20Learning/#classification-loss-functions","title":"Classification Loss Functions","text":"<ul> <li>Binary error: TK piecewise function, 0 if correct, 1 if wrong</li> </ul>"},{"location":"AI%20and%20ML/Unit%202/Machine%20Learning/#types-of-machine-learning","title":"Types of Machine Learning","text":"<ul> <li> <p>Unsupervised Learning</p> </li> <li> <p>Supervised Learning</p> </li> <li> <p>Deep Learning</p> </li> <li> <p>Reinforced Learning</p> </li> </ul>"},{"location":"AI%20and%20ML/Unit%202/Machine%20Learning/#model","title":"Model","text":"<p>A model is a representation of a system that learns from training data, it consists of a set of algorithms and statistical models that interpret the training data in some way such that it can make (hopefully correct) predictions and decisions when given new data.</p> <p>Models can be distinguished into two categories:</p> <ul> <li>Parametric models, the training data is converted into a set of parameters \\(\\theta\\) that can be used to make predictions;</li> <li>Non-parametric models, the training data is stored directly and processed each time a prediction needs to be made, there are no parameters to be inferred.</li> </ul> <p>[!tip] Instance-based learning</p> <p>Non-parametric models are sometimes referred to as instance-based learning, because learning for such models is simply storing training data.</p>"},{"location":"AI%20and%20ML/Unit%202/Machine%20Learning/#fitting-the-data","title":"Fitting the Data","text":"<p>Efficiency, correctness and accuracy of a machine learning model all depend from how the model fits the data.</p> <p>A model may fit the data in three ways:</p> <ul> <li> <p>Underfitting, the model is too simple and lacks the ability to learn the patterns and structure present in the data, usually occurs when a model does not fit the training data well enough;</p> </li> <li> <p>Overfitting, the model is too complex and has learned too many of the noise and small details from the training data, it occurs when a model fits the training data too well;</p> </li> <li> <p>Best fitting, the model fits the training data well enough to capture the underlying patterns and structure without being biased by the noise and details. The best fitting model is neither underfitting nor overfitting and can accurately predict outcomes of unseen data.</p> </li> </ul> <p></p>"},{"location":"AI%20and%20ML/Unit%202/Machine%20Learning/#limits-of-machine-learning","title":"Limits of Machine Learning","text":"<p>Machine learning isn't perfect: there are multiple limitations to it.</p> <ul> <li>Bias</li> </ul> <p>The machine is learning from humans, and humans aren't perfect creatures. There could be some kind of bias (either historical or some other type) that could infect the predictions.</p> <ul> <li>Noise</li> </ul> <p>Noise could be present in the data: input data could present some noise, which could make a prediction harder. Moreover, labels in training data may present some form of noise too, which if unproperly treated could shape the predictions to retain the noise.</p> <ul> <li>Blackbox</li> </ul> <p>Most of the time, especially with complex and big models, the models themselves could pose as a blackbox: feed in the data and get out a prediction, but it may be insanely hard to grasp why the models made such a prediction.</p>"},{"location":"AI%20and%20ML/Unit%202/Preprocessing/Data%20Normalization/","title":"Normalization","text":"<p>Normalizing the data is an important step before applying any learning algorithm. If the data isn't normalized, it could be positioned in the higher-dimension space in a way such that the Euclidian distance (or any other distance metric) might not be a good metric.</p> <p>Moreover, algorithms such as k-NN have irregular and non-linear decision boundaries, which is a sign of overfitting. Normalization is applied to ensure smooth decision boundaries and reducing the risk of overfitting the data.</p>"},{"location":"AI%20and%20ML/Unit%202/Preprocessing/Data%20Normalization/#min-max-normalization","title":"Min-Max Normalization","text":"<p>Min-Max normalization aims to scale all the axes to fit the data in a range \\([0,1]^{|\\mathcal D|}\\) , so that all features have equal importance.</p> <p>The formula to scale a single axis to the range \\([0,1]\\) is the following.</p> \\[\\large     x' = \\frac{x - x_\\min}{x_\\max - x_\\min} \\]"},{"location":"AI%20and%20ML/Unit%202/Preprocessing/Data%20Normalization/#standard-normalization","title":"Standard Normalization","text":"<p>A.k.a. normal normalization, the standard normalization supposes that the data is generated by a single gaussian and is reshaped in a way to have zero mean and unit variance on all axis.</p> <p>The formula to standardize a single axis is the following.</p> \\[\\large     x' = \\frac{x - \\mu}{\\sigma} \\] <p>This type of normalization is equivalent to centring and decorrelating the features with PCA.</p>"},{"location":"AI%20and%20ML/Unit%202/Preprocessing/Data%20Normalization/#feature-normalization","title":"Feature Normalization","text":"<p>In reality, features carry different weights, meaning that some features are more important than other features. Some features could be even categorised as irrelevant features.</p> <p>The are two options to normalize the features.</p> <ol> <li>Classify good and irrelevant features</li> </ol> <p>Assume that the distribution is composed of good features (useful to classification) and irrelevant features (useless to classification).</p> <p>Let \\(\\mathcal S_{gt}, \\mathcal S_{ir}\\) be two sets containing the indices of good features (ground truth) and irrelevant features. Then, the Euclidian metric can be defined in the following way.</p> \\[\\large     d(x,v) = \\sqrt{         \\sum_{i \\in \\mathcal S_{gt} } (x_i - v_i)^2 +         \\sum_{j \\in \\mathcal S_{ir} } (x_j - v_j)^2     } \\] <p>Learning to recognize which are the irrelevant features and removing them could help increase the accuracy of the algorithm.</p> <ol> <li>Feature weighting</li> </ol> <p>The Euclidian distance treats all features equally, with the same importance, but each axis could carry a different meaning with a different importance (wrt to the others).</p> <p>The Euclidian distance can be redefined as a weighted sum of the magnitude difference on all axis: let \\(w = [\\seq w D]\\) be a vector containing the weights for each axis, then the weighted Euclidian distance \\(d(\\cdot, \\cdot)\\) is defined as follows.</p> \\[\\large     d(x, v) = \\sqrt{ \\sum_{i=1}^D w_i (v_i - x_i)^2 } \\]"},{"location":"AI%20and%20ML/Unit%202/Preprocessing/Data%20Normalization/#manifold","title":"Manifold","text":"<p>TK</p>"},{"location":"AI%20and%20ML/Unit%202/Preprocessing/Principal%20Component%20Analysis/","title":"Principal Component Analysis","text":"<p>Principal Component Analysis is a popular unsupervised machine learning technique used for data analysis and dimensionality reduction.</p> <p>PCA works by finds a set of orthogonal vectors that explain the maximum variance in a dataset, known as principal components. By projecting the data onto these principal components, we can reduce the dimensionality of the data while still preserving most of its variance. PCA can also be used for data visualization and noise reduction.</p> <p>[!note] Principal Components</p> <p>The principal components are ordered by the amount of variance: the first principal component explains the largest amount of variance, followed by the second principal component, and so on.</p> <p></p> <p>PCA is often used as a pre-processing step before applying other machine learning techniques, such as clustering or classification. By reducing the dimensionality of the data, PCA can improve the performance of these techniques and reduce the risk of overfitting.</p> <p>[!tip] Applications of PCA</p> <p>PCA can find usage in applications such as dimensionality reduction, data visualization, data compression and reconstruction, features decorrelation.</p>"},{"location":"AI%20and%20ML/Unit%202/Preprocessing/Principal%20Component%20Analysis/#performing-pca","title":"Performing PCA","text":"<p>Here is a python algorithm to perform PCA on a dataset \\(X\\)</p> <pre><code>import numpy as np\n\n# Let\n# - X be a NxD matrix, where N and D are\n#   the number of samples and dimensions, respectively.\n# - k (k &lt;= D) be the number of desired dimensions.\n\n# The algorithm assumes that the columns of X represent\n# the variables, the rows the observations.\n\n# 1. Subtract mean and divide by std dev\nX -= np.mean(X)\nX /= np.std(X)\n\n# 2. Compute the covariance matrix\ncov = np.cov(x)\n\n# 3. Compute eigenvectors and eigenvalues\neig_val, eig_vec = np.linalg.eig(cov)\n\n# 4. Sort the eigenvectors by their corresponding eigenvalues in decreasing order\neig_vec = eig_vec[:, eig_val.argsort()[::-1]]\n\n# 5. Choose the top k eigenvectors as the new basis,\n#    k is the desired number of principal components (dimensions)\neig_vec = eig_vec[:, :k]\n\n# 6. Project the data onto the principal components\nX = np.dot(eig_vec.T, X)\n</code></pre> <p>The dataset \\(X\\), after applying PCA, will be a \\(N \\times k\\) matrix where each observation (sample) is projected on the top \\(k\\) principal components.</p>"},{"location":"AI%20and%20ML/Unit%202/Preprocessing/Principal%20Component%20Analysis/#choosing-how-many-components","title":"Choosing how many components","text":"<p>There are various methods to choose how many components a PCA compression should retain:</p> <ul> <li>Akaike Information Criteria</li> <li>Bayesian Information Criteria</li> <li>95% variance</li> </ul> <p>[!abstract] 95% variance</p> <p>A good rule of thumb is to keep \\(\\min (k)\\) components with the most variance (i.e. higher eigenvalues) such that compressing the data on those \\(k\\) components will retain at least \\(95\\%\\) of the original variance.</p>"},{"location":"AI%20and%20ML/Unit%202/Supervised%20Learning/Bayes%20Optimal%20Classifier/","title":"Bayes Optimal Classifier","text":"<p>Theoretically, a complex and unknown generator \\(\\mathcal D\\) is assumed to provide output pairs \\((x,y)\\), where:</p> <ul> <li>\\(x \\in \\mathcal X\\) is an element of the set we're interested in learning;</li> <li>\\(y \\in \\mathcal Y\\) is a label assigned to the elements.</li> </ul> <p>\\(\\mathcal D\\) can be referred to as a probability distribution, where the probability of \\((x,y) \\in \\mathcal X \\times \\mathcal Y\\) is the probability that the label \\(y\\) is assignable to \\(x\\).</p> <p>Knowing the probability distribution \\(\\mathcal D\\), building a prediction function \\(h : \\mathcal X \\rightarrow \\mathcal Y\\) would be trivial. For an input \\(x\\), the function should return the label \\(y\\) such that the probability of \\((x,y)\\) is maximised, i.e.</p> \\[\\large     h(x) = \\arg \\max_{ y \\in \\mathcal Y } \\mathcal{D}(x,y) \\]"},{"location":"AI%20and%20ML/Unit%202/Supervised%20Learning/Decision%20Trees/","title":"Decision Trees","text":"<p>TK</p>"},{"location":"AI%20and%20ML/Unit%202/Supervised%20Learning/Nearest%20Neighbour/","title":"Nearest Neighbour","text":"<p>\\(\\def \\seq#1#2{{ {#1}_1, {#1}_2, \\ldots, {#1}_{#2} }}\\)</p>"},{"location":"AI%20and%20ML/Unit%202/Supervised%20Learning/Nearest%20Neighbour/#nearest-neighbour","title":"Nearest Neighbour","text":"<p>Nearest Neighbour, a.k.a. k-NN, is a supervised machine learning algorithm used to predict classifications on data.</p> <p>Let the input data live in a \\(D\\) dimensional space, then the goal of k-NN is to infer the classification of the data by looking at the closest \\(k\\) neighbours (data points) to the input in the space.</p> <p>[!abstract] Assumption</p> <p>k-NN works on the assumption that the label of a point should be similar to the label of other nearby points.</p>"},{"location":"AI%20and%20ML/Unit%202/Supervised%20Learning/Nearest%20Neighbour/#classifying","title":"Classifying","text":"<p>Given the training data \\(\\mathcal D = \\set{ (x_1, y_1), \\ldots, (x_n, y_n)}\\) and an input point \\(v\\), the label \\(y_v\\) of \\(v\\) is inferred to be the label of the closest point to \\(v\\),</p> \\[\\large \\displaylines {     x^* = \\arg \\min_{x_i \\in \\mathcal D} \\text d (x_i, v) \\\\     y_v = y^* \\quad | \\quad (x^*, y^*) \\in \\mathcal D }\\] <p>where \\(\\text d(\\cdot, \\cdot)\\) refers to a suitable distance metric.</p> <p>Predicting the label by looking to just the closest datapoint would overfit the model, so the predicted label is decided by the mode of the closest \\(k\\) datapoints to the input.</p> <p>[!warning] Multiple Modes</p> <p>If the mode are multiple labels (i.e. the distribution is multimodal), then the algorithm can choose the label with the most weight (i.e. the sum of the distances is the lowest) or just choose one at random.</p>"},{"location":"AI%20and%20ML/Unit%202/Supervised%20Learning/Nearest%20Neighbour/#choosing-k","title":"Choosing K","text":"<p>Choosing the right hyper-parameter \\(k\\) is important to avoid underfitting or overfitting the model. One heuristic rule of thumb is to set \\(k = \\sqrt{|\\mathcal D|}\\).</p> <p>A stable way to choose \\(k\\) is to divide the dataset into three parts: - training, which is stored to be used by the k-NN algorithm; - validation, which is used to fix \\(k\\); - testing, which is used to test the resulting model.</p> <p>[!tip] Splitting the dataset</p> <p>A good ratio to split the dataset into training / validation / testing would be 60% / 20% / 20%, but it's not a fixed rule. </p> <p>\\(k\\) is chosen in an empirical way: 1. pick a range of possible values for \\(k\\) (e.g. 1 - 20); 2. compute the accuracy (or the loss) on the validation set for each value in the \\(k\\)-range; 3. pick the \\(k\\) value from the range such that the accuracy is the highest (or the loss is the lowest).</p> <p>[!info] Overfit</p> <p>Setting \\(k=1\\) will overfit the model, because the model would choose the label on too few information (i.e. just the closest point). This could create area patches that are too specific to the training data.</p> <p></p> <p>[!info] Underfit</p> <p>Setting \\(k = |\\mathcal D|\\) will underfit the model, because the model would always choose the label with the highest frequency. This is called the best constant predictor.</p> <p></p> <p>Data Normalization</p>"},{"location":"AI%20and%20ML/Unit%202/Supervised%20Learning/Supervised%20Learning/","title":"Supervised Learning","text":"<p>Supervised learning is a type of machine learning in which an algorithm learns to make predictions or decisions by training on a labelled dataset, which consists of input data and the corresponding output label.</p> \\[\\large     \\underbrace{         \\set{ x_i, y_i }_{i=1}^N     }_\\text{known}     \\sim     \\underbrace { \\mathcal D }_\\text{unknown} \\] <p>The goal of supervised learning is to learn a mapping function from the input variables to the output variables. Once the mapping function is learned, it can be used to make predictions on new, unseen data.</p> <p>[!info] Prediction Types</p> <p>Based on the label, there exist two types of supervised learning prediction: - Classification, when the label is a discrete value, either binary (\\(0,1\\)) or multi-class (\\(1, \\ldots, N\\)); - Regression, when the output is a continuous value.</p> <p> </p>"},{"location":"AI%20and%20ML/Unit%202/Supervised%20Learning/Supervised%20Learning/#supervised-algorithms","title":"Supervised Algorithms","text":"<ul> <li>Nearest Neighbour</li> </ul>"},{"location":"AI%20and%20ML/Unit%202/Unsupervised%20Learning/Clustering/","title":"Clustering","text":"<p>Clustering is a machine learning technique that involves grouping similar data points together into clusters. It is unsupervised, meaning that it doesn't require any labelled data to be trained on.</p> <p>Clustering algorithms attempt to find patterns and similarities in the data based on the features of each data point. The resulting clusters can be used for a variety of purposes, such as identifying subgroups within a larger population, detecting anomalies or outliers in the data, or even as a pre-processing step for other machine learning tasks.</p> <p>[!tip] Principal Component Analysis</p> <p>PCA is a data analysis algorithm that may be run before applying any clustering algorithm: it helps by reducing the dimensionality of the input space while retaining most of the variance in the data (i.e. compressing the data) and by trying to reduce the noise in the input data.</p>"},{"location":"AI%20and%20ML/Unit%202/Unsupervised%20Learning/Clustering/#clustering-algorithms","title":"Clustering Algorithms","text":"<ul> <li> <p>K-means Clustering</p> </li> <li> <p>Gaussian Mixture Model</p> </li> </ul>"},{"location":"AI%20and%20ML/Unit%202/Unsupervised%20Learning/Gaussian%20Mixture%20Model/","title":"Gaussian Mixture Model","text":"<p>A Gaussian Mixture Model, a.k.a. GMM, is a statistical model that uses a mixture of Gaussian probability distributions to represent a pattern or set of data. It is often used for modelling complex data that cannot be accurately described by a single Gaussian distribution, and to cluster and classify datasets where the clusters can be approximated by Gaussian distributions.</p> <p>In this model, a data point is assumed to be generated from one of several Gaussian distributions, each with its own mean and covariance matrix. The probability density function of a GMM is a weighted sum of Gaussian distributions, where each component represents a cluster or subpopulation of the data.</p> <p>[!info]</p> <p>Each Gaussian distribution is called a component; the set of all means and covariances of the components are called the model's parameters.</p> <p>The goal of a GMM is to estimate the parameters of each Gaussian component, the number of components and the mixing weights that determine the proportion of each component in the mixture. GMMs are commonly used in pattern recognition (clustering), image segmentation, speech recognition, and other applications where data is inherently multimodal or has complex structure.</p> <p>[!tip] Dimensionality Reduction</p> <p>Another extremely helpful usage of GMM is dimensionality reduction: dimensionality reduction can be applied distinctively on each gaussian component, thus retaining the complex shape of the model.</p>"},{"location":"AI%20and%20ML/Unit%202/Unsupervised%20Learning/Gaussian%20Mixture%20Model/#multivariate-gaussian","title":"Multivariate Gaussian","text":"<p>A Multivariate Gaussian is a Gaussian distribution that lives in higher-dimensional spaces, a GMM is composed of multiples multivariate gaussians.</p> <p>Let \\(\\mu\\) be the mean of the distribution, \\(\\Sigma\\) the covariance matrix of dimensions \\(k \\times k\\), then the probability density function of a multivariate gaussian in \\(k\\) dimensions is defined as follows.</p> \\[\\large     f(x) =     \\frac{1}{ \\sqrt{(2 \\pi)^k \\det \\Sigma} }     \\exp\\left(         -\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu)     \\right) \\] <p>[!example] Multivariate Gaussian</p> <p>Given \\(k = 2\\), \\(\\mu = (0,0)\\), \\(\\Sigma = \\text{diag}(2,1)\\), the multivariate gaussian assumes the following pdf.</p> <p></p>"},{"location":"AI%20and%20ML/Unit%202/Unsupervised%20Learning/K-means%20Clustering/","title":"K means Clustering","text":"<p>\\(\\def \\seq#1#2{{ {#1}_1, {#1}_2, \\ldots, {#1}_{#2} }}\\) \\(\\def \\bb#1{{ \\mathbb #1 }}\\)</p>"},{"location":"AI%20and%20ML/Unit%202/Unsupervised%20Learning/K-means%20Clustering/#k-means-clustering","title":"K-means Clustering","text":"<p>Input</p> <ul> <li>Set of points \\(\\set\\seq {\\vec x} n\\) in a d-dim vector space (\\(\\vec x_i \\in \\mathbb R^d\\))</li> <li>\\(k\\) clusters to find</li> </ul> <p>Output</p> <ul> <li>A set of points called centroids \\(\\set\\seq \\mu k\\), where \\(\\mu_i \\in \\mathbb R^d\\)</li> <li>A set of assignment labels \\(\\set\\seq y n\\) that create a mapping from data points to centroids (i.e. \\(y_i\\) is the label for the \\(i\\)-th point \\(x_i\\))</li> </ul>"},{"location":"AI%20and%20ML/Unit%202/Unsupervised%20Learning/K-means%20Clustering/#lloyds-method","title":"Lloyd's Method","text":"<p>The most popular algorithm to perform K-means clustering is called Lloyd's method, it features three steps that are performed until convergence.</p> <ol> <li> <p>Initialization: \\(k\\) points are chosen at random from the set of all points, they will be the initial centroids \\(\\seq \\mu k\\).</p> </li> <li> <p>Assignment: for each point \\(\\vec x_i\\), compute the closet centroid and assign the label of the centroid to the point.</p> </li> <li> <p>Update: for each centroid \\(\\mu_j\\), compute the mean of all the points in the cluster. Then, update \\(\\mu_j\\) to be the computed mean.</p> </li> </ol> <p>Assignment and update (steps 2 and 3) are performed iteratively until all points are assigned again their previously (i.e. previous iteration) assigned centroid.</p> <p>[!warning] Empty clusters</p> <p>There are </p>"},{"location":"AI%20and%20ML/Unit%202/Unsupervised%20Learning/K-means%20Clustering/#furthest-first-heuristic","title":"Furthest-First Heuristic","text":"<p>The furthest-first heuristic is a method to select the centroids during the initialization step.</p> <p>The first centroid is picked randomly between all the points. Then, for each centroid to be found:</p> <ol> <li>Compute the distance from the point to all available centroids;</li> <li>Of the distances, pick the one to the closest centroid, i.e. the smaller one;</li> <li>Of all points closest to each centroid, pick the one with maximum distance;</li> <li>Mark that point as a new centroid.</li> </ol> <p>Formally, the mathematical formula for picking the next centroid is the following.</p> \\[\\large     m = \\arg {\\max}_m (         \\min_{k &lt; k'} || \\vec x_m - \\mu_k' ||_2^2     ) \\] <p>[!warning] Outliers</p> <p>Furthest-FIrst Heuristic is sensible to outliers: if a point is far removed from the other points, that point could become a cluster just on its own, instead of being part of a cluster.</p>"},{"location":"AI%20and%20ML/Unit%202/Unsupervised%20Learning/K-means%20Clustering/#k-means","title":"K-means++","text":"<p>K-means++ is another method to select the centroids during the initialization step.</p> <p>The logic is similar to Furthest-First Heuristic, but with a non-deterministic trait: instead of selecting the centroids from the points that are further away, centroids are selected by random sampling the set of closest points while maintaining the probability of the single point proportional to the distance.</p> <p>The first centroid is picked randomly between all the points. Then, for each centroid to be found:</p> <ol> <li>Compute the distance from the point to all available centroids;</li> <li>Of the distances, pick the one to the closest centroid, i.e. the smaller one;</li> <li>Turn all the found distances into a set of probability weights \\(p_i\\) by dividing each distance by the sum of all distances:</li> <li>Compute the cumulative distribution function \\(f : \\bb R \\rightarrow [0,1]\\);</li> <li>Pick a random value \\(y \\in [0,1]\\) and then pick, from the cumulative distribution, the first index \\(i\\) such that \\(y \\le f(i)\\);</li> <li>Mark the point \\(x_i\\) as the centroid.</li> </ol> <p>[!info] Inverse Transform Sampling</p> <p>This method to pick points proportionally to their distance to the closest centroid is called inverse transform sampling.</p>"},{"location":"AI%20and%20ML/Unit%202/Unsupervised%20Learning/Unsupervised%20Learning/","title":"Unsupervised Learning","text":"<p>Unsupervised learning is a type of machine learning in which an algorithm learns to make predictions or decisions by training on an unlabelled dataset, which consists of input data only, without classifying labels.</p> \\[\\large     \\underbrace{         \\set{ x_i }_{i=1}^N     }_\\text{known}     \\sim     \\underbrace { \\mathcal D }_\\text{unknown} \\] <p>The goal of unsupervised learning is to group together datapoints with common characteristics in an unsupervised environment, i.e. without initial classifiers such as labels, but using the provided training data only. Once an unsupervised model has been trained with a satisfactory accuracy, then the model can be queried to predict to which group a new input could belong to.</p>"},{"location":"AI%20and%20ML/Unit%202/Unsupervised%20Learning/Unsupervised%20Learning/#unsupervised-algorithms","title":"Unsupervised Algorithms","text":"<ul> <li> <p>K-means Clustering</p> </li> <li> <p>Gaussian Mixture Model</p> </li> </ul>"},{"location":"Algorithms/Algorithms/","title":"Algorithms","text":"<p>TK</p>"},{"location":"Algorithms/Algorithms/#table-of-contents","title":"Table of Contents","text":""},{"location":"Algorithms/Algorithms/A-Star%20Search/","title":"A* Search","text":""},{"location":"Algorithms/Algorithms/Best%20First%20Search/","title":"Best-First Search","text":"<p>Best-First Search is an informed search algorithm that explores a graph or state space by prioritizing nodes based on a heuristic evaluation function. It selects the most promising node to expand next, without considering the actual cost of reaching that node from the start.</p> <p>The heuristic function, denoted as \\(h(n)\\), provides an evaluation of how close a node is to the goal or how likely it is to lead to a solution. The algorithm selects the node with the highest heuristic value as the next node to expand.</p> <p>Here is a high-level overview of the Best-First Search algorithm:</p> <ol> <li> <p>Initialization: Set the start node as the current node and initialize the open set to contain only the start node.</p> </li> <li> <p>Main Loop: While the open set is not empty, perform the following steps:</p> <ul> <li>Select the node with the highest heuristic value from the open set as the current node.</li> <li>If the current node is the goal node or satisfies the desired condition, the search is complete. Return the solution or output the desired information.</li> <li>Expand the current node by generating its neighboring nodes.</li> <li>For each neighboring node, calculate its heuristic value using the heuristic function.</li> <li>If the neighboring node is not in the open set or the heuristic value is higher than the existing value, update its heuristic value and add it to the open set.</li> <li>Termination: If the open set becomes empty without finding a goal node or meeting the desired condition, the search fails. In this case, there is no solution.</li> </ul> </li> </ol> <p>Best-First Search is a greedy algorithm that prioritizes the most promising nodes based solely on their heuristic values. It does not consider the actual cost of reaching a node from the start, which means it may not always find the optimal solution. However, it can be highly efficient when the heuristic function provides accurate guidance towards the goal.</p> <p>The choice of heuristic function is crucial in Best-First Search, as it directly affects the search behavior and solution quality. The heuristic should be admissible (never overestimating the true cost) and consistent (monotonic with respect to the actual cost) to ensure desirable search properties.</p>"},{"location":"Algorithms/Algorithms/Best%20First%20Search/#best-first-search_1","title":"Best First Search","text":"<p>The best first search is an algorithm that shares similarities with breadth first search  and depth first search.</p> <p>Given a function \\(h(n)\\), where \\(n\\) is a node, the algorithm will always try to explore first the nodes with the smallest \\(h(n)\\) \\(min_{h(n)}\\set n_i\\)</p>"},{"location":"Algorithms/Algorithms/Best%20First%20Search/#greedy-best-first-search","title":"Greedy Best First Search","text":"<p>TK informed w/heuristic</p> <p>[!info] Informed Algorithm</p> <p>An informed algorithm has some kind of information such that the algorithm can follow some criteria (heuristic)</p>"},{"location":"Algorithms/Algorithms/Dijkstra%27s%20Algorithm/","title":"Dijkstra's Algorithm","text":"<p>Dijkstra's algorithm is a widely used algorithm for finding the shortest path in a graph from a given source node to all other nodes. It works for both weighted and unweighted graphs, but it is commonly used with non-negative edge weights. The algorithm uses a greedy approach to iteratively expand the frontier and update the shortest path distances to the nodes.</p>"},{"location":"Algorithms/Data%20Structures/Graphs/","title":"Graphs","text":"<p>A graph is a pair \\(G = (V, E)\\) where - \\(V\\) is a set whose elements are called vertices, - \\(E\\) is a set of pairs of vertices such that \\(E \\subseteq V ^2\\), the elements are called edges.</p>"},{"location":"Algorithms/Data%20Structures/Graphs/#types-of-graphs","title":"Types of Graphs","text":""},{"location":"Algorithms/Data%20Structures/Graphs/#directed-or-undirected-graphs","title":"Directed  or Undirected Graphs","text":"<p>[!abstract] Undirected Graph</p> <p>An undirected graph does not have an order, i.e. a single edge \\((v_1, v_2)\\) connects both \\(v_1\\) to \\(v_2\\) and \\(v_2\\) to \\(v_1\\), without order of traversing.</p> \\[\\large e = (v_1, v_2) = (v_2, v_1)\\] <p>[!abstract] Directed Graph</p> <p>A directed graph has an order, i.e. a single edge \\((v_1, v_2)\\) connects the vertex \\(v_1\\) to \\(v_2\\), but \\(v_2\\) may or may not be connected to \\(v_1\\).</p> \\[\\large e = (v_1, v_2) \\neq (v_2, v_1)\\]"},{"location":"Algorithms/Data%20Structures/Graphs/#connected-or-disconnected-graphs","title":"Connected or Disconnected Graphs","text":"<p>[!abstract] Connected or Disconnected Graphs</p> <p>A graph is said to be connected if there exists a path from any vertex \\(x\\) to another vertex \\(y\\), otherwise the graph is said to be disconnected.</p>"},{"location":"Algorithms/Data%20Structures/Graphs/#walk","title":"Walk","text":"<p>A walk of a graph from \\(v_0\\) to \\(v_n\\) is a sequence \\(v_0, v_1, \\ldots, v_n\\) such that \\((v_{i-1}, v_i) \\in E\\).</p> <p>[!note] Length</p> <p>The length of the walk is \\(n\\).</p> <p>If \\(v_0 = v_n\\), then the walk is said to be a closed walk.</p>"},{"location":"Algorithms/Data%20Structures/Graphs/#trail","title":"Trail","text":"<p>A trail of a graph is a walk in which the edges are not traversed more than once, i.e. \\((v_{i-1}, v_i) \\neq (v_{j-1}, v_j)\\) \\(\\forall i \\neq j\\).</p>"},{"location":"Algorithms/Data%20Structures/Graphs/#path","title":"Path","text":"<p>A path is a walk in which every node is not traversed more than once. Exceptionally, the starting node may be the ending node, in this case the path is said to be a cycle.</p> \\[\\large     v_0 \\neq \\cdots \\neq v_{n-1} \\]"},{"location":"Algorithms/Data%20Structures/Graphs/#directed-acyclic-graph","title":"Directed Acyclic Graph","text":"<p>If a graph presents no cycles, then the graph is said a Directed Acyclic Graph.</p>"},{"location":"Algorithms/Data%20Structures/Graphs/#labelled-graph","title":"Labelled Graph","text":"<p>A vertex-labelled graph is a tuple \\((V, E, L_V, \\mathcal l_V)\\) where - \\(V, E\\) are the sets of vertices and edges of a graph, - \\(L_V\\) is a finite set of symbols (labels), - \\(\\mathcal l_V\\) is a function \\(\\mathcal l_V : V \\rightarrow L_V\\).</p> <p>A edge-labelled graph is a tuple \\((V, E, L_E, \\mathcal l_E)\\) where - \\(V, E\\) are the sets of vertices and edges of a graph, - \\(L_E\\) is a finite set of symbols (labels), - \\(\\mathcal l_E\\) is a function \\(\\mathcal l_E : E \\rightarrow L_E\\).</p> <p>A labelled graph is a tuple \\((V, E, L_V, L_E, \\mathcal l_V, \\mathcal l_E)\\) where everything is as defined as above.</p> <p>[!note] Weighed Graph</p> <p>When the labels \\(L_V, L_E\\) belong to an ordered set, then the labelled graph is also said weighted graph.</p>"},{"location":"Algorithms/Data%20Structures/Tree/","title":"Tree","text":"<p>A tree is a directed connected graph such that:  </p> <ul> <li>there exists one and only one node from which every node is reachable (called root);  </li> <li>there exists one and only one path from the root to every other node (i.e. no loops or overlapping paths).</li> </ul> <p>The nodes with no outgoing edges are called leaves.</p>"},{"location":"Algorithms/Data%20Structures/Tree/#binary-tree","title":"Binary Tree","text":"<p>A binary tree is a tree in which the root is connected to at most two nodes, and the other nodes are connected to at most three other nodes.</p>"},{"location":"Algorithms/Data%20Structures/Tree/#chain","title":"Chain","text":"<p>A chain is a tree in which there exists one and only one leaf. This implies that every node, except for the root and the leaf, must be connected to two other nodes (with one ingoing and one outgoing edges).</p>"},{"location":"Computer%20Architecture/","title":"Index of Computer Architecture","text":""},{"location":"Computer%20Architecture/#first-semester","title":"First semester","text":"<ol> <li>Representing Numbers</li> <li>Binary Representation</li> <li>Integers Encoding</li> <li>Rationals Encoding</li> </ol>"},{"location":"Computer%20Architecture/Data%20Encoding/Binary%20Representation/","title":"Binary Representation","text":"<p>Using the system explained in Representing Numbers, we can define the binary code:</p> \\[ \\large b=2 \\quad \\Sigma = \\{0,1\\} \\] <p>The single digits 0 and 1 are called bit, contraction for binary digit.</p> <p>The pioneer of this \"language\" is George Boole, which realised an entire algebraic system on top of binary code. Moreover, there's a real correspondence between binary algebra and the behaviour of digital circuits, core logic of today's machines.</p> <p>Almost every piece of information can be represented (or approximated) in a way or another with binary code: - Integer numbers - Rational numbers - Alphabet characters - Images - Sounds</p>"},{"location":"Computer%20Architecture/Data%20Encoding/Integers%20Encoding/","title":"Integers Encoding","text":""},{"location":"Computer%20Architecture/Data%20Encoding/Integers%20Encoding/#binary-representation-of-naturals","title":"Binary Representation of Naturals","text":"<p>A natural number \\(N\\) in base 2 with \\(n\\) bits is represented by the formal formula</p> \\[ \\large  N_2 = \\sum^{n-1}_{i=0} c_i 2^i \\quad c_i \\in \\{0,1\\} \\] <p>The bit \\(\\large c_0\\) is called LSB (Less Signifing Bit, holds the lowest value), the bit \\(\\large c_{n-1}\\) is called MSB (Most Signifying Bit, holds the highest value).</p> <p>Ex. | Binary integer to decimal</p> \\[ \\begin{align} 111001_2 &amp;= (1 \\cdot 2^5 + 1 \\cdot 2^4 + 1 \\cdot 2^3 + 0 \\cdot 2^2 + 0 \\cdot 2^1 + 1 \\cdot 2^0)_{10} \\\\ &amp;= 57_{10} \\end{align} \\] <p>All the elementary operations in the same way as base-10, except for the subtraction. When we're subtracting one number from another, we always have to check that the first number is equal or greater than the second one, because we're working with naturals and we've yet to introduce negative numbers.</p>"},{"location":"Computer%20Architecture/Data%20Encoding/Integers%20Encoding/#binary-representation-of-integers","title":"Binary Representation of Integers","text":"<p>There are multiple ways to represent integers in binary: - Modulo and sign - One's complement - Two's complement</p> <p>The first two make operations complicated (preliminary checks on sign and values), while the last one allows for immediate sum and difference, so the two's complement method is the most used.</p> <p>In two's complement, the value of the sequence of digits \\(c_{n-1} \\cdots c_1 c_0\\) is given by the following expression:</p> \\[ \\large  N_{2-compl} = -c_{n-1} b^{n-1} \\sum^{n-2}_{i=0} c_i 2^i \\quad c_i \\in \\{0,1\\} \\] <p>To use this representation, it is important that we know the exact length of the sequence:</p> \\[ \\large \\begin{gather} 1101_{2-compl} \\text{ with 4 digits: } -2^3 + 2^2 + 1 = -3 \\\\ 1101_{2-compl} \\text{ with 5 digits: } 2^3 + 2^2 + 1 = 13 \\end{gather} \\] <p>The MSB is a sign indicator: - If set to 0, we're multiplying by 0 the only negative value, so the number is going to be positive; - If set to 1, the number is negative (the sum of all positive values is always smaller than the absolute value of the only negative coefficient, formally \\(\\large { b^{n-1} &gt; \\sum^{n-2}_{i=0} c_i b^i }\\))</p> <p>2-compl conversion To turn an integer \\(N\\) from base-10 to base-2 in 2-compl with \\(n\\) bits: - If \\(N\\ge0\\), use iterated divisions,     - if less than \\(n\\) bits are required, the number can be represented and we must 0-pad until \\(n\\) bits are used;     - otherwise the number cannot be represented using \\(n\\) bits. - If \\(N&lt;0\\), use iterated divisions on \\(-N\\),     - if less than \\(n\\) bits are required, the number can be represented and we must 0-pad until \\(n\\) bits are used, the number is the opposite of what we've got now;     - otherwise the number cannot be represented using \\(n\\) bits.</p> <p>Given a number \\(N_{2-compl}\\), its opposite can be found by performing a bitwise complement of \\(N\\) and then adding 1. </p> <p>Ex. | Numbers in 2-compl with 4 bits</p> <p>$$ \\begin{array}{rll}</p> <p>9: &amp; \\quad \\text{ 9 is coded as 1001, requires 4 bits} &amp;\\rightarrow \\neg\\text{ repr.}\\ 5: &amp; \\quad \\text{ 5 is coded as 101, requires 3 bits} &amp;\\rightarrow 0101\\ -3: &amp; \\quad \\text{-3 is coded as 0011, requires 4 bits} &amp;\\rightarrow 1101 \\ -9: &amp; \\quad \\text{ 9 is coded as 1001, requires 4 bits} &amp;\\rightarrow \\neg\\text{ repr.}\\</p> <p>\\end{array} $$</p>"},{"location":"Computer%20Architecture/Data%20Encoding/Iterated%20division%20proof/","title":"Iterated division proof","text":"<p>Let a number \\(N\\) of \\(n\\) digits in base \\(b\\) be represented by the Polynomial method:</p> \\[ \\large \\begin{align} N_b &amp;= c_0 + c_1 \\cdot b^1 + c_2 \\cdot b^2 + \\cdots + c_{n-2} \\cdot b^{n-2} + c_{n-1} \\cdot b^{n-1} \\\\ &amp;= c_0 + b \\cdot ( c_1 + c_2 \\cdot b^1 + \\cdots + c_{n-2} \\cdot b^{n-3} + c_{n-1} \\cdot b^{n-2} ) \\end{align} \\] <p>We can see that, by dividing by \\(b\\), we get a part of that number as quotient (the part in parenthesis), and another part (always less than \\(b\\)) as remainder (\\(c_0\\)).</p> <p>Those remainders, put together in the reverse order in which we got them, give us the representation of the number in base-\\(b\\).</p> <p>$$ \\large \\begin{align}</p> <p>N \\text{ mod } b &amp;= c_0 \\ N \\text{ div } b &amp;= c_1 + c_2 \\cdot b^1 + \\cdots + c_{n-2} \\cdot b^{n-3} + c_{n-1} \\cdot b^{n-2} = N^{(1)} \\ N^{(1)} \\text{ mod } b &amp;= c_1 \\ N^{(1)} \\text{ div } b &amp;= c_2 + c_3 \\cdot b^1 + \\cdots + c_{n-2} \\cdot b^{n-4} + c_{n-1} \\cdot b^{n-3} = N^{(2)} \\ N^{(2)} \\text{ mod } b &amp;= c_2 \\ N^{(2)} \\text{ div } b &amp;= c_3 + \\cdots + c_{n-2} \\cdot b^{n-5} + c_{n-1} \\cdot b^{n-4} = N^{(3)} \\ &amp;  \\vdots \\ N^{(n-1)} \\text{ mod } b &amp;= c_{n-1} \\ N^{(n-1)} \\text{ div } b &amp;= 0 \\</p> <p>\\end{align} $$</p> <p>Iterating like this, what we get is a sequence of remainders that, using the polynomial system back, give us the number we've started from. Basically, this is the inverse of the polynomial system.</p> <p>In fact, using the positional system the number is written as</p> \\[ \\large N_b = ( c_0 c_1 \\cdots c_{n-2} c_{n-1} )_b \\]"},{"location":"Computer%20Architecture/Data%20Encoding/Rationals%20Encoding/","title":"Rational  Encoding","text":"<p>We can't use the integer system to represent rational numbers, so we have to invent other systems.</p>"},{"location":"Computer%20Architecture/Data%20Encoding/Rationals%20Encoding/#fixed-point-notation","title":"Fixed Point Notation","text":"<p>It's still a positional system. The number is represented by \\(m+n\\) digits: - The first \\(m\\) digits are the integer part - The remaining \\(n\\) are the fractional part</p> <p>$$ \\large \\begin{gather}</p> <p>c_{m-1} \\cdots c_1 c_0, c_{-1} c_{-2} \\cdots c_{-n} \\quad c_i \\in {0,\\cdots,b-1} \\</p> <p>= \\sum^{m-1}{i=0} c_i b^i + \\sum^{-n}{i=-1} c_i b^i \\</p> <p>= \\sum^{m-1}{i=0} c_i b^i + \\sum^{n}{i=1} \\frac{c_{-i}}{b^i} \\</p> <p>\\end{gather} $$</p> <p>So the rational number \\(N\\) is a pair represented as \\(&lt;N_i,N_f&gt;\\), made up from an integer part \\(N_i\\) and a fractional one \\(N_f\\).</p>"},{"location":"Computer%20Architecture/Data%20Encoding/Rationals%20Encoding/#change-of-base","title":"Change of Base","text":"<p>To turn \\(&lt;N_i,N_f&gt;_a\\) into \\(&lt;N_i,N_f&gt;_b\\) :</p> <p>Integer part, same procedure as naturals. Fractional part, similar procedure: - If the arrival base is 10, use the </p>"},{"location":"Computer%20Architecture/Data%20Encoding/Representing%20Numbers/","title":"Representing Numbers","text":""},{"location":"Computer%20Architecture/Data%20Encoding/Representing%20Numbers/#information-encoding","title":"Information Encoding","text":"<p>When dealing with information, we need a mean to picture the information and work with it. In computer science, information can be represented through sequences of symbols taken from a fixed alphabet.</p> <p>Knowing this, we can define a code \\(C\\), which is a set of words formed by the symbols of an alphabet \\(\\Sigma\\), called support of \\(C\\).</p> <p>We can use a coding function to transmute some kind of information into a uniform type of coded data. A coding function is defined as \\(f : I \\rightarrow C\\), where \\(I\\) is a set of words and \\(C\\) is the set of encoded words (made up by the new alphabet).</p> <p>Ex. | Car --&gt; 00, Shuttle --&gt; 01, Airplane --&gt; 10</p> <p>We can also define a decoding function, to transmute back the coded data into understandable information. A decoding function is defined as \\(g : C \\rightarrow I\\), with the same meanings as the coding function. Typically, this function is the inverse of the coding function.</p> <p>Ex. | 00 --&gt; Car, 01 --&gt; Shuttle, 10 --&gt; Airplane</p>"},{"location":"Computer%20Architecture/Data%20Encoding/Representing%20Numbers/#numbers-encoding","title":"Numbers Encoding","text":"<p>When we use our numbers, what we're implicitly doing is using a positional numeric system. With this system, we have an alphabet \\(\\Sigma\\) made up of \\(b\\) distinct symbols, and every position in the sequence has a weight, a value. We can represent the total value of the number \\(N\\) (of \\(m\\) digits) with the formula</p> \\[ \\large N_b = \\sum^{m-1}_{i=0} c_i b^i \\quad c_i \\in \\Sigma \\] <p>The same number, represented with different alphabets, can hold different value</p> <p>Ex. | Decimal system (\\(b = 10,\\, \\Sigma = \\{0,\\cdots,9\\}\\))</p> \\[ \\begin{align} 254_{10} &amp;= 2 \\cdot 10^2 + 5 \\cdot 10^1 + 4 \\cdot 10^0 \\\\ &amp;= 200 + 50 + 4 \\end{align}\\] <p>Ex. | Septenary system (\\(b = 7,\\, \\Sigma = \\{0,\\cdots,6\\}\\))</p> \\[ \\begin{align} 254_{7} &amp;= (2 \\cdot 7^2 + 5 \\cdot 7^1 + 4 \\cdot 7^0)_{10} \\\\ &amp;= (98 + 35 + 4)_{10} \\\\ &amp;= 137_{10} \\end{align}\\] <p>These systems have some properties:</p> <ul> <li> <p>Representation interval The set of numbers we can represent with the current system. With \\(n\\) digits in base \\(b\\), the set of all representable numbers is \\(\\{0,\\cdots,b^n-1\\}\\), its size (\\(b^n\\)) is how many numbers we can represent.</p> </li> <li> <p>Representation length The number of how many digits we need to represent a number \\(N\\) in base \\(b\\). This number is calculated with the formula \\(\\lfloor{ \\log_b N }\\rfloor + 1\\)</p> </li> </ul>"},{"location":"Computer%20Architecture/Data%20Encoding/Representing%20Numbers/#polynomial-method","title":"Polynomial method","text":"<p>The formal formula for converting from base \\(a\\) to base \\(b\\) with this method (Polynomial method) is the following:</p> \\[ \\large N_{a} = \\sum^{n-1}_{i=0} c_i a^i \\quad c_i \\in \\{0, \\dots, a-1\\} \\] <p>With this, we're expressing \\(N_a\\) as a polynomium using the digits from the alphabet of \\(b\\), then evaluating the expression using arithmetics in base \\(b\\).</p> <p>Ex. | Number from base-2 to base-10</p> \\[ \\begin{align} 111001_2 &amp;= (1 \\cdot 2^5 + 1 \\cdot 2^4 + 1 \\cdot 2^3 + 0 \\cdot 2^2 + 0 \\cdot 2^1 + 1 \\cdot 2^0)_{10} \\\\ &amp;= (32 + 16 + 8 + 0 + 0 + 1)_{10} \\\\ &amp;= 57_{10} \\end{align} \\] <p>This method, though, it's difficult to use if the final base \\(b\\) is not base-10 (you'd have to use arithmetic rules you're not used to, arithmetics modulo \\(b\\)), so it'd be best to use other methods in this case.</p> <p>Ex. | Number from base-7 to base-2</p> \\[ \\begin{align} 3602_7 &amp;= (10 \\cdot 21^3 + 20 \\cdot 21^2 + 0 \\cdot 21^1 + 2 \\cdot 21^0)_3 \\\\ \\end{align} \\]"},{"location":"Computer%20Architecture/Data%20Encoding/Representing%20Numbers/#iterated-divisions-method","title":"Iterated divisions method","text":"<p>There exists a proven theorem, the theorem of the Euclidean Division, that states:</p> \\[ \\begin{gather} \\text{For every } D,d \\in \\mathbb{N} \\text{ there exists a unique pair } q,r \\in \\mathbb{N} \\\\ \\text{ s.t. } D = q \\cdot d + r \\text{, with } 0 \\le r &lt; d \\end{gather} \\] <p>What this means, really, is that for every number \\(D\\) and a divisor \\(d\\) there is a quotient \\(q\\) and a reminder \\(r\\) :</p> \\[ \\begin{align} q &amp;= D \\ \\text{div} \\ d \\\\ r &amp;= D \\ \\text{mod} \\ d \\end{align} \\] <p>Ex. | Number 7, divisor 3</p> \\[ \\begin{align} q &amp;= 7 \\ \\text{div} \\ 3 = 2\\\\ r &amp;= 7 \\ \\text{mod} \\ 3 = 1 \\end{align} \\] <p>The proof is quite long, so here it is.</p> <p>So, to convert a number \\(N_a\\) in base \\(b\\) : 1. Repeatedly divide \\(N_a\\) for \\(b_a\\) (\\(\\large b\\) must be expressed in base \\(\\large a\\) and the division must be done in base \\(\\large a\\)); 2. The remainders of the division, converted in base \\(b\\), give us the digits, from less signifying to most one, of  \\(N_a\\) expressed in base \\(b\\).</p> <p>Ex. | 675 (base-10) to base-4</p> \\[ \\begin{align} 657 &amp;: 4 = 164 &amp;&amp;\\text{ remainder } 1 \\\\ 164 &amp;: 4 = 41  &amp;&amp;\\text{ remainder } 0 \\\\  41 &amp;: 4 = 10  &amp;&amp;\\text{ remainder } 1 \\\\  10 &amp;: 4 = 2   &amp;&amp;\\text{ remainder } 2 \\\\   2 &amp;: 4 = 0   &amp;&amp;\\text{ remainder } 2 \\\\ \\end{align} $$ $$ \\rightarrow 675_{10} = 22101_4 \\] <p>Ex. | 317 (base-10) to base-16</p> \\[ \\begin{align} 317 &amp;: 16 = 19 &amp;&amp;\\text{ remainder D } (13) \\\\  19 &amp;: 16 = 1  &amp;&amp;\\text{ remainder } 3 \\\\   1 &amp;: 16 = 0  &amp;&amp;\\text{ remainder } 1 \\\\ \\end{align} $$ $$ \\rightarrow 317_{10} = 13\\text{D}_4 \\] <p>This method, though, it's difficult to use if the starting base \\(b\\) is not base-10 (you'd have to use arithmetic rules you're not used to, arithmetics modulo \\(b\\)), so it'd be best to use other methods in this case.</p>"},{"location":"Computer%20Architecture/Data%20Encoding/Representing%20Numbers/#mixed-method","title":"Mixed method","text":"<p>To convert a number \\(N_a\\) in base \\(b\\), where \\(a,b\\ne10\\), it'd be best to use both methods at once: first convert from base-\\(a\\) to base-10, then convert the result to base-\\(b\\).</p> <p>Ex. | 102202 (base-3) to base-5</p> \\[ 102202_3 = (3^5 + 2 \\cdot 3^3 + 2 \\cdot 3^2 + 2)_{10} = 317_{10} \\] \\[ \\begin{align} 317 &amp;: 5 = 63 &amp;&amp;\\text{ remainder } 2 \\\\  63 &amp;: 5 = 12 &amp;&amp;\\text{ remainder } 3 \\\\  12 &amp;: 5 = 2  &amp;&amp;\\text{ remainder } 2 \\\\   2 &amp;: 5 = 0  &amp;&amp;\\text{ remainder } 2 \\\\ \\end{align} \\] \\[ \\rightarrow 102202_3 = 2232_5 \\]"},{"location":"Computer%20Architecture/Data%20Encoding/Representing%20Numbers/#particular-bases","title":"Particular bases","text":"<p>When converting from base-\\(a\\) to base-\\(b\\), we can have particular cases where \\(b = a^k\\) or \\(a = b^k\\)</p> <p>In the case base-\\(a\\) --&gt; base-\\(a^k\\) we have that</p> \\[ \\large \\begin{align} c_{n-1} \\cdots c_1 c_0 \\text{ mod } a^k &amp;= c_{k-1} \\cdots c_0 \\\\ c_{n-1} \\cdots c_1 c_0 \\text{ div } a^k &amp;= c_{n-1} \\cdots c_k \\\\ \\end{align} \\] <p>So, if \\(b = a^k\\) and \\({ N_a = c_{n-1} \\cdots c_1 c_0 }\\), then the number in base-\\(b\\) is</p> \\[ \\large (c_{n-1} \\cdots c_{hk})_b \\cdots (c_{3k-1} \\cdots c_{2k})_b (c_{2k-1} \\cdots c_k)_b (c_{k-1} \\cdots c_{2k})_b \\] <p>Ex. | 453 (base-10) to base-100  (\\(10^2\\))</p> \\[ \\large \\begin{align} 403_{10} \\text{ mod } 100 &amp;= 3 \\\\ 403_{10} \\text{ div } 100 &amp;= 4 \\\\ \\end{align} $$ $$ \\large \\rightarrow 403_{10} = 43_{100}\\] <p>Ex. | 1000111101 (base-2) to base-4 (\\(2^2\\))</p> \\[ \\large \\begin{array}{rcccccl}  ( &amp; 10 &amp; 00 &amp; 11 &amp; 11 &amp; 01 &amp; )_2 \\\\ =( &amp;  2 &amp;  0 &amp;  3 &amp;  3 &amp;  1 &amp; )_4 \\\\ \\end{array} \\] <p>Ex. | 1000111101 (base-2) to base-8 (\\(2^3\\))</p> \\[ \\large \\begin{array}{rccccl}  ( &amp; 1 &amp; 000 &amp; 111 &amp; 101 &amp; )_2 \\\\ =( &amp; 1 &amp;  0  &amp;  7  &amp;  5  &amp; )_8 \\\\ \\end{array} \\] <p>In the case base-\\(a^k\\) --&gt; base-\\(a\\) we just have to convert the single digits into the lower base:</p> \\[ \\large (c_{n-1})_b \\cdots (c_2)_b (c_1)_b (c_0)_b \\] <p>Ex. | 8315 (base-9) to base-3 (\\(3^2\\))</p> \\[ \\large \\begin{array}{rccccl}  ( &amp;  8 &amp;  3 &amp;  1 &amp;  5 &amp; )_9 \\\\ =( &amp; 22 &amp; 10 &amp; 01 &amp; 12 &amp; )_3 \\\\ \\end{array} \\] <p>Ex. | 8D3A (base-16) to base-2 (\\(2^4\\))</p> \\[ \\large \\begin{array}{rccccl}  ( &amp;    8 &amp;    D &amp;    3 &amp;    A &amp; )_{16} \\\\ =( &amp; 1000 &amp; 1101 &amp; 0011 &amp; 1010 &amp; )_2    \\\\ \\end{array} \\]"},{"location":"Computer%20Vision%20and%20NLP/","title":"Computer Vision and NLP","text":"<p>Computer Vision is that branch of Computer Science that tries to emulate the human visual system on digital systems, its aim is to understand the contents of a scene.</p> <p>Natural Language Processing</p> <p>Aim of the course: Understand what do you need to solve a Computer Vision or NLP problem, and design or use the solution.</p>"},{"location":"Computer%20Vision%20and%20NLP/#course-outline-tentative","title":"Course Outline (Tentative)","text":"<ul> <li>Introduction to Image Processing</li> <li>Introduction to Image Processing Tools</li> <li>Machine/Deep Learning Recap</li> <li>Introduction to Machine Learning Tools</li> <li>Introduction to Computer Vision</li> <li>Introduction to Computer Vision/Deep Learning Tools</li> <li>Introduction to NLP</li> </ul>"},{"location":"Computer%20Vision%20and%20NLP/#tools","title":"Tools","text":"<ul> <li>Python / Anaconda</li> <li>OpenCV (Image Processing)</li> <li>Scikit Learn (Machine Learning)</li> <li>Pytorch (Deep Learning)</li> <li>Jupyter Notebook</li> </ul>"},{"location":"Computer%20Vision%20and%20NLP/#table-of-contents","title":"Table of Contents","text":"<p>NumPy</p> <ul> <li>Matrices</li> </ul> <p>OpenCV</p> <ul> <li>OpenCV useful functions</li> </ul> <p>CV and NLP</p> <ul> <li>Image Processing</li> </ul>"},{"location":"Computer%20Vision%20and%20NLP/Image%20Processing/","title":"Image Processing","text":"<p>TK</p>"},{"location":"Computer%20Vision%20and%20NLP/NumPy/NumPy%20Functions/","title":"NumPy Functions","text":""},{"location":"Computer%20Vision%20and%20NLP/NumPy/NumPy%20Functions/#zeroes","title":"Zeroes","text":"<p>The function <code>np.zeros</code> creates a matrix initialized with zeroes.</p> <p>The prototype of the function is <code>np.zeros(sizes)</code>, where:</p> <ul> <li><code>sizes</code> is an array containing the sizes of the matrix. The first two elements represent the number of rows and columns respectively, the third element represents the number of channels, which if not provided defaults to 1 (grayscale).</li> </ul>"},{"location":"Computer%20Vision%20and%20NLP/NumPy/NumPy%20Matrix/","title":"NumPy Matrix","text":"<p>TK matrix</p> <p>TK slicing</p>"},{"location":"Computer%20Vision%20and%20NLP/NumPy/NumPy/","title":"NumPy","text":"<p>TK</p>"},{"location":"Computer%20Vision%20and%20NLP/OpenCV/OpenCV%20Tips%20and%20Functions/","title":"OpenCV Tips and Functions","text":"<p>TK</p>"},{"location":"Computer%20Vision%20and%20NLP/OpenCV/OpenCV%20Tips%20and%20Functions/#tips","title":"Tips","text":"<p>[!tip] Alpha Channel</p> <p>Generally, OpenCV removes the alpha channel when loading an image. To enable it back, (TK)</p> <p>[!warning] Colour Format</p> <p>When using the <code>rgb</code> colour format, the pixels are represented as a tuple using the inverted format <code>(b,g,r)</code>. For example, the pixel <code>(0,0,255)</code> represents a full intensity red.</p>"},{"location":"Computer%20Vision%20and%20NLP/OpenCV/OpenCV%20Tips%20and%20Functions/#functions","title":"Functions","text":"<p>TK imread imwrite</p>"},{"location":"Computer%20Vision%20and%20NLP/OpenCV/OpenCV%20Tips%20and%20Functions/#imshow","title":"imshow","text":"<p>The <code>cv2.imshow</code> function opens a window containing an image.</p> <p>The prototype of the function is <code>cv2.imshow(window, img)</code>, where:</p> <ul> <li><code>window</code> is the title of the window;</li> <li><code>img</code> is the matrix representing the image.</li> </ul>"},{"location":"Computer%20Vision%20and%20NLP/OpenCV/OpenCV%20Tips%20and%20Functions/#split","title":"split","text":"<p>The <code>cv2.split</code> function splits the different channels into the grayscale components of each channel.</p> <p>The prototype of the function is <code>cv2.split(img)</code>, where:</p> <ul> <li><code>img</code> is the matrix representing the image.</li> </ul> <p>[!tip] Split vs NumPy slicing</p> <p>The <code>merge</code> function has an advantage over NumPy slicing only if all the grayscale components of the channels of an image are needed. If just a subset of the channels are needed, generally NumPy slicing is faster.</p>"},{"location":"Computer%20Vision%20and%20NLP/OpenCV/OpenCV%20Tips%20and%20Functions/#merge","title":"merge","text":"<p>The <code>cv2.merge</code> function merges different image channels into a single image.</p> <p>The prototype of the function is <code>cv2.merge(channels)</code>, where:</p> <ul> <li><code>channels</code> is a tuple containing the channels to merge.</li> </ul>"},{"location":"Computer%20Vision%20and%20NLP/OpenCV/OpenCV%20Tips%20and%20Functions/#line","title":"line","text":"<p>The <code>cv2.line</code> function draws a line over an image.</p> <p>The prototype of the function is <code>cv2.line(img, start, end, color, thickness)</code>, where:</p> <ul> <li><code>img</code> is the matrix containing the image;</li> <li><code>start</code> is the point <code>(x,y)</code> where the line should start;</li> <li><code>end</code> is the point <code>(x,y)</code> where the line should end;</li> <li><code>color</code> is a tuple <code>(b,g,r)</code> containing the colour of the line;</li> <li><code>thickness</code> is the thickness of the line.</li> </ul>"},{"location":"Computer%20Vision%20and%20NLP/OpenCV/OpenCV%20Tips%20and%20Functions/#rectangle","title":"rectangle","text":"<p>The <code>cv2.rectangle</code> function draws a rectangle over an image.</p> <p>The prototype of the function is <code>cv2.rectangle(img, start, end, color, thickness)</code>, where:</p> <ul> <li><code>img</code> is the matrix containing the image;</li> <li><code>start</code> are the coordinates <code>(x,y)</code> of the top-left corner of the rectangle;</li> <li><code>end</code> are the coordinates <code>(x,y)</code> of the bottom-right corner of the rectangle;</li> <li><code>color</code> is a tuple <code>(b,g,r)</code> containing the colour of the rectangle;</li> <li><code>thickness</code> is the thickness of the border.</li> </ul> <p>[!tip] Thickness</p> <ul> <li>If the thickness has a positive value, then the rectangle will be hollow and the border will have the specified thickness.</li> <li>If the thickness equals to \\(-1\\), then the rectangle will be filled.</li> </ul>"},{"location":"Computer%20Vision%20and%20NLP/OpenCV/OpenCV%20Tips%20and%20Functions/#circle","title":"circle","text":"<p>The <code>cv2.circle</code> function draws a circle over an image.</p> <p>The prototype of the function is <code>cv2.rectangle(img, center, radius, color, thickness)</code>, where:</p> <ul> <li><code>img</code> is the matrix containing the image;</li> <li><code>center</code> are the coordinates <code>(x,y)</code> of the centre of the circle;</li> <li><code>radius</code> is the radius of the circle;</li> <li><code>color</code> is a tuple <code>(b,g,r)</code> containing the colour of the circle;</li> <li><code>thickness</code> is the thickness of the border.</li> </ul> <p>[!tip] Thickness</p> <ul> <li>If the thickness has a positive value, then the circle will be hollow and the border will have the specified thickness.</li> <li>If the thickness equals to \\(-1\\), then the circle will be filled.</li> </ul>"},{"location":"Computer%20Vision%20and%20NLP/OpenCV/OpenCV/","title":"OpenCV","text":"<p>OpenCV</p> <p>TK open computer vision</p>"},{"location":"Data%20Management%20and%20Analysis/","title":"Data Management and Analysis","text":"<p>TK</p>"},{"location":"Data%20Management%20and%20Analysis/#table-of-contents","title":"Table of Contents","text":""},{"location":"Data%20Management%20and%20Analysis/#unit-1","title":"Unit 1","text":"<ul> <li>Data</li> <li>Information Systems</li> </ul> <p>Relational</p> <ul> <li>Relational Model</li> <li>Relational Algebra</li> <li>Functional Dependencies</li> <li>Keys</li> <li>Third Normal Form</li> <li>Closure of Functional Dependencies</li> <li>Closure of Attributes</li> <li>Decomposition of Relations</li> </ul> <p>Database</p> <ul> <li>Schemas</li> <li>Constraints</li> <li>Schema Anomalies</li> </ul>"},{"location":"Data%20Management%20and%20Analysis/#unit-2","title":"Unit 2","text":"<p>Empty, for now...</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Data/","title":"Data","text":"<p>Data is a collection of discrete values that convey information, describing quantity, quality, fact, statistics, other basic units of meaning, or simply sequences of symbols that may be further interpreted.</p> <p>Data can be stored in two ways: - Structured, data is represented with a fixed format (strings of symbols or numbers, like the table above); - Unstructured, data is written an natural text.</p> <p>[!tip] Data vs Information Data is different from information because data lacks context, whereas information can be seen as contextualized data.</p> <p>[!example] Information Example</p> <p>We can say that the following table contains information, because with the rows is provided an heading, which contextualizes the various attributes.</p> Name Surname Exams Average Mario Bianchi 10 28.5 Paolo Rossi 2 26.5"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Information%20Systems/","title":"Information Systems","text":"<p>An information system is an integrated set of components for collecting, storing, and processing data and for providing information and knowledge. An information system must allow its users actions such creation, updating and interrogation of such information.</p> <p>The notion of information system is independent of the actual implementation, and have been since long used (e.g. medical records, phone records). Nowadays, information systems are implemented through databases.</p> <p>%% TK link application software of sysnet u1 %%</p> <p>The list of components of a digital information system is the following: - Database (DB) - Database Management System (DBMS) - Application Software (to access the DB) - Computer Hardware (storage devices) - Personnel developing, managing or using the system</p> <p>%% TK models pg. 16 %%</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Database/Constraints/","title":"Constraints","text":"<p>Sometimes, real contexts that are represented by relations have some kind of constraints which must be respected.</p> <p>Integrity constraints are properties that must be satisfied by every instance of the relation/database. A relation instance is correct (or legal) if it satisfies every constraint associated with its schema.</p> <p>Two categories of constraint can be defined: - Intra-relational, defined on the domain of a single attribute, on the values of the same tuple or on tuples of the same relation; - Inter-relational, defined between multiple relations.</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Database/Constraints/#domain-constraints","title":"Domain constraints","text":"<p>Defined on the domain of an attribute, bounds the domain to a subset of itself.</p> <p>[!example] Example 1. <code>YEAR_BIRTH &gt; 1980</code> 2. <code>(GRADE &gt;= 18) AND (GRADE &lt;= 31)</code></p> <p>The value existence constraint is a special type of domain constraint, it forbids the use of NULL values. - <code>GRADE NOT NULL</code></p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Database/Constraints/#tuple-constraints","title":"Tuple constraints","text":"<p>Defined on the values of the same tuple, bounds the possible combinations the tuple can assume.</p> <p>Examples: -  <code>(GRADE = 31) OR NOT (HONOUR = true)</code></p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Database/Constraints/#uniqueness-constraints","title":"Uniqueness constraints","text":"<p>Defined on an attribute, it makes it so that when a tuple's specified attribute assumes a value, any other tuple can't assume the same value under the same attribute.</p> <p>Examples: -  <code>USER_ID UNIQUE</code></p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Database/Constraints/#primary-key-constraint","title":"Primary key constraint","text":"<p>It is a shorthand to include both uniqueness constraint and value existence constraint, based on the concept of key.</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Database/Constraints/#referential-constraint","title":"Referential constraint","text":"<p>Also called foreign key constraint. It is defined on a domain of an attribute, it makes it so that the domain of an attribute \\(A\\) of a relation \\(R\\) is the set of all values assumed by an attribute \\(A'\\) of an instance on a relation \\(R'\\).</p> <p>[!example] Example <code>DEPT REFERENCES DEPARTMENT.NUMBER</code> \\(A = \\texttt{DEPT},\\ R' = \\texttt{DEPARTMENT},\\ A' = \\texttt{NUMBER}\\)</p> <p>A referential integrity constraint between the \\(X\\) attributes of a relation \\(R\\) and another relation \\(R'\\) forces the values on \\(X\\) in \\(R\\) to appear as values of the primary key of \\(R\\).</p> <p>[!tip] Null Values By allowing null values, some constrains can be made less restrictive (e.g. on a unique attribute, multiple tuples can have a null value for that attribute).</p> <p>It is possible to define certain behaviours to mitigate violations of constraints, e.g. what to do when a tuple referenced by another tuple (foreign key) is removed.</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Database/Databases/","title":"Databases","text":"<p>%% TK definition %%</p> <p>Constraints</p> <p>Schemas</p> <p>A DBMS allows: - Definition of constraints - Verification of constraints - Prevention of violation of previously defined constraints</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Database/Schema%20Anomalies/","title":"Schema Anomalies","text":"<p>Take as example the following schema and instance.</p> <p><code>Curriculum (Matr, TC, SurN, Name, BirthD, City, Prov, C#, Tit, Doc, Date, Grade)</code></p> Matr TC SurN Name BirthD City Prov C# Tit Doc Date Grade 01 ... Rossi Mario ... Tolfa Rome 10 Physics Goofy ... ... 02 ... Bianchi Paolo ... Tolfa Rome 10 Physics Goofy ... ... 01 ... Rossi Mario ... Tolfa Rome 20 Chemistry Pluto ... ... <p>Multiple anomalies can be noted. In detail, - Redundancy: student's biographical data is stored for each exam taken by the student; course data is stored for each exam taken for that course. - Update anomaly: if the professor of a course changes, the information has to be modified for each exam of that course</p> <p>%% TK continue - slides 06 (good schema) %%</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Database/Schemas/","title":"Schemas","text":"<p>A schema is the base structure of relations and databases, it defines what types of data are stored.</p> <p>There are two types of schemas: the internal schema, the one databases use to store data on files (unknown to the end user), and the external schema, the outer layer of databases (the one end users use).</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Database/Schemas/#external-schema","title":"External Schema","text":"<p>It's the outer layer of a database, the one a normal user interacts with; it is a portion of the database which is coincident to relations, it is also called relation schema.</p> <p>[!abstract] Notation - \\(R = \\{A_1, A_2, \\ldots, A_n\\}\\) - The first letters of the alphabet \\(A, B, \\ldots\\) denote single attributes - The last letters of the alphabet \\(X, Y, \\ldots\\) denote subsets of attributes - If \\(X, Y\\) denote two set of attributes, then \\(XY\\) defines the union of the two</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Closure%20of%20Attributes/","title":"Closure of Attributes","text":"<p>\\(\\def \\clj#1{{ #1^+ }}\\) \\(\\def \\clj#1#2{{ #1_#2^+ }}\\)</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Closure%20of%20Attributes/#closure-of-attributes","title":"Closure of Attributes","text":"<p>The closure of a set of attributes \\(X\\) with respect to \\(F\\), denoted by \\(X^+\\) or more explicitly \\(\\clj X F\\), is the set that contains every attribute \\(A\\) determined by \\(X\\), i.e.</p> \\[\\large     \\clj X F = \\set{ A \\st X \\rightarrow A \\in F^A } \\] <p>[!tip] Trivial Dependency</p> <p>By the reflexivity axiom, \\(X \\rightarrow X \\Rightarrow X \\in \\clj X F\\)</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Closure%20of%20Attributes/#finding-the-closure-of-attributes","title":"Finding the Closure of Attributes","text":"<p>There exists an algorithm to find the closure of a set of attributes \\(X\\).</p> <p>[!quote] Algorithm</p> <p>\\(\\text{Begin}\\)</p> <p>\\(\\quad \\text{Let } Z = X\\)</p> <p>\\(\\quad \\text{Let } S = \\set{A \\in V \\mid Y \\rightarrow V \\in F \\ \\land \\ Y \\subseteq Z}\\)</p> <p>\\(\\quad \\text{While } S \\not\\subseteq Z \\text{ then:}\\)</p> <p>\\(\\quad \\quad Z = Z \\cup S\\)</p> <p>\\(\\quad \\quad S = \\set{A \\in V \\mid Y \\rightarrow V \\in F \\ \\land \\ Y \\subseteq Z}\\)</p> <p>\\(\\quad \\text{Endwhile}\\)</p> <p>\\(\\text{End}\\)</p> <p>The algorithm takes in input the set of attributes \\(X\\), its closure at the end of the algorithm is given by the set \\(Z\\).</p> <p>The logic of the algorithm is to start with the attributes of \\(X\\), and iteratively find attributes, contained in \\(S\\), such that they're defined by transitive dependencies of \\(X\\), i.e. (during an iteration) by a subset of the set \\(Z\\). The algorithm stops when no more attributes are found (i.e. no more attributes are added to \\(S\\), which means that after \\(Z = Z \\cup S\\) the loop will end).</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Closure%20of%20Attributes/#checking-keys-with-closure","title":"Checking Keys with Closure","text":"<p>The closure of a set of attributes \\(X\\) can be used to determine if \\(X\\) is a key of a relation:</p> <ol> <li>if the closure \\(X^+\\) equals to \\(R\\), then \\(X \\rightarrow R\\) and \\(X\\) is a superkey;</li> <li>if there's no proper subset \\(X' \\subset X\\) such that \\(X' \\rightarrow R\\), then \\(X\\) is a candidate key.</li> </ol> <p>[!tip] Tip</p> <p>If one or more attributes aren't functionally determined (i.e. don't appear on the right side of any functional dependency), then they must be part of a key.</p> <p>This can help reduce the set of all possible keys to check.</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Closure%20of%20Functional%20Dependencies/","title":"Closure of Functional Dependencies","text":"<p>\\(\\def \\clj#1{{ #1^+ }}\\) \\(\\def \\clj#1#2{{ #1_#2^+ }}\\)</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Closure%20of%20Functional%20Dependencies/#closure-of-functional-dependencies","title":"Closure of Functional Dependencies","text":"<p>A legal instance \\(r\\) of \\(R\\) is ensured to satisfy all the functional dependencies in \\(F\\), but it could satisfy additional functional dependencies that aren't included in \\(F\\).</p> <p>[!note] Closure of \\(F\\)</p> <p>The set of all functional dependencies satisfied by each legal instance of \\(R\\), even the ones not in \\(F\\), is called the closure of \\(F\\), which is denoted by \\(F^+\\).</p> <p>[!tip] \\(F \\subseteq F^+\\)</p> <p>Because \\(F^+\\) contains every functional dependency satisfied by legal instances of \\(R\\), then \\(F^+\\) must satisfy every functional dependency contained in \\(F\\), i.e.</p> \\[\\large   XY \\in F^+ \\quad   \\forall XY \\in F   \\Rightarrow   F \\subseteq F^+ \\]"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Closure%20of%20Functional%20Dependencies/#trivial-functional-dependencies","title":"Trivial Functional Dependencies","text":"<p>Given two subsets \\(X, Y\\) such that \\(Y \\subset X \\subseteq R\\), every legal instance \\(r\\) of \\(R\\) must satisfy the (so called) trivial functional dependency \\(X \\rightarrow Y\\).</p> <p>Even if not defined, trivial functional dependencies are satisfied by any relation, hence if \\(Y \\subset X\\) then \\(X \\rightarrow Y \\in F^+\\).</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Closure%20of%20Functional%20Dependencies/#equivalence-of-functional-dependencies","title":"Equivalence of Functional Dependencies","text":"<p>Two sets of functional dependencies are said to be equivalent (\\(F \\equiv G\\)) if their closures are the same, i.e. \\(F^+ = G^+\\).</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Functional%20Dependencies/","title":"Functional Dependencies","text":"<p>Given a relation schema \\(R\\), a functional dependency on \\(R\\) is an ordered pair of non-empty subsets \\(X,Y \\in R\\), with notation \\(X \\rightarrow Y\\) (sometimes \\(XY\\)), such that</p> \\[\\large     t_1[X] = t_2[X] \\Rightarrow t_1[Y] = t_2[Y]     \\quad\\quad \\forall t_1, t_2 \\in r \\] <p>where \\(r\\) is an instance of \\(R\\).</p> <p>[!example] Example If two tuples have the same matriculation number (so they refer to the same student), then they must also contain the same name and surname.</p> ID Name Surname ... Grade 200 Emanuele Scaccia ... 28 146 Mario Rossi ... 30 200 Emanuele Scaccia ... 29 <p>In this case, \\(\\set\\text{ID} \\rightarrow \\set{ \\text{Name}, \\text{Surname} }\\).</p> <ul> <li>The set of all functional dependencies defined on \\(R\\) is called \\(F\\) ;</li> <li>\\(r\\) is said to be legal if it satisfies all the functional dependencies in \\(F\\).</li> </ul> <p>[!tip] Constraints Functional dependencies are another way to express some types of constraints.</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Functional%20Dependencies/#armstrongs-axioms","title":"Armstrong's Axioms","text":"<p>Armstrong's axioms define new functional dependencies from the ones that already exist.</p> <p>The three axioms are:</p> <ul> <li>reflexivity axiom;</li> <li>augmentation axiom;</li> <li>transitivity.</li> </ul> <p>Moreover, three additional rules can be derived from the axioms:</p> <ul> <li>union rule;</li> <li>decomposition rule;</li> <li>pseudo-transitivity rule.</li> </ul> <p>By iteratively applying the axioms and rules to \\(F\\), a new set of functional dependencies \\(F^A\\) can be constructed, which will contain additional functional dependencies that were not contained in \\(F\\).</p> <p>[!tip] Closure of \\(F\\)</p> <p>When no other functional dependencies can be found by applying Armstrong's axioms on \\(F^A\\), then \\(F^A\\) will be equal to the closure of \\(F\\) (\\(F^+\\)).</p> <p>[!note] Reflexivity Axiom</p> <p>All functional dependencies \\(X \\rightarrow Y\\), defined for every \\(Y \\subset X\\), belong to \\(F^A\\).</p> \\[\\large   X \\rightarrow Y \\in F^A   \\quad \\quad   \\forall Y \\subset X \\] <p>[!note] Augmentation Axiom</p> <p>All functional dependencies \\(XZ \\rightarrow YZ\\), defined for every \\(X \\rightarrow Y \\in F\\), belong to \\(F^A\\).</p> \\[\\large   XZ \\rightarrow YZ \\in F^A   \\quad \\quad   \\forall X \\rightarrow Y \\in F,\\ Z \\in R \\] <p>[!note] Transitivity Axiom</p> <p>All functional dependencies \\(X \\rightarrow Z\\), defined for every couple \\(X \\rightarrow Y, Y \\rightarrow Z \\in F\\), belong to \\(F^A\\).</p> \\[\\large   X \\rightarrow Z \\in F^A   \\quad \\quad   \\forall X \\rightarrow Y,\\ Y \\rightarrow Z \\in F \\] <p>[!note] Union Rule</p> <p>All functional dependencies \\(X \\rightarrow YZ\\), defined for every couple \\(X \\rightarrow Y, X \\rightarrow Z \\in F\\), belong to \\(F^A\\).</p> \\[\\large   X \\rightarrow YZ \\in F^A   \\quad \\quad   \\forall X \\rightarrow Y,\\ X \\rightarrow Z \\in F \\] <p>[!note] Decomposition Rule</p> <p>All functional dependencies \\(X \\rightarrow Z\\), defined for every \\(Z \\subset Y \\text{ s.t. } X \\rightarrow Y \\in F\\), belong to \\(F^A\\).</p> \\[\\large   X \\rightarrow Z \\in F^A   \\quad \\quad   \\forall Z \\subset Y \\text{ s.t. } X \\rightarrow Y \\in F \\] <p>[!note] Pseudo-transitivity Rule</p> <p>All functional dependencies \\(XW \\rightarrow Z\\), defined for every couple \\(X \\rightarrow Y, WY \\rightarrow Z \\in F\\), belong to \\(F^A\\).</p> \\[\\large   WX \\rightarrow Z \\in F^A   \\quad \\quad   \\forall X \\rightarrow Y,\\ WY \\rightarrow Z \\in F \\]"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Functional%20Dependencies/#partial-dependencies","title":"Partial Dependencies","text":"<p>\\(X \\rightarrow A \\in F^+\\) is said to be a partial dependency if both:</p> <ul> <li>\\(A\\) is prime;</li> <li>\\(X \\subset K\\), i.e. \\(X\\) is a proper subset of a key.</li> </ul> <p>[!example]</p> <p>TK</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Functional%20Dependencies/#transitive-dependencies","title":"Transitive Dependencies","text":"<p>\\(X \\rightarrow A \\in F^+\\) is said to be a transitive dependency if all:</p> <ul> <li>\\(A\\) is prime;</li> <li>\\(X \\not\\subset K \\quad \\forall K\\);</li> <li>\\(K \\setminus X = \\emptyset\\).</li> </ul> <p>[!example]</p> <p>TK</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Keys/","title":"Keys","text":"<p>A key is a subset of attributes \\(K \\subseteq R\\) such that every tuple instance of \\(K\\) uniquely determines every tuple of a relation \\(R\\).</p> <p>[!example] Example</p> ID Name Surname DateOfBirth ... 200 Emanuele Scaccia 11/05/2002 ... 146 Mario Rossi 11/05/2002 ... <p>In this case, by imposing unique on \\(\\set\\text{Name, Surname}\\), both - \\(\\set\\text{ID} \\rightarrow R\\) - \\(\\set\\text{Name, Surname} \\rightarrow R\\)</p> <p>are candidate keys of \\(R\\), while \\(\\set\\text{ID, Name, Surname} \\rightarrow R\\) is a superkey.</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Keys/#types-of-keys","title":"Types of Keys","text":"<p>There are three types of keys, ordered in a hierarchy.</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Keys/#superkey","title":"Superkey","text":"<p>A superkey is a set of attributes \\(K \\in R\\) such that every instance of \\(K\\) uniquely determines every instance of the relation \\(R\\).  Using functional dependencies, the definition can be written as follows.</p> \\[\\large     K \\rightarrow R \\in F^+ \\]"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Keys/#candidate-key","title":"Candidate Key","text":"<p>A candidate key (or minimal superkey) is a superkey that can't both be reduced to a subset of itself and be a key.</p> \\[\\large     K \\rightarrow R \\in F^+     \\quad \\land \\quad     \\nexists K' \\subset K     \\text{ s.t. }     K' \\rightarrow R \\]"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Keys/#primary-key","title":"Primary Key","text":"<p>A primary key is chosen candidate key. In a relation where there are multiple valid candidate keys, one is picked to consistently identity all the tuples.</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Keys/#uniqueness-test","title":"Uniqueness Test","text":"<p>There exists a test to check whether there exists only one key in a relation.</p> <p>Let \\(F\\) be a set of functional dependencies on a relation \\(R\\), define</p> \\[\\large \\displaylines{     X = R \\setminus \\bigcup     \\set{ W \\setminus V \\mid V \\rightarrow W \\in F} \\\\     \\text{or} \\\\     X = \\bigcap \\set{R \\setminus (W \\setminus V)     \\mid V \\rightarrow W \\in F} } \\] <p>i.e. the set of all attributes that aren't explicitly determined by any functional dependency in \\(F\\).</p> <ul> <li>If \\(X \\rightarrow R\\), then there is one candidate key only;</li> <li>there are multiple candidate keys otherwise.</li> </ul>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Relational%20Algebra/","title":"Relational Algebra","text":"<p>Relational algebra is a mathematical notation to query the contents of relation. Languages like SQL are founded on relational algebra.</p> <p>It is a formal language to interrogate a relational database, consisting of a set of unary and binary operators that, if applied to one or two relation instances (sets of tuples), generate a new relation instance.</p> <p>TK: logical operators</p> <p>The operators are: - Projection - Selection - Union - Difference - Intersection - Cartesian Product - Join - Renaming</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Relational%20Algebra/#projection","title":"Projection","text":"<p>It is a unary operator. A projection performs a vertical cut on a relation, selecting a subset of the relation's attributes.</p> <p>It is represented by the symbol \\(\\large\\pi\\).</p> <p>Let \\(R(X)\\) be a relation, where \\(X = \\{A_1, \\cdots, A_n\\}\\) . Then \\(\\pi_Y(R)\\) where \\(Y \\subseteq X\\) is a projection of R. The result of the operation will be a set with all the tuples of the operand, but the tuples will contains only the attributes specified by \\(Y\\).</p> <p>\\(\\text{Customer}\\)</p> Name Code Town Rossi C1 Roma Rossi C2 Milano Rossi C3 Roma Bianchi C4 Roma Verdi C5 Roma <p>\\(\\pi_\\text{Name}(\\text{Customer})\\)</p> Name Rossi Bianchi Verdi <p>\\(\\pi_\\text{Name, Code}(\\text{Customer})\\)</p> Name Code Rossi C1 Rossi C2 Rossi C3 Bianchi C4 Verdi C5 <p>[!warning] Duplicates Note how there aren't duplicates in the resulting table, because every table is actually a set of tuples.</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Relational%20Algebra/#selection","title":"Selection","text":"<p>It is a unary operator. A selection performs a horizontal cut on the relation, that is, it selects a subset including all the tuples that meet some constraint(s).</p> <p>It is represented by \\(\\sigma\\).</p> <p>Let \\(R(X)\\) be a relation.  Then \\(\\sigma_C(R) \\subseteq R,\\) where \\(C\\) is a boolean condition. \\(C\\) must be in form \\(A\\ \\theta\\ B\\) or \\(A\\ \\theta\\ a\\), where: - \\(A, B\\) are attributes of the same domain (\\(dom(A) = dom(B)\\)) - \\(a\\) is a literal (constant) belonging to the domain of A (\\(a \\in dom(A)\\)) - \\(\\theta\\) is a comparison operator (\\(\\theta \\in \\{=, \\lt, \\gt, \\leq, \\geq \\}\\))</p> <p>Multiple conditions can be joined using logical operators (\\(\\lor, \\land, \\lnot\\)).</p> <p>\\(\\text{Customer}\\)</p> Name Code Town Rossi C1 Roma Rossi C2 Milano Rossi C3 Roma Bianchi C4 Roma Verdi C5 Roma <p>\\(\\sigma_\\text{Town='Roma'}(\\text{Customer})\\)</p> Name Code Town Rossi C1 Roma Rossi C3 Roma Bianchi C4 Roma Verdi C5 Roma <p>\\(\\sigma_{ \\text{Town='Roma'} \\land \\text{Name='Rossi'} }(\\text{Customer})\\)</p> Name Code Town Rossi C1 Roma Rossi C3 Roma"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Relational%20Algebra/#union","title":"Union","text":"<p>It is a binary commutative operator. A union creates a new relation instance containing all the tuples belonging to at least one of the of the operands. The two operands must be compatible.</p> <p>It is represented by the symbol \\(\\cup\\).</p> <p>Let \\(R_1(X),\\ R_2(Y)\\) be two relations such that \\(X = Y\\).  Then \\(R = R_1 \\cup R_2 = R_2 \\cup R_1\\) such that \\(R \\supseteq R_1, R_2\\).</p> <p>\\(\\text{Teachers}\\)</p> Name Code Department Rossi D1 Math Rossi D2 Italian Bianchi D3 Math Verdi D4 English <p>\\(\\text{Admins}\\)</p> Name Code Department Esposito A1 English Riccio A2 Math Pierro A3 Italian Verdi A4 English Bianchi A5 English <p>\\(\\text{AllStaff} = \\text{Teachers} \\cup \\text{Admins}\\)</p> Name Code Department Rossi D1 Math Rossi D2 Italian Bianchi D3 Math Verdi D4 English Esposito A1 English Riccio A2 Math Pierro A3 Italian Verdi A4 English Bianchi A5 English <p>[!warning] Duplicates The resulting table is always a set, so it must not contain any duplicate tuple. Duplicate ones will be discarded!</p> <p>\\(\\text{Teachers}\\)</p> Name Code Department Rossi D1 Math Rossi D2 Italian Bianchi D3 Math Verdi D4 English <p>\\(\\text{Admins}\\)</p> Name Code Department Salary Esposito A1 English 1250 Riccio A2 Math 2000 Pierro A3 Italian 1000 <p>\\(\\text{AllStaff} = \\text{Teachers} \\cup \\pi_{\\text{Name, Code, Department}}(\\text{Admins})\\)</p> Name Code Department Rossi D1 Math Rossi D2 Italian Bianchi D3 Math Verdi D4 English Esposito A1 English Riccio A2 Math Pierro A3 Italian"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Relational%20Algebra/#difference","title":"Difference","text":"<p>It is a binary operator. A difference creates a new relation instance containing all the tuples of the first operand that are not in the second operand. The two operands must be compatible.</p> <p>It is represented by the symbol \\(-\\).</p> <p>Let \\(R_1(X),\\ R_2(Y)\\) be two relations such that \\(X = Y\\).  Then \\(R_1 - R_2 = \\{ t : t \\in R_1 \\land t \\notin R_2\\}\\)</p> <p>\\(\\text{Students}\\)</p> Name TaxCode Department Rossi C1 Math Rossi C1 Italian Bianchi C3 Math Verdi C4 English <p>\\(\\text{Admins}\\)</p> Name TaxCode Department Esposito C5 Italian Riccio C6 Math Pierro C7 English Bianchi C3 Math <p>\\(\\text{Students} - \\text{Admins}\\)</p> Name TaxCode Department Rossi C1 Math Rossi C1 Italian Verdi C4 English <p>\\(\\text{Admins} - \\text{Students}\\)</p> Name TaxCode Department Esposito C5 Italian Riccio C6 Math Pierro C7 English"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Relational%20Algebra/#intersection","title":"Intersection","text":"<p>It is a binary commutative operator. An intersection creates a new relation instance containing all the tuples that are both in operands. The two operands must be compatible.</p> <p>It is represented by the symbol \\(\\cap\\).</p> <p>Let \\(R_1(X),\\ R_2(Y)\\) be two relations such that \\(X = Y\\).  Then \\(R_1 \\cap R_2 = \\{ t : t \\in R_1, R_2\\} = R_1 - (R_1 - R_2)\\)</p> <p>\\(\\text{Students}\\)</p> Name TaxCode Department Rossi C1 Math Rossi C1 Italian Bianchi C3 Math Verdi C4 English <p>\\(\\text{Admins}\\)</p> Name TaxCode Department Esposito C5 Italian Riccio C6 Math Pierro C7 English Bianchi C3 Math <p>\\(\\text{Students} \\cap \\text{Admins} = \\text{Admins} \\cap \\text{Students}\\)</p> Name TaxCode Department Bianchi C3 Math"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Relational%20Algebra/#cartesian-product","title":"Cartesian Product","text":"<p>It is a binary commutative operator. A cartesian product creates a relation with tuples obtained by combining all the tuples in the first operand with all the tuples in the second operand. It is usually used when the information needed is partitioned in multiple tables.</p> <p>It is represented by the symbol \\(\\times\\).</p> <p>\\(\\text{Customer}\\)</p> Name Code Town Rossi C1 Roma Rossi C2 Milano Bianchi C3 Roma Verdi C4 Roma <p>\\(\\text{Order}\\)</p> Code A Pieces C1 A1 100 C2 A2 200 C3 A2 150 C4 A3 200 C1 A2 200 C1 A3 100 <p>To list every customer with their orders we can use the cross product. A cross product will result in all combinations of all tuples, so we have to filter the resulting set to match the codes using the selection operator. But to do this, we have to be able to distinguish between <code>Code of Customer</code> and <code>Code of Order</code>, we can use the renaming operator. In the end, we project only the attributes we want, ignoring duplicates.</p> \\[\\large \\pi_\\text{Name, Code, Town, A, Pieces}(     \\sigma_\\text{Code = OCode} (         \\text{Customer} \\times \\rho_{\\text{OCode} \\gets \\text{Code}}(             \\text{Order}         )     ) ) \\] Name Code Town A Pieces Rossi C1 Roma A1 100 Rossi C1 Roma A2 200 Rossi C1 Roma A3 100 Rossi C2 Milano A2 200 Bianchi C3 Roma A2 150 Verdi C4 Roma A3 200"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Relational%20Algebra/#join","title":"Join","text":"<p>A natural join performs a cartesian product of two relations \\(R_1(X), R_2(Y)\\) and selects only some tuples that satisfy the condition \\(R_1[A_1] = R_2[A_1] \\land R_1[A_2] = R_2[A_2] \\land \\cdots \\land R_1[A_k] = R_2[A_k]\\) where \\(A_1, \\cdots, A_k \\in X, Y\\) are the attributes the two relations have in common.</p> <p>It is represented by the symbol \\(\\Join\\).</p> \\[\\large R_1 \\Join R_2 = \\pi_{X \\cup Y}( \\sigma_C( R_1 \\times R_2 ) )\\] <p>The attributes \\(A_1, \\cdots, A_k\\) in the condition must have the same name to perform a join on it.</p> <p>\\(\\text{Customer}\\)</p> Name Code Town Rossi C1 Roma Rossi C2 Milano Bianchi C3 Roma Verdi C4 Roma <p>\\(\\text{Order}\\)</p> Code A Pieces C1 A1 100 C2 A2 200 C3 A2 150 C4 A3 200 C1 A2 200 C1 A3 100 <p>\\(\\text{Customer} \\Join \\text{Order}\\)</p> Name Code Town A Pieces Rossi C1 Roma A1 100 Rossi C1 Roma A2 200 Rossi C1 Roma A3 100 Rossi C2 Milano A2 200 Bianchi C3 Roma A2 150 Verdi C4 Roma A3 200 <p>Note if two relations have no attributes in common, the join condition cannot be evaluated and the operation will default to a normal cartesian product!</p> <p>Another type of join is the theta join, which is the same as the natural join but with customizable condition: it selects the tuples resulting from the cartesian product of two relations that satisfy the condition \\(A \\theta B\\), where: - \\(\\theta\\) is a comparison operator (\\(\\theta \\in \\{ =, \\lt, \\gt, \\le, \\ge \\}\\)) - \\(A, B\\) are two attributes attributes of the first and second relations respectively - \\(dom(A) = dom(B)\\)</p> \\[\\large R_1 \\Join R_2 = \\sigma_{A \\theta B}(R_1 \\times R_2)\\]"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Relational%20Model/","title":"Relational Model","text":"<p>The Relational Model is a a mathematical data model proposed by Edgar Codd in 1970. Information is represented by values stored into records (or tuples), which are grouped into uniform tables.</p> <p>The whole model is based on the notion of relationship. Relations in math help to give a way of establishing a connection between any two objects or things. A relation describes the relationship between two objects that are usually represented as an ordered pair (a, b), in this case the objects are the records.</p> <p>In the relational model: - the domain is a (possibly infinite) set of values (e.g. all integers, strings of length 5, \\(\\{0, 1\\}\\)); - a relation is any subset of the Cartesian product of one or more domains; - a relation that is a subset of the Cartesian product of \\(k\\) domains is said to be of degree \\(k\\); - the number of tuples of a relation is called cardinality; - the tuples of a relation are all distinct, as a relation is a set.</p> <p>A relation can be regarded as a table in which: - each row is a distinct tuple that represent an instance of information; - each column corresponds to a component of information and contains homogeneous values (same domain).</p> <p>[!example] Context to Information</p> <p>The following table is a relation with two tuples of 4 elements with the specified domain, but it provides no information: there's no context.</p> String String Integer Real Mario Bianchi 10 28.5 Paolo Rossi 2 26.5"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Relational%20Model/#relation","title":"Relation","text":"<p>A relation has a fixed structure: - the columns must each have a unique, self-explanatory name within a table, called attribute name; - the pair (column name, column domain) is called attribute, which represents a piece of information; - the set of all attributes of a relation is called relational schema.</p> <p>If a relation \\(R\\) has attributes \\(A_1, \\ldots, A_k\\), then the schema is represented as \\(R(A_1, \\ldots, A_k)\\) or \\(R = A_1, \\ldots, A_k\\) .</p> <p>[!note] Aspects In a relation we can define two properties: - intentional aspect, the schema of a relation should remain unchanged over time, because it describes its structure; - extensional aspect, values in a relation (instance of a relation) can change, and change very quickly.</p> <p>Tuples between relations can be linked: references between data in different relations are represented by values that are included in the tuples. This whole behaviour is defined with foreign keys.</p> <p>[!note] Tuples Every tuple \\(t\\) of an instance of a relation \\(R\\) can be seen as a function that associates each attribute \\(A_i\\) to a value \\(t[A_i] \\in dom(A_i)\\).</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Relational%20Model/#null-values","title":"Null values","text":"<p>NULL values represent lack of information or the fact that the information is not applicable (e.g. phone number).</p> <p>Null does not belong to any domain, but it can be used as a wildcard to replace values in any domain. Be aware, though, that two null values are always considered different, even on the same domain.</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Relational%20Model/#accessing-attributes","title":"Accessing attributes","text":"<p>The components of a relation are indicated by the names rather than their position: \\(t[A_i]\\) indicates the value of the attribute with name \\(A_i\\) of the tuple \\(t\\).</p> <p>If \\(Y\\) is a subset of attributes of the schema \\(X\\) of a relation \\((Y \\subseteq X)\\) then \\(t_Y = (t[y_1], \\cdots, t[y_i]) \\text{ s.t. } y_i \\in Y\\) is the tuple composed only of the values of \\(t\\) that correspond to the attributes contained in \\(Y\\). \\(t_Y\\) is also called a restriction of \\(t\\).</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Relational%20Model/#keys","title":"Keys","text":"<p>There is a method to uniquely identify the tuples of an instance of a relation: keys. A relation key is one or more subsets of attributes that uniquely identify a tuple; it can be used to reference tuples from a different relation (foreign key).</p> <p>A subset \\(X\\) of attributes of a relation \\(R\\) is a key of \\(R\\) iff: 1. for each instance of \\(R\\), there do not exist two distinct tuples \\(t_1,\\ t_2\\) such that the two tuples have the same values for all attributes of \\(X\\), i.e. \\(\\(\\large t_1[a] \\neq t_2[a] \\quad \\forall a \\in X\\)\\) 2. there does not exist a proper subset of \\(X\\) satisfying condition 1.</p> <p>A relation could have several alternative keys (valid subsets of attributes), usually the best choice would be most used one or the one composed of a smaller number of attributes (primary key). A primary key does not allow for null values.</p> <p>There always is at least one key in every relation, because there can't be identical tuples (in this case, the key is the whole set of attributes).</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Third%20Normal%20Form/","title":"Third Normal Form","text":"<p>The third normal form (shortened as 3NF) of a schema is a property that ensures that no anomalies can occur in the schema.</p> <p>[!quote] Interpretation of 3NF</p> <p>Given a schema \\(R\\) and a set of functional dependencies \\(F\\), \\(R\\) is in 3NF iff there are neither partial dependencies nor transitive dependencies in \\(F\\).</p> <p>A schema is said to be in 3NF if, for all functional dependencies \\(X \\rightarrow A\\) such that \\(A \\notin X\\):</p> <ul> <li>either \\(A\\) belongs to a key (\\(A\\) is also called prime);</li> <li>or \\(X\\) contains a key (\\(X\\) is also said superkey).</li> </ul> <p>[!tip] \\(A \\notin X\\)</p> <p>The condition \\(A \\notin X\\) is present ignore some trivial functional dependencies in \\(F^+\\) to which the 3NF can't possibly be applied.</p> <p>For example, given \\(R = \\set{A,B}\\) and \\(F = \\set{A \\rightarrow B}\\), the functional dependency \\(B \\rightarrow B\\) doesn't respect the criteria for 3NF: \\(B\\) isn't neither prime nor superkey, even though \\(R\\) could be considered in 3NF.</p> <p>[!example] Example: 3NF</p> <p>TK.</p> <p>[!example] Example: not 3NF</p> <p>TK</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Decomposition/Decomposition%20of%20Relations/","title":"Decomposition of Relations","text":"<p>If a relations isn't in 3rd normal form, there is always be a way to decompose \\(R\\) into a collection \\(\\rho\\) (called a decomposition) of subsets \\(R_1, R_2, \\cdots, R_n\\) such that</p> <ul> <li>\\(R = \\bigcup_{i=1}^n R_i\\);</li> <li>every subschema \\(R_i\\) is in 3NF;</li> <li>all functional dependencies are preserved;</li> <li>the natural join of all \\(R_i\\) should return the original instance of \\(R\\) (lossless join).</li> </ul> <p>[!info] Projecting Functional Dependencies and Relation Instances</p> <p>By decomposing a schema, we are projecting each functional dependency (and relation instance) over the set of attributes defined by each subschema.</p> \\[\\large \\begin{aligned}   F_i &amp;= \\pi_{R_i}(F) \\\\   r_i &amp;= \\pi_{R_i}(r) \\end{aligned} \\] <ol> <li>Preserving functional dependencies</li> <li>Lossless Join</li> </ol>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Decomposition/Decomposition%20of%20Relations/#finding-a-decomposition","title":"Finding a Decomposition","text":"<ol> <li> <p>Compute a minimal cover \\(G\\) of \\(F\\), set \\(\\rho = \\emptyset\\) ;</p> </li> <li> <p>If there are any, set all attributes \\(X\\) which do not appear in any dependency as a first sub-relation (\\(\\rho = \\set X\\)) and discard them from \\(R\\) (\\(R' = R \\setminus X\\));</p> </li> <li> <p>If there is a dependency in \\(G\\) such that all attributes in \\(R'\\) are in the dependency, then add \\(R'\\) as a sub-relation (\\(\\rho = \\rho \\cup \\set{R'}\\));</p> </li> <li> <p>Otherwise, for each \\(X \\rightarrow A \\in G\\) add \\(XA\\) as a sub-relation (\\(\\rho = \\rho \\cup \\set{XA}\\));</p> </li> <li> <p>If the decomposition doesn't have a lossless join, then add a key as a sub-schema (\\(\\rho = \\rho \\cup \\set K\\)).</p> </li> </ol>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Decomposition/Lossless%20Join/","title":"Lossless Join","text":"<p>\\(\\def \\seq#1#2{{ {#1}_1, {#1}_2, \\ldots, {#1}_{#2} }}\\) \\(\\def \\rhojoin#1{{ m_\\rho(#1) }}\\) \\(\\def \\proj#1#2{{ \\pi_{#1}({#2}) }}\\)</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Decomposition/Lossless%20Join/#lossless-join","title":"Lossless Join","text":"<p>When decomposing a relation, there could be a loss or altering of data when joining back all the decomposed instances.</p> <p>A decomposition \\(\\rho = \\seq R n\\) of a relation \\(R\\) is said to have a lossless join if, for every legal instance \\(r\\) of \\(R\\), it holds that</p> \\[\\large     r = \\rhojoin r \\] <p>where \\(\\rhojoin r\\) represents the natural join of all sub-relations in \\(\\rho\\), i.e.</p> \\[\\large     \\rhojoin r = \\, \\bowtie_{i=1}^n \\pi_{R_i}(r) \\]"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Decomposition/Lossless%20Join/#properties-of-decomposition-join","title":"Properties of Decomposition Join","text":"<p>The natural join of all sub-relations in a decomposition \\(\\rho\\) has the following properties.</p> <ul> <li>\\(\\large r \\subseteq \\rhojoin r\\)</li> <li>\\(\\large \\proj{R_i}{\\rhojoin r} = \\proj{R_i}{r}\\)</li> <li>\\(\\large \\rhojoin {\\rhojoin r} = \\rhojoin r\\)</li> </ul>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Decomposition/Lossless%20Join/#checking-lossless-join","title":"Checking Lossless Join","text":"<p>Given a decomposition \\(\\rho = R_1, R_2, \\cdots, R_n\\), whether \\(\\rho\\) has a lossless join can be checked using the following algorithm.</p> <p>[!quote] Algorithm: \\(\\clj X G\\)</p> <p>\\(\\text{Begin}\\)</p> <p>\\(\\quad \\text{Let } r = (\\text{a table with dim } n \\times |R|)\\)</p> <p>\\(\\quad r_{iA} = (A \\in R_i) \\text{ ? } \\alpha_A : \\beta_{iA}\\)</p> <p>\\(\\quad \\text{For every } X \\rightarrow Y \\in F \\text{ then,}\\)</p> <p>\\(\\quad \\text{if } \\exists \\ t_1, t_2 \\text{ s.t. } t_1[X] = t_2[X] \\land t_1[Y] \\neq t_2[Y] \\text{ do:}\\)</p> <p>\\(\\quad \\quad \\text{For every } A \\in Y \\text{ do:}\\)</p> <p>\\(\\quad \\quad \\quad \\text{If } t_1[A] = \\alpha_A \\text{ do:}\\)</p> <p>\\(\\quad \\quad \\quad \\quad t_2[A] = t_1[A]\\)</p> <p>\\(\\quad \\quad \\quad \\text{Else do:}\\)</p> <p>\\(\\quad \\quad \\quad \\quad t_1[A] = t_2[A]\\)</p> <p>\\(\\quad \\quad \\quad \\text{Endif}\\)</p> <p>\\(\\quad \\quad \\text{Endfor } A \\in Y\\)</p> <p>\\(\\quad \\text{Endfor } X \\rightarrow Y \\in F\\)</p> <p>\\(\\quad \\text{Repeat For } X \\rightarrow Y \\in F \\text{ until r has aline with all } \\alpha \\lor r \\text{ hasn't changed.}\\)</p> <p>\\(\\quad \\text{If r has a row with all } \\alpha \\text{ then:}\\)</p> <p>\\(\\quad \\quad \\text{Lossless Join}\\)</p> <p>\\(\\quad \\text{Else then:}\\)</p> <p>\\(\\quad \\quad \\lnot \\text{Lossless Join}\\)</p> <p>\\(\\quad \\text{Endif}\\)</p> <p>\\(\\text{End}\\)</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Decomposition/Minimal%20Cover/","title":"Minimal Cover","text":"<p>\\(\\def \\clj#1#2{{ {#1}_{#2}^+ }}\\)</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Decomposition/Minimal%20Cover/#minimal-cover","title":"Minimal Cover","text":"<p>A minimal cover of a set \\(F\\) of functional dependencies is an equivalent set \\(G\\) of functional dependencies such that some of the original ones are reduced or removed. Minimal covers are the starting point to compute a preserving-lossless decomposition.</p> <p>Formally, a minimal cover of a set \\(F\\) of functional dependencies is an equivalent set \\(G\\) such that</p> <ol> <li>\\(|Y| = 1 \\quad \\forall X \\rightarrow Y \\in G\\)</li> <li>\\(\\nexists X' \\subset X : G \\equiv (G \\setminus \\set{X \\rightarrow A}) \\cup \\set{X' \\rightarrow A} \\quad \\forall X \\rightarrow A \\in G\\)</li> <li>\\(\\nexists X \\rightarrow A \\in G : G \\equiv G \\setminus \\set{X \\rightarrow A}\\)</li> </ol> <p>or, verbally,</p> <ol> <li>all dependants are singletons;</li> <li>all determinants are not redundant (determinants can't be reduced);</li> <li>all dependencies are not redundant (dependencies can't be removed).</li> </ol> <p>[!info] Existence of Minimal Covers</p> <p>For any set of functional dependencies, there could exist more than one minimal cover (all equivalent), but at least one minimal cover is guaranteed to exist (which could be the set itself, if it is already in minimal cover form).</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Decomposition/Minimal%20Cover/#minimal-cover-algorithm","title":"Minimal Cover Algorithm","text":"<p>There exists an algorithm to find a minimal cover:</p> <ol> <li>Decouple dependants</li> </ol> <p>Decompose all dependencies \\(X \\rightarrow Y\\) with \\(|Y| &gt; 1\\), using the decomposition rule, into the dependencies \\(X \\rightarrow A : A \\in Y\\).</p> <ol> <li>Reduce redundant determinants</li> </ol> <p>Try to reduce any dependency \\(X \\rightarrow A \\in F\\) into a dependency \\(X' \\rightarrow A \\in F^+: X' \\subset X\\). In other words, check if \\(A \\in \\clj{(X')}{F}\\).</p> <p>If a \\(X' \\subset X\\) can be found such that \\(A \\in \\clj{(X')}{F}\\), then \\(X \\rightarrow A\\) can be reduced (replaced) to \\(X' \\rightarrow A\\), otherwise it can't be reduced.</p> <p>[!tip]</p> <p>First of all, check if there exists any couple of dependencies \\(X \\rightarrow A, B \\rightarrow A \\in F\\) such that \\(B \\in X\\). If such a couple exists, then \\(X \\rightarrow A\\) can be removed because \\(A\\) is determined by a subset of \\(X\\), i.e. \\(X \\rightarrow A\\) is redundant.</p> <p>[!tip]</p> <p>TK Closures don't need to be computed again for every step, but closures persist thru the step (determinants are being reduced in such a way that doesn't alter the closure of \\(G\\))</p> <ol> <li>Remove redundant dependencies</li> </ol> <p>Try to remove any dependency \\(X \\rightarrow A\\) such that \\(G \\equiv G \\setminus \\set{X \\rightarrow A}\\).</p> <p>If, for any \\(X \\rightarrow A\\), \\(A \\in \\clj{X}{G \\setminus \\set{X \\rightarrow A}}\\), then the dependency can be removed because it is redundant (i.e. \\(X\\) determines \\(A\\) through other transitive dependencies), otherwise it can't be removed.</p> <p>[!tip]</p> <p>If, for a dependency \\(X \\rightarrow A\\), there are no other dependencies such that \\(A\\) is determined or is a determinant, then the dependency can't be removed, because then \\(A\\) wouldn't be determined anymore.</p> <ol> <li>At the end \\(G = \\text{min cover(} F \\text{)}\\)</li> </ol>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Decomposition/Preserving%20Functional%20Dependencies/","title":"Preserving Functional Dependencies","text":"<p>\\(\\def \\clj#1{{ #1^+ }}\\) \\(\\def \\clj#1#2{{ #1_#2^+ }}\\)</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Decomposition/Preserving%20Functional%20Dependencies/#preserving-the-functional-dependencies","title":"Preserving the Functional Dependencies","text":"<p>A decomposition \\(\\rho = R_1, R_2, \\cdots, R_n\\) is said to preserve \\(F\\) if the set of functional dependencies \\(F\\) and a decomposition \\(G\\) are equivalent, i.e.</p> \\[\\large     F \\equiv G = \\bigcup_{i=1}^n \\pi_{R_i}(F) \\] <p>where \\(\\pi_{R_i}(F) = \\set{X \\rightarrow Y \\in F^+ \\mid XY \\subseteq R_i}\\).</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Decomposition/Preserving%20Functional%20Dependencies/#checking-equivalence","title":"Checking Equivalence","text":"<p>To check that \\(F \\equiv G\\) (i.e. \\(F^+ = G^+\\)), it could also be checked that \\(F^+ \\supseteq G^+\\) and \\(F^+ \\subseteq G^+\\).</p> <p>[!question] \\(F^+ \\supseteq G^+\\)</p> <p>By the definition of the decomposition, it follows that \\(F \\supseteq G\\), so naturally \\(F^+ \\supseteq G^+\\).</p> <p>[!question] \\(F^+ \\subseteq G^+\\)</p> <p>Assume that \\(F \\subseteq G^+\\), then</p> <ol> <li>any \\(f \\in F\\) could be found by applying Armstrong's Axioms to some \\(g\\in G^+\\);</li> <li>any \\(f' \\in F^+\\) could be found by applying Armstrong's Axioms to some \\(f \\in F^+\\).</li> </ol> <p>By the previous statements, given a set of functional dependencies \\(F\\) and a decomposition \\(G\\), it can be checked that the two are equivalent if \\(F \\subseteq G^+\\), otherwise they're not equivalent.</p> <p>\\(G^+\\) is exponentially hard to compute, but:</p> <ol> <li>\\(F \\subseteq G^+ \\Leftrightarrow f \\in G^+ \\quad \\forall f \\in F\\)</li> <li>\\(X \\rightarrow Y \\in G^+ \\Leftrightarrow Y \\subseteq \\clj X G\\)</li> </ol> <p>Hence, if \\(F \\subseteq G^+\\), then \\(Y \\subseteq \\clj X G\\) \\(\\forall X \\rightarrow Y \\in G^+\\) and the two sets of functional dependencies are equivalent. Conversely, if a functional dependency can be found such that \\(Y \\nsubseteq \\clj X G\\), then \\(F \\not\\equiv G\\).</p>"},{"location":"Data%20Management%20and%20Analysis/Unit%201/Relational/Decomposition/Preserving%20Functional%20Dependencies/#computing-clj-x-g","title":"Computing \\(\\clj X G\\)","text":"<p>Given a decomposition \\(\\rho = R_1, R_2, \\cdots, R_n\\), the closure \\(\\clj X G\\) can be computed using the following algorithm, where the final result is stored in the variable \\(Z\\).</p> <p>[!quote] Algorithm: \\(\\clj X G\\)</p> <p>\\(\\text{Begin}\\)</p> <p>\\(\\quad \\text{Let } Z = X\\)</p> <p>\\(\\quad \\text{Let } S = \\bigcup_{i=1}^n \\clj{(Z \\cap R_i)}{G} \\cap R_i\\)</p> <p>\\(\\quad \\text{While } S \\not\\subset Z \\text{ then:}\\)</p> <p>\\(\\quad \\quad Z = Z \\cup S\\)</p> <p>\\(\\quad \\quad S = \\bigcup_{i=1}^n \\clj{(Z \\cap R_i)}{G} \\cap R_i\\)</p> <p>\\(\\quad \\text{Endwhile}\\)</p> <p>\\(\\text{End}\\)</p> <p>The algorithm iteratively computes the union of the closures of \\(X\\) w.r.t. every sub-relation \\(R_i\\), until no more attributes in the closure can be found (i.e. the closure is complete). The closure of \\(X\\) is computed w.r.t. \\(F\\), so intersecting at every iteration with \\(R_i\\) makes sure to eliminate every original dependency not included in \\(G\\).</p>"},{"location":"Linear%20Algebra/","title":"Linear Algebra","text":"<p>Linear algebra is a branch of mathematics that deals with linear equations and their representations in terms of vectors and matrices.</p> <p>It is used to solve systems of linear equations, to analyse and model data, and to study the geometry of objects in space. Linear algebra plays a crucial role in many areas of modern mathematics and has important applications in fields such as computer graphics, cryptography, and signal processing.</p>"},{"location":"Linear%20Algebra/#table-of-contents","title":"Table of Contents","text":"<p>Root</p> <ul> <li>Sets</li> <li>Linear Equations</li> <li>Vectors</li> <li>Matrices</li> <li>Hyperplanes</li> <li>Eigenvectors</li> </ul> <p>Linear Transformations - Linear Transformations - Spectral Decomposition - Singular Value Decomposition</p>"},{"location":"Linear%20Algebra/Eigenvectors/","title":"Eigenvectors","text":"<p>Eigenvectors are vectors that, when applying a linear transformation, will get scaled but will remain on the same direction. Eigenvectors are represented by the following equation,</p> \\[\\large     A \\vec v = \\lambda \\vec v \\] <p>where:</p> <ul> <li>\\(A\\) is the transformation matrix;</li> <li>\\(\\vec v\\) is the eigenvector;</li> <li>\\(\\lambda\\) is the scaling factor of the eigenvector.</li> </ul> <p>[!tip] How many eigenvectors</p> <p>There can be at most \\(\\text{rank}(A)\\) eigenvectors in a transformation, but it isn't guaranteed to be exactly that number.</p> <p>For example, take a generic rotation matrix. Geometrically, it's impossible for a vector to be rotated and stay on the same direction as before; even though the rank of a rotation matrix is \\(2\\), there are no eigenvectors.</p>"},{"location":"Linear%20Algebra/Eigenvectors/#finding-eigenvectors","title":"Finding Eigenvectors","text":"<p>Eigenvectors can be found via the following process.</p> \\[\\large \\displaylines{     A \\vec v = \\lambda \\vec v \\\\     A \\vec v = \\lambda I \\vec v \\\\     A \\vec v - \\lambda I \\vec v = 0 \\\\     (A - \\lambda I) \\vec v = 0 \\\\     \\det(A - \\lambda I) = 0 } \\]"},{"location":"Linear%20Algebra/Hyperplanes/","title":"Hyperplanes","text":"<p>A hyperplane is a subspace of a higher-dimensional space that has a dimension one less than the ambient space. For example, in a two-dimensional space, a hyperplane is a one-dimensional line, while in a three-dimensional space, it is a two-dimensional plane.</p> <p>Hyperplanes can be represented by equations of the form</p> \\[\\large     a_1 x_1 + a_2 x_2 + \\dots + a_n x_n = b, \\] <p>where \\(a_1, a_2, \\dots, a_n\\) are constants and \\(x_1, x_2, \\dots, x_n\\) are the variables, \\(n\\) is the dimension of the space.</p> <p>The equation of a hyperplane can also be represented in vector form as</p> \\[\\large     \\vec a \\cdot \\vec x = b, \\] <p>where \\(\\vec a\\) is a vector normal to the hyperplane and \\(\\vec x\\) is a vector representing a point in the hyperplane.</p> <p>A hyperplane divides the space into two half-spaces, one on each side of the hyperplane. A point is said to be on one side of the hyperplane depending on which half-space it lies in, the dot product with the normal vector can be used to find it out.</p>"},{"location":"Linear%20Algebra/Linear%20Equations/","title":"Linear Equations","text":"<p>A linear equations is a basic type of equation where all the unknowns have exponent equal 1. All unknowns are grouped on the left, while the known values are on the right.</p> <p>Usually, all the values belong to the Real set, so the equation takes the following form:</p> \\[ \\large a_1 x_1 + a_2 x_2 + \\dots + a_n x_n = b \\quad a_i, b \\in \\mathbb{R} \\]"},{"location":"Linear%20Algebra/Matrices/","title":"Matrices","text":"<p>A matrix is a special structure composed of numbers, it is composed of \\(n\\) rows and \\(m\\) columns. It is a like a table with custom properties, every cell is a real value.</p> \\[\\begin{bmatrix} 1 &amp; 2 &amp;  3 &amp;  4\\\\ 5 &amp; 6 &amp;  7 &amp;  8\\\\  8 &amp; 9 &amp; 10 &amp; 11\\\\ \\end{bmatrix} \\text{ is a 3x4 matrix} \\]"},{"location":"Linear%20Algebra/Matrices/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>Matrices can be multiplied with either other matrices or vectors.</p> <p>The result of matrix multiplication is a new matrix that has the same number of rows as the first matrix and the same number of columns as the second matrix. To calculate the entries of the resulting matrix, we take the dot product of each row of the first matrix with each column of the second matrix.</p> \\[\\large \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ \\end{bmatrix} \\begin{bmatrix} 7 &amp; 8 \\\\ 9 &amp; 10 \\\\ 11 &amp; 12 \\\\ \\end{bmatrix} = \\begin{bmatrix} 58 &amp; 64 \\\\ 139 &amp; 154 \\\\ \\end{bmatrix} \\] <p>[!tip] Rows and Columns</p> <p>When multiplying two matrices, the number of columns of the first matrix must match the number of rows of the second matrix.</p>"},{"location":"Linear%20Algebra/Matrices/#inverse-of-a-matrix","title":"Inverse of a Matrix","text":"<p>The inverse of a matrix \\(A\\), denoted as \\(A^{-1}\\), is the matrix such that the product of A and its inverse gives the identity matrix.</p> \\[\\large     A A^{-1} = A^{-1} A = I \\] <p>Geometrically, the inverse of a matrix \\(A\\) is a matrix that reverses the linear transformation applied by \\(A\\). If \\(A\\) applies an irreversible transformation (space collapses on a lower dimension, i.e. when the determinant is \\(0\\)), then \\(A^{-1}\\) doesn't exist and \\(A\\) is said to be singular.</p>"},{"location":"Linear%20Algebra/Matrices/#diagonal-matrix","title":"Diagonal Matrix","text":"<p>A diagonal matrix is a special type of square matrix where all the non-diagonal elements are zero, but the values on the diagonal can be anything.</p> \\[\\large     \\begin{bmatrix}         d_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\         0 &amp; d_2 &amp; \\cdots &amp; 0 \\\\         \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\         0 &amp; 0 &amp; \\cdots &amp; d_n     \\end{bmatrix} \\] <p>[!tip] Identity Matrix</p> <p>The identity matrix is a special case of diagonal matrix, where all the entries in the diagonal are \\(1\\).</p> \\[\\large   I = \\begin{bmatrix}       1 &amp; 0 &amp; \\cdots &amp; 0 \\\\       0 &amp; 1 &amp; \\cdots &amp; 0 \\\\       \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\       0 &amp; 0 &amp; \\cdots &amp; 1   \\end{bmatrix} \\] <p>The identity matrix has the special property that multiplying a vector or another matrix by the identity will result in the starting vector or matrix, i.e. the identity matrix is the only matrix which does not morph the space.</p>"},{"location":"Linear%20Algebra/Matrices/#symmetric-matrix","title":"Symmetric Matrix","text":"<p>A symmetric matrix is a square matrix that is equal to its transpose, i.e.</p> \\[\\large     A = A^T     \\Longleftrightarrow     a_{ij} = a_{ji} \\quad \\forall i,j \\] <p>[!note] Eigenvectors</p> <p>If matrix is symmetric, then all the eigenvectors will be orthogonal one to each other.</p>"},{"location":"Linear%20Algebra/Matrices/#orthogonal-matrix","title":"Orthogonal Matrix","text":"<p>An orthogonal matrix is a square matrix in which all the column vectors are orthonormal to each other.</p> <p>[!tip] Rotation</p> <p>If all the column vectors in an orthogonal matrix are unit vectors (i.e. magnitude \\(1\\)), then the matrix represents a pure rotation without any other transformation.</p> <p>Orthogonal matrices have another property: the transpose of an orthogonal matrix is equal to it's inverse.</p> \\[\\large     Q^T = Q^{-1} \\] <p>[!example] Rotation Matrix</p> <p>The previous identity can be proven using the formula for the rotation matrix: a rotation by \\(\\theta\\) can be reversed by rotating by \\(-\\theta\\).</p> \\[\\large   \\text{Let } Q(\\theta) =   \\begin{bmatrix}       \\cos\\theta &amp; -\\sin\\theta \\\\       \\sin\\theta &amp; \\cos\\theta   \\end{bmatrix}, $$ $$\\large   \\begin{aligned}   \\text{then } Q^{-1}(\\theta) &amp;= Q(-\\theta) \\\\   &amp;= \\begin{bmatrix}       \\cos(-\\theta) &amp; \\sin(-\\theta) \\\\       \\sin(-\\theta) &amp; \\cos(-\\theta)   \\end{bmatrix} \\\\   &amp;= \\begin{bmatrix}       \\cos\\theta &amp; \\sin\\theta \\\\       -\\sin\\theta &amp; \\cos\\theta   \\end{bmatrix} \\\\   &amp;= Q^T(\\theta)   \\end{aligned} \\]"},{"location":"Linear%20Algebra/Sets/","title":"Sets","text":"<p>A set is a collection of homogeneous elements, with no order and every elements is unique in the set. A set can have finite or infinite elements.</p>"},{"location":"Linear%20Algebra/Sets/#defining-a-set","title":"Defining a set","text":""},{"location":"Linear%20Algebra/Sets/#roster-notation","title":"Roster Notation","text":"<p>The literal elements are separated by comma: $$ \\large { 1, 2, 3, 4 } $$</p> <p>We can also include infinite elements with three dots like this (all numbers from 1 to infinity): $$ \\large { 1, 2 ,3, \\cdots } $$</p> <p>Or, to include a large number of sequential elements (all numbers from 1 to 100): $$ \\large { 1, 2, \\cdots, 99, 100 } $$</p>"},{"location":"Linear%20Algebra/Sets/#semantic-notation","title":"Semantic Notation","text":"<p>A sentence is used to describe the elements: $$ \\large \\text{Let S be the set of whole numbers from 1 to 4} $$</p>"},{"location":"Linear%20Algebra/Sets/#builder-notation","title":"Builder Notation","text":"<p>A special syntax is used to generate elements, or to \"select\" them from another set: $$ \\large { n \\in \\mathbb{R} \\mid 1 \\le n \\le 4 } $$ The first part is the \"generator function\", which takes one or more values and gives back an element; the second part is the condition for selection. The vertical bar means \"such that\".</p> <p>Ex. | Defining the set of rational numbers \\(\\mathbb{Q}\\)</p> \\[ \\large \\mathbb{Q} = \\{ \\frac{a}{b} \\mid b \\ne 0 \\quad a,b \\in \\mathbb{Z}\\} \\]"},{"location":"Linear%20Algebra/Sets/#properties-of-sets","title":"Properties of Sets","text":""},{"location":"Linear%20Algebra/Sets/#membership","title":"Membership","text":"<p>If a Set \\(S\\) contains an element \\(x\\), it is said that element belongs to \\(S\\). $$ \\large x \\in S $$</p> <p>On the contrary, if \\(x\\) isn't an element of \\(S\\), it is said that element doesn't belong to \\(S\\). $$ \\large x \\notin S $$</p> <p>Ex. | Membership relative to the set of integer numbers \\(\\mathbb{Z}\\)</p> \\[ \\large 2 \\in \\mathbb{Z} \\quad 2.5 \\notin \\mathbb{Z} \\]"},{"location":"Linear%20Algebra/Sets/#subsets","title":"Subsets","text":"<p>If \\(A\\) and \\(B\\) are two sets and every elements in \\(A\\) is in \\(B\\), then \\(A\\) is a subset of \\(B\\).</p> \\[ \\large A \\subseteq B \\] <p>On the other hand, it is said that \\(B\\) is a superset of \\(A\\), because it contains every element of \\(A\\).</p> \\[ \\large B \\supseteq A \\] <p>If \\(A\\) is a subset of \\(B\\), then \\(A = B\\) could be true, because every set includes every its own element in itself. The same can be said for the superset. If this is not expected behaviour, we can define the proper subset and superset:</p> \\[ \\large \\begin{gather} A \\subseteq B,\\ A \\neq B \\rightarrow A \\subset B \\\\ B \\supseteq A,\\ A \\neq B \\rightarrow B \\supset A \\end{gather} \\] <p>Ex. | Relationship between the naturals set \\(\\mathbb{N}\\), the integers set \\(\\mathbb{Z}\\) and the reals set \\(\\mathbb{R}\\):</p> \\[ \\large \\mathbb{N} \\subset \\mathbb{Z} \\subset \\mathbb{R}\\]"},{"location":"Linear%20Algebra/Sets/#set-operations","title":"Set Operations","text":""},{"location":"Linear%20Algebra/Sets/#union","title":"Union","text":"<p>Two sets can be joined, the new set contains all the members of both \\(A\\) and \\(B\\), repeted once.</p> \\[ \\large A \\ \\cup \\ B = C \\] <p>Ex. | Union examples</p> \\[ \\large \\begin{align} \\{1,2\\} \\ \\cup \\ \\{3,4\\} &amp;= \\{1,2,3,4\\} \\\\ \\{1,2\\} \\ \\cup \\ \\{2,3\\} &amp;= \\{1,2,3\\} \\\\ \\end{align} \\]"},{"location":"Linear%20Algebra/Sets/#intersection","title":"Intersection","text":"<p>The intersection of two sets is a new set with the elements \\(A\\) and \\(B\\) have in common. If there are no elements in common, an empty set is returned.</p> \\[ \\large A \\ \\cap \\ B = C \\] <p>Ex. | Intersection examples</p> \\[ \\large \\begin{align} \\{1,2,3\\} \\ \\cup \\ \\{2,3,4\\} &amp;= \\{2,3\\} \\\\ \\{1,2,3\\} \\ \\cup \\ \\{4,5,6\\} &amp;= \\varnothing \\\\ \\end{align} \\]"},{"location":"Linear%20Algebra/Sets/#cartesian-product","title":"Cartesian Product","text":"<p>The cartesian product of two sets \\(A\\) and \\(B\\) is the set of all ordered pairs \\((a,b)\\) such that \\(a\\) is a member of \\(A\\) and \\(b\\) is a member of \\(B\\).</p> \\[ \\large A \\times B = \\{ (a,b) \\mid a \\in A,\\ b \\in B \\} \\] <p>Ex. | Cartesian Product example</p> \\[ \\large \\{x_1, x_2\\} \\times \\{y_1, y_2\\} = \\{(x_1, y_1), (x_1, y_2), (x_2, y_1), (x_2, y_2)\\} \\]"},{"location":"Linear%20Algebra/Vectors/","title":"Vectors","text":"<p>A vector is a mathematical object that either represents a physical quantity or position with magnitude and direction.</p> <p>A vector can be written as a linear combination (sum of the multiples) of the components of a basis set of vectors. The basis vectors are usually unit vectors in the direction of the coordinate axes.</p> <p>[!info] Cardinal Basis</p> <p>In the Cartesian coordinate system, the basis vectors are \\(\\hat{i}\\), \\(\\hat{j}\\), and \\(\\hat{k}\\). They all have length of 1 and are in the direction of the \\(x\\)-axis, \\(y\\)-axis and \\(z\\)-axis respectively.</p> <p>This basis is called Cardinal Basis.</p>"},{"location":"Linear%20Algebra/Vectors/#magnitude","title":"Magnitude","text":"<p>The magnitude (or norm) of a vector is the physical length from the tail of the vector to its head, it can be calculated using the Pythagorean theorem.</p> <p>For a vector \\(\\vec{v} = \\begin{bmatrix} v_1 &amp; v_2 &amp; \\cdots &amp; v_n \\end{bmatrix}\\), the magnitude is \\(|\\vec{v}| = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^2}\\).</p>"},{"location":"Linear%20Algebra/Vectors/#operations","title":"Operations","text":""},{"location":"Linear%20Algebra/Vectors/#vector-addition","title":"Vector Addition","text":"<p>Vectors can be added and subtracted by summing or subtracting their components. </p> \\[\\large     \\vec u + \\vec v =     \\begin{bmatrix}         u_1 + v_1 \\\\ u_2 + v_2 \\\\ \\vdots \\\\ u_n + v_n     \\end{bmatrix} \\] \\[\\large     \\vec u - \\vec v =     \\begin{bmatrix}         u_1 - v_1 \\\\ u_2 - v_2 \\\\ \\vdots \\\\ u_n - v_n     \\end{bmatrix} \\]"},{"location":"Linear%20Algebra/Vectors/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>Vectors can be multiplied by a scalar. The result of multiplying a vector \\(\\vec{v}\\) by a scalar \\(\\lambda\\) is a new vector with the same direction but different magnitude. The resulting magnitude is the previous magnitude multiplied by the scalar.</p> \\[\\large     \\lambda \\vec v =     \\begin{bmatrix}         \\lambda v_1 \\\\ \\lambda v_2 \\\\ \\vdots \\\\ \\lambda v_n     \\end{bmatrix}, \\quad     |\\lambda \\vec v| = \\lambda |\\vec v| \\]"},{"location":"Linear%20Algebra/Vectors/#dot-product","title":"Dot Product","text":"<p>Also called inner product, geometrically the dot product represents a vector's orthogonal projection on another vector.</p> <p>TK geometric meaning</p> <p>The dot product can be computed by summing the product of the two vectors' components with the same indices, i.e.</p> \\[\\large     \\vec u \\cdot \\vec v =     \\sum_{i=1}^n u_i v_i =     u_1 v_1 + u_2 v_2 + \\cdots + u_n v_n \\] <p>There exists another formula to compute the dot product, which is deeply connected to the angle between the two vectors and their length.</p> \\[\\large     \\vec u \\cdot \\vec v = \\cos\\theta |\\vec{u}| |\\vec{v}| \\] <p>From the previous formula, a ratio (cosine similarity) and a new formula to find the angle between two vectors can be found.</p> \\[\\large     \\theta = \\arccos \\frac         {\\vec u \\cdot \\vec v}         {|\\vec u| |\\vec v|} \\]"},{"location":"Linear%20Algebra/Vectors/#matrix-vector-multiplication","title":"Matrix-Vector Multiplication","text":"<p>A vector can be multiplied by a matrix, resulting in a new vector. The \\(i\\)-th component of the new vector is calculated by computing the dot product between the \\(i\\)-th row of the matrix and the operand vector.</p> <p>$$\\large     A \\vec v =</p> <pre><code>\\begin{bmatrix}\n    a_{11} &amp; a_{12} &amp; \\dots &amp; a_{1n} \\\\\n    a_{21} &amp; a_{22} &amp; \\dots &amp; a_{2n} \\\\\n    \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\n    a_{m1} &amp; a_{m2} &amp; \\dots &amp; a_{mn} \\\\\n\\end{bmatrix}\n\n\\begin{bmatrix}\n    v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n\n\\end{bmatrix} =\n\n\\begin{bmatrix}\n    a_{11}v_1 + a_{12}v_2 + \\dots + a_{1n}v_n \\\\\n    a_{21}v_1 + a_{22}v_2 + \\dots + a_{2n}v_n \\\\\n    \\vdots \\\\\n    a_{m1}v_1 + a_{m2}v_2 + \\dots + a_{mn}v_n \\\\\n\\end{bmatrix}\n</code></pre> <p>$$</p> <p>[!info] Geometric Meaning</p> <p>Matrix-Vector multiplication has a special geometric meaning: multiplying a vector by a matrix applies on the vector the linear transformation represented by the matrix.</p>"},{"location":"Linear%20Algebra/Transformations/Linear%20Transformations/","title":"Linear Transformations","text":"<p>A linear transformation can be seen as a function that transforms a vector space in some kind of way. Some transformations that can be applied on a vector space are scaling, rotating, shearing.</p> <p>Matrix-Vector multiplication is used to apply a linear transformation; many transformations applied sequentially can be applied at the same time by multiplying the vector space by a single matrix, which can be computed by multiplying all the matrices together in the order of application.</p> \\[\\large     CBA \\vec v = M \\vec v, \\quad     \\text{where } M = C \\times (B \\times A) \\]"},{"location":"Linear%20Algebra/Transformations/Linear%20Transformations/#determinant","title":"Determinant","text":"<p>The determinant of a transformation (and of the respective matrix) represents the scaling factor of the transformation, it represents the ratio between the areas of the parallelograms spanned by the final basis and by the cardinal basis.</p> <p>If the determinant is:</p> <ul> <li>positive, the transformation preserves orientation and stretches or shrinks the space;</li> <li>negative, the transformation reverses orientation and reflects the space;</li> <li>zero, the transformation collapses the space onto a lower-dimensional subspace.</li> </ul>"},{"location":"Linear%20Algebra/Transformations/Linear%20Transformations/#translation","title":"Translation","text":"<p>TK</p>"},{"location":"Linear%20Algebra/Transformations/Linear%20Transformations/#scaling","title":"Scaling","text":"<p>A scaling transformation changes the size of an object. The scaling transformation multiplies each component of a vector \\(\\vec v\\) by \\(\\alpha\\), resulting in a new vector \\(\\vec w = \\alpha \\cdot \\vec v\\).</p> <p>The matrix representation of the scaling transformation has diagonal entries equal to the scaling factor \\(\\alpha\\) and all other entries equal to zero. In two dimensions, the scaling transformation matrix is as follows.</p> \\[\\large     \\begin{bmatrix}     \\alpha &amp; 0 \\\\     0 &amp; \\alpha     \\end{bmatrix} \\] <p>Scaling can also be applied to the single components separately by changing the component's relative entry in the matrix's diagonal. For example, the following matrix doubles only the vertical component of a two-dimensional vector space.</p> <p>$$\\large     \\begin{bmatrix}     \\alpha &amp; 0 \\     0 &amp; \\beta     \\end{bmatrix}</p> <pre><code>\\begin{bmatrix}\nx \\\\ y\n\\end{bmatrix} =\n\n\\begin{bmatrix}\n\\alpha x \\\\ \\beta y\n\\end{bmatrix}\n</code></pre> <p>$$</p>"},{"location":"Linear%20Algebra/Transformations/Linear%20Transformations/#rotation","title":"Rotation","text":"<p>A bidimensional vector space can be rotated by a given angle \\(\\theta\\) by multiplying the vector space by the following matrix.</p> \\[\\large     \\begin{bmatrix}     \\cos\\theta &amp; -\\sin\\theta \\\\     \\sin\\theta &amp; \\cos\\theta     \\end{bmatrix} \\] <p>The angle \\(\\theta\\) determines the amount of rotation applied to the vector. For example, rotating a vector by \\(90\\) degrees counter-clockwise results in a new vector that is perpendicular to the original.</p> <p>[!tip] Preserving Distance and Angles</p> <p>Rotation is a linear transformation that preserves distance and angles between vectors: - if two vectors were orthogonal before the transformation, they will still be orthogonal after the transformation; - if the angle between two vectors was \\(\\alpha\\) before the transformation, it will remain \\(\\alpha\\) after the transformation.</p>"},{"location":"Linear%20Algebra/Transformations/Linear%20Transformations/#shearing","title":"Shearing","text":"<p>A shear is a linear transformation that \"shears\" the vector space in a certain direction. It is defined by a matrix of the form:</p> \\[\\large     S_{xy} = \\begin{bmatrix} 1 &amp; k \\\\ 0 &amp; 1 \\end{bmatrix}     \\quad \\text{or} \\quad     S_{yx} = \\begin{bmatrix} 1 &amp; 0 \\\\ k &amp; 1 \\end{bmatrix} \\] <p>where \\(k\\) determines the amount and direction of the shear.</p> <p>When applied to a vector, a shear transformation moves the components of the vector parallel to the direction of the shear, with the amount of movement proportional to the distance from the origin.</p>"},{"location":"Linear%20Algebra/Transformations/Linear%20Transformations/#all-transformation-together","title":"All Transformation Together","text":"<p>All the previous transformations (scaling, rotation, shearing) can be applied at the same time by multiplying the vector space by the following matrix,</p> \\[\\large     \\begin{bmatrix}         s_x\\cos\\theta &amp; -c_x\\sin\\theta \\\\         c_y\\sin\\theta &amp; s_y\\cos\\theta     \\end{bmatrix} \\] <p>where:</p> <ul> <li>\\(s_x, s_y\\) are the scaling factors;</li> <li>\\(\\theta\\) is the rotation angle;</li> <li>\\(c_x, x_y\\) are the shearing factors.</li> </ul>"},{"location":"Linear%20Algebra/Transformations/Singular%20Value%20Decomposition/","title":"Singular Value Decomposition","text":"<p>The singular value decomposition is a method to decompose a complex transformation into a series of three simpler transformations, i.e. two rotations and a scaling.</p> <p>Any matrix \\(A\\), regardless of rank and dimensions, can be decomposed in the following way,</p> \\[\\large     A = U \\Sigma V^* \\] <p>where: - \\(U,V\\) are orthogonal matrices that represent two rotations; - \\(V^*\\) is the transpose of \\(V\\); - \\(\\Sigma\\) is a diagonal matrix that represents the scaling.</p> <p>[!tip] Transpose of V</p> <p>The transpose of \\(V\\) is written as \\(V^*\\) as a reminder that \\(V\\) is orthogonal, i.e.  \\(\\(V^* = V^T = V^{-1}\\)\\)</p>"},{"location":"Linear%20Algebra/Transformations/Singular%20Value%20Decomposition/#geometric-meaning-of-svd","title":"Geometric Meaning of SVD","text":"<p>The SVD stems from the following reasoning, similarly to spectral decomposition: we want to find a set of orthogonal vectors \\(v_i\\) such that, when transformed by \\(A\\), they will stay orthogonal, even if scaled (\\(\\sigma_i \\vec u_i\\)).</p> \\[\\large     A \\vec v_i = \\sigma_i \\vec u_i \\] <p>Compacting the sets of vectors in matrices, the following identity can be obtained, from which the final formula can be obtained by multiplying by \\(V^*\\) on the right on both sides.</p> \\[\\large \\begin{aligned}     A V &amp;= U \\Sigma \\\\     A &amp;= U \\Sigma V^* \\end{aligned} \\] <p>The previous formula gives another intuition in the geometric representation of SVD: we want to find a rotation </p>"},{"location":"Linear%20Algebra/Transformations/Singular%20Value%20Decomposition/#singular-vectors-and-values","title":"Singular Vectors and Values","text":"<ul> <li>The column vectors of \\(U\\) and \\(V\\) are said to be the left and right (respectively) singular vectors of \\(A\\);</li> <li>\\(\\Sigma\\) is said to contain the singular values of \\(A\\).</li> </ul> <p>The meaning of singular vectors and values are similar to the meaning of eigenvectors for spectral decomposition: - the singular vectors are orthogonal vectors that will stay orthogonal once the transformation \\(A\\) has been applied, the right singular vectors \\(\\vec v_i\\) will be mapped in the direction of the respective left singular vectors \\(\\vec u_i\\); - the singular values \\(\\sigma_i\\) are the factors which scale the left singular vectors \\(\\vec u\\) to align with the transformed right singular vectors \\(v_i\\) (\\(A v_i =\\sigma_i u_i\\)).</p>"},{"location":"Linear%20Algebra/Transformations/Singular%20Value%20Decomposition/#left-singular-matrix","title":"Left Singular Matrix","text":"<p>The left singular matrix \\(U\\) can be computed by performing a spectral decomposition on \\(AA^T\\), i.e. \\(U\\) is composed by the eigenvectors of the symmetric matrix \\(AA^T\\), ranked in descending order of their respective eigenvalues.</p> <p>[!quote] Proof</p> <p>The following series of identities proves that \\(U\\) and \\(\\Sigma\\) can be found by performing a spectral decomposition on \\(AA^T\\).</p> \\[\\large \\begin{aligned}   A A^T &amp;= U \\Sigma V^* (U \\Sigma V^*)^T \\\\   &amp;= U \\Sigma \\underbrace{V^* V}_I \\underbrace{\\Sigma^T}_\\Sigma U^T \\\\   &amp;= U \\Sigma \\Sigma U^T \\\\   &amp;= U \\Sigma^2 U^T \\end{aligned} \\] <p>[!tip] Left Singular Matrix as a Rotation</p> <p>The fact that \\(U\\) can be found by a spectral decomposition guarantees that \\(U\\) is an orthogonal matrix, meaning it represents a rotation.</p>"},{"location":"Linear%20Algebra/Transformations/Singular%20Value%20Decomposition/#right-singular-matrix","title":"Right Singular Matrix","text":"<p>The right singular matrix \\(V\\) can be computed by performing a spectral decomposition on \\(A^T A\\), i.e. \\(V\\) is composed by the eigenvectors of the symmetric matrix \\(A^T A\\), ranked in descending order of their respective eigenvalues.</p> <p>[!quote] Proof</p> <p>The following series of identities proves that \\(V\\) and \\(\\Sigma\\) can be found by performing a spectral decomposition on \\(A^T A\\).</p> \\[\\large \\begin{aligned}   A^T A &amp;= (U \\Sigma V^*)^T U \\Sigma V^* \\\\   &amp;= V \\underbrace{\\Sigma^T}_\\Sigma \\underbrace{U^T U}_I \\Sigma V^* \\\\   &amp;= V \\Sigma \\Sigma V^T \\\\   &amp;= V \\Sigma^2 V^T \\end{aligned} \\] <p>[!tip] Right Singular Matrix as a Rotation</p> <p>The fact that \\(V\\) can be found by a spectral decomposition guarantees that \\(V\\) is an orthogonal matrix, meaning it represents a rotation.</p>"},{"location":"Linear%20Algebra/Transformations/Singular%20Value%20Decomposition/#singular-values-matrix","title":"Singular Values Matrix","text":"<p>The singular values matrix \\(\\Sigma\\) can be computed by performing a spectral decomposition on either \\(A A^T\\) or \\(A^T A\\). In both spectral decompositions, the scaling matrix \\(\\Lambda\\) is the square of \\(\\Sigma\\); given that \\(\\Sigma\\) is diagonal, the singular values \\(\\sigma_i\\) can be found by taking the square root of the eigenvalues \\(\\lambda_i\\) contained in \\(\\Lambda\\).</p> \\[\\large \\begin{aligned}     \\Sigma^2 &amp;= \\Lambda \\\\     \\sigma_i^2 &amp;= \\lambda_i \\\\     \\sigma_i &amp;= \\sqrt \\lambda_i \\end{aligned} \\]"},{"location":"Linear%20Algebra/Transformations/Singular%20Value%20Decomposition/#data-meaning-of-svd","title":"Data Meaning of SVD","text":"<p>TK</p>"},{"location":"Linear%20Algebra/Transformations/Spectral%20Decomposition/","title":"Spectral Decomposition","text":"<p>The spectral decomposition is a way, based on the spectral theorem, to decompose a complex matrix into three simpler matrices. The decomposition is as follows,</p> \\[\\large     A = Q \\Lambda Q^T \\] <p>where: - \\(A\\) is a symmetric matrix; - \\(Q\\) is an orthogonal matrix (rotation) composed of the eigenvectors of \\(A\\); - \\(\\Lambda\\) is a diagonal matrix (scaling) composed of the eigenvalues of \\(A\\).</p> <p>The rotation matrices should apply rotations only, so the eigenvectors that compose \\(Q\\) (and \\(Q^T\\)) should be normalised.</p> <p>[!tip] Decomposition Logic</p> <p>The logic behind spectral decomposition is to find the eigenvectors of \\(A\\), rotate them to the cardinal basis, apply the scaling given by the respective eigenvalues and then rotate the eigenvectors back to their original position.</p> <p>[!warning] Symmetric Matrix</p> <p>Spectral Decomposition works only with symmetric matrices. This is because a symmetric matrix has orthogonal eigenvectors, which can align to the cardinal basis.</p>"},{"location":"Physics/","title":"Physics","text":"<p>Physics is a tool we can use to study the real word and its various interactions. It provides formulas, as well as theorems, which the universe is based upon.</p>"},{"location":"Physics/#macro-topics","title":"Macro topics","text":"<ul> <li>Mechanics</li> <li>Fluids and thermodynamics</li> <li>Electricity and magnetism</li> </ul> <p>The course is based on Halliday's &amp; Resnick's book, Fundamentals of Physics</p>"},{"location":"Physics/International%20System/","title":"International System","text":"<p>The International System of Units (SI for short) </p>"},{"location":"Physics/Measurements/","title":"Measurements","text":"<p>To start working with physics, we first need some values to work with. Using the real world, we can take measurements which they give us the values we need.</p> <p>It'd be bad if everybody reads measurements in their own different way, so the International System of Units (SI) defines a systematic way to take measurements, so that everybody can read values the same.</p> <p>The SI defines three things to take measurements: - Base quantities - Unit of Measure - Standards</p>"},{"location":"Physics/Measurements/#base-quantities","title":"Base Quantities","text":"<p>Base quantities are certaint types of measurement which are independent from other quantities. Base quantities, and the other quantities, must be invariable.</p> <p>For example, length and time are base quantities, because they are taken independently. Speed, on the other hand, is NOT a base quantity, because it's a ratio of length and time.</p>"},{"location":"Physics/Measurements/#unit-of-measure","title":"Unit of Measure","text":"<p>The unit is a fixed quantity we assing to a measurement, every measurement will be written as a multiple of said unit.</p> <p>/ TODO: units table</p>"},{"location":"Physics/Measurements/#standards","title":"Standards","text":"<p>Standards are a way to take precise and uniform measurements of quantities around the world.</p> <p>Examples of accesible standards are rulers for length and cronometers for time, but there are more precise standards which define units.</p> <ul> <li>[Length / Meter] Length path travelled by light in a vacuum during a time interval of 1/299'792'458 of a second;</li> <li>[Time / Second] Time taken by \\(9.192\\dots \\times 10^9\\) oscillations of light of a specific wavelength emitted by Casium-133 atom;</li> <li>[Mass / Kilogram] Cylinder of Platinum/Iridium kept at the International Bureou of Weight and Measurement in Paris.</li> </ul>"},{"location":"Probability/","title":"Probability","text":"<p>The study of probability concerns the study of experiments with random outcomes (the outcome of an action isn't predetermined), such as the result of tossing a coin.</p> <p>The study of probability has two main branches:</p> <ul> <li>Discrete Probability, which goal is to study probability on countable sets of outcomes;</li> <li>Continuous Probability, which goal is to study probability on uncountable sets of outcomes.</li> </ul>"},{"location":"Probability/#table-of-contents","title":"Table of Contents","text":"<p>Introduction</p> <ol> <li>Probability Space</li> <li>Combinatorial Analysis</li> <li>Independence</li> <li>Inclusion-Exclusion</li> <li>Conditional Probability</li> <li>Total Probability Law</li> <li>Bayes' Theorem</li> </ol> <p>Random Variables</p> <ol> <li>Probability Distributions</li> <li>Random Variables</li> <li>Expectation</li> <li>Variance</li> <li>Covariance</li> <li>Joint Distribution</li> <li>Conditional Distribution</li> <li>Random Variables Inequalities</li> </ol> <p>Continuous</p> <ol> <li>Continuous Probability</li> <li>Continuous Probability Distributions</li> <li>Continuous Random Variables</li> </ol>"},{"location":"Probability/Continuous/Continuous%20Probability%20Distributions/","title":"Continuous Probability Distributions","text":"<p>\\(\\def \\P#1{{ \\mathbb{P} \\left(#1\\right) }}\\) \\(\\def \\E#1{{ \\mathbb{E} \\left(#1\\right) }}\\) \\(\\def \\Var#1{{ \\text{Var} \\left(#1\\right) }}\\) \\(\\def \\Ind#1#2{{ \\mathbb {1}_{#1} \\left( {#2} \\right) }}\\) \\(\\def \\ND#1#2{{ \\mathcal N \\left( {#1},{#2} \\right) }}\\)</p>"},{"location":"Probability/Continuous/Continuous%20Probability%20Distributions/#continuous-probability-distributions","title":"Continuous Probability Distributions","text":"<p>Here are listed some continuous probability distributions with known features.</p>"},{"location":"Probability/Continuous/Continuous%20Probability%20Distributions/#uniform-distribution","title":"Uniform Distribution","text":"<p>The Uniform Distribution, written as \\(X \\sim Uniform(a,b)\\) or just \\(X \\sim U(a,b)\\), is a distribution that gives to each interval contained in \\([a,b]\\) a probability proportional to its length.</p> <p>Given an interval \\([a,b]\\), the uniform distribution is defined as follows.</p> \\[\\large     f(x) = \\frac{1}{b - a} \\Ind{[a,b]} x \\] <p>Note that if \\(x \\not \\in [a,b]\\) then \\(\\Ind{[a,b]} x = 0\\), hence</p> \\[\\large     \\int_{-\\infty}^{+\\infty} f(x) dx =     \\int_a^b f(x) dx = 1 \\] <p>[!tip] Properties of a Uniform Distribution</p> <p>The expectation of a uniform distribution is half the sum of \\(a\\) and \\(b\\). $$\\large   \\E{ U(a,b) } = \\frac{a+b}{2} $$</p> <p>The variance of a uniform distribution is given by the following expression. $$\\large   \\Var{ U(a,b) } = \\frac{(a-b)^2}{12} $$</p>"},{"location":"Probability/Continuous/Continuous%20Probability%20Distributions/#exponential-distribution","title":"Exponential Distribution","text":"<p>The Exponential Distribution, written as \\(X \\sim Exponential(\\lambda)\\) or just \\(X \\sim exp(\\lambda)\\), is defined as follows.</p> \\[\\large     f(x) = \\lambda e^{-\\lambda x} \\Ind{[0,+\\infty]} x \\] <p>Note that if \\(x &lt; 0\\) then \\(\\Ind{[0,+\\infty]} x = 0\\), hence</p> \\[\\large     \\int_{-\\infty}^{+\\infty} f(x) dx =     \\int_0^\\infty f(x) dx = 1 \\] <p>[!tip] Properties of an Exponential Distribution</p> <p>The expectation of an exponential distribution is inversely proportional to the parameter \\(\\lambda\\). $$\\large   \\E{ exp(\\lambda) } = \\frac{1}{\\lambda} $$</p> <p>The expectation of an exponential distribution is inversely proportional to the square of \\(\\lambda\\). $$\\large   \\Var{ exp(\\lambda) } = \\frac{1}{\\lambda^2} $$</p>"},{"location":"Probability/Continuous/Continuous%20Probability%20Distributions/#cauchy-distribution","title":"Cauchy Distribution","text":"<p>The Cauchy Distribution, written as \\(X \\sim Cauchy(0,1)\\), is defined as follows.</p> \\[\\large     f(x) = \\frac{1}{\\pi (1 + x^2)} \\] <p>Note that by symmetry,</p> \\[\\large     \\int_{-\\infty}^{+\\infty} f(x) dx =     \\frac{2}{\\pi} \\int_0^\\infty \\frac{1}{1 + x^2} dx =     \\frac{2}{\\pi} [\\arctan x]^{+\\infty}_0 = 1 \\] <p>[!tip] Properties of a Cauchy Distribution</p> <p>The integral \\(\\int_{-\\infty}^{+\\infty} x f(x) dx\\) diverges, hence both the expectation and the variance of a Cauchy distribution aren't defined.</p>"},{"location":"Probability/Continuous/Continuous%20Probability%20Distributions/#gaussian-distribution","title":"Gaussian Distribution","text":"<p>The Gaussian distribution with parameters \\(\\mu,\\sigma^2\\), a.k.a. Normal distribution and written as \\(\\ND{\\mu}{\\sigma^2}\\), is defined as follows.</p> \\[\\large     f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}     \\exp \\left( {-\\frac{{(x-\\mu)}^2}{2\\sigma^2}} \\right) \\] <p>A special case of Gaussian distribution is the distribution with parameters \\(\\mu = 0, \\sigma^2 = 1\\); it is written as \\(\\ND{0}{1}\\) and it is called Standard Gaussian or Standard Normal distribution.</p> \\[\\large     f(x) = \\Phi(x) = \\frac{1}{\\sqrt{2\\pi}}     \\exp \\left({ -\\frac{x^2}{2} }\\right) \\] <p>[!tip] Properties of a Normal Distribution</p> <p>The expectation of a normal distribution is given by the parameter \\(\\mu\\). $$\\large   \\E{ \\ND{\\mu}{\\sigma^2} } = \\mu $$</p> <p>The variance of a normal distribution is given by the parameter \\(\\sigma^2\\). $$\\large   \\Var{ \\ND{\\mu}{\\sigma^2} } = \\sigma^2 $$</p>"},{"location":"Probability/Continuous/Continuous%20Probability/","title":"Continuous Probability","text":"<p>\\(\\def \\P#1{{ \\mathbb{P} \\left(#1\\right) }}\\) \\(\\def \\Ind#1#2{{ \\mathbb {1}_{#1} \\left( {#2} \\right) }}\\)</p>"},{"location":"Probability/Continuous/Continuous%20Probability/#continuous-probability","title":"Continuous Probability","text":"<p>Continuous probability distributions are distributions that take values in the whole set of real numbers. Because there are infinite outcomes, \\(\\P A = 0\\) for every non-dense set \\(A\\); probabilities are instead measured on sets of intervals \\([a,b]\\).</p> <p>A function \\(f : R \\rightarrow [0,1]\\) is a probability density function pdf if</p> \\[\\large     f(x) \\ge 0 \\quad \\forall x \\in R,     \\quad \\quad     \\int_{-\\infty}^{+\\infty} f(x) dx = 1 \\] <p>The probability of a value \\(x \\in R\\) belonging in an interval \\([a,b]\\), given a pdf \\(f\\), is defined as</p> \\[\\large     \\P{[a,b]} = \\int_a^b f(x) dx \\] <p>[!note] Interval bounds</p> <p>Note that, (a simple reason, but there are other ones) because single outcomes have infinitesimal weight, $$\\large   \\P{[a,b]} = \\P{[a,b)} = \\P{(a,b]} = \\P{(a,b)} $$</p> <p>An indicator function, which will aid in pdf definitions, can be defined as</p> \\[\\large     \\Ind A x = \\cases{         1 \\quad \\quad \\omega \\in A \\\\         0 \\quad \\quad \\omega \\not \\in A     } \\]"},{"location":"Probability/Continuous/Continuous%20Random%20Variables/","title":"Continuous Random Variables","text":"<p>\\(\\def \\bb#1{{ \\mathbb{#1} }}\\) \\(\\def \\P#1{{ \\mathbb{P} \\left(#1\\right) }}\\) \\(\\def \\E#1{{ \\mathbb{E} \\left(#1\\right) }}\\) \\(\\def \\Var#1{{ \\text{Var} \\left(#1\\right) }}\\)</p>"},{"location":"Probability/Continuous/Continuous%20Random%20Variables/#continuous-random-variables","title":"Continuous Random Variables","text":"<p>A random variable \\(X : \\Omega \\rightarrow \\bb R\\) has a probability density function \\(f\\) if</p> \\[\\large     \\P{X \\in [a,b]} = \\int_a^b f(x) dx     \\quad \\forall a &lt; b \\in \\bb R \\]"},{"location":"Probability/Continuous/Continuous%20Random%20Variables/#continuous-distribution-function","title":"Continuous Distribution Function","text":"<p>The distribution function of a random variable \\(X\\) is defined as</p> \\[\\large     F_X(X) = \\P{X \\le x} = \\int_{-\\infty}^x f(y) dy \\] <p>The distribution function has the following properties:</p> <ol> <li> <p>The pdf is the derivative of the distribution function, i.e. \\(F_X^\\prime(x) = f(x)\\)</p> </li> <li> <p>A random variable \\(X\\) is said to be continuous if the associated distribution function \\(F_X(x)\\) is continuous</p> </li> <li> <p>The limits towards \\(-\\infty,+\\infty\\) of \\(F_X(x)\\) are respectively \\(0,1\\).</p> </li> </ol>"},{"location":"Probability/Continuous/Continuous%20Random%20Variables/#expectation-of-continuous-random-variables","title":"Expectation of Continuous Random Variables","text":"<p>Let \\(X\\) be a continuous random variable with probability density function \\(f\\) and distribution function \\(F\\). Then, the expected value of \\(X\\) is defined as follows.</p> \\[\\large     \\E{X} = \\int_{-\\infty}^{+\\infty} x f(x) dx \\] <p>[!note] Convergence of Expectation</p> <p>Note that the expectation of a continuous random variables isn't ensured to converge to a finite value, but could diverge. For example, the expectation of a Cauchy random variable isn't defined because the integral diverges.</p> <p>The expectation of continuous random variables satisfies the same properties of the expectation of discrete random variables. In particular, the expectation of a function of a continuous random variables is defined as follows.</p> \\[\\large     \\E{g(X)} = \\int_{-\\infty}^{+\\infty} g(x) f(x) dx \\]"},{"location":"Probability/Continuous/Continuous%20Random%20Variables/#variance-of-continuous-random-variables","title":"Variance of Continuous Random Variables","text":"<p>Analogously to the variance of discrete random variables, the variance of a continuous random variable is defined as follows.</p> \\[\\large     \\Var{X} = \\bb E[(X - \\E X)^2]     = \\int_{-\\infty}^{+\\infty} (x - \\mu)^2 f(x) dx \\] \\[\\large     \\Var{X} = \\E{X^2} - \\bb E^2 (X)     = \\int_{-\\infty}^{+\\infty} x^2 f(x) dx     - \\left(\\int_{-\\infty}^{+\\infty} x f(x) dx \\right)^2 \\] <p>[!note]</p> <p>If \\(\\E X\\) isn't defined (i.e. divergent), then \\(\\Var X\\) isn't defined either. On the other hand, if \\(\\E X\\) is defined (i.e. finite value), then \\(\\Var X\\) is defined too.</p>"},{"location":"Probability/Random%20Variables/Conditional%20Distribution/","title":"Conditional Distribution","text":"<p>\\(\\def \\P#1{{ \\mathbb{P} \\left(#1\\right) }}\\)</p>"},{"location":"Probability/Random%20Variables/Conditional%20Distribution/#conditional-distribution","title":"Conditional Distribution","text":"<p>The conditional probability of a random variable \\(X\\), given an event \\(\\set{Y = y}\\) with positive probability, is defined to be the collection of probabilities</p> \\[\\large     \\P{X = x | Y = y} = \\frac{ \\P{X = x, Y = y} }{ \\P{Y = y} }     \\quad \\forall x \\in S_X \\] <p>The marginal distributions can be recovered by the total probability law.</p> \\[\\large     \\P{X = x} = \\sum_{y \\in S_Y} \\P{X = x | Y = y} \\P{Y = y} \\] \\[\\large     \\P{Y = y} = \\sum_{x \\in S_X} \\P{Y = y | X = x} \\P{X = x} \\]"},{"location":"Probability/Random%20Variables/Covariance/","title":"Covariance","text":"<p>\\(\\def \\E#1{{ \\mathbb{E} \\left(#1\\right) }}\\) \\(\\def \\Var#1{{ \\text{Var} \\left(#1\\right) }}\\) \\(\\def \\Cov#1#2{{ \\text{Cov} \\left(#1,#2\\right) }}\\) \\(\\def \\bb#1{{ \\mathbb{#1} }}\\) \\(\\def \\seq#1#2{{ {#1}_1, {#1}_2, \\ldots, {#1}_{#2} }}\\) \\(\\def \\seqop#1#2#3{{ {#2}_1 #1 {#2}_2 #1 \\cdots #1 {#2}_{#3} }}\\) \\(\\def \\indep{{ \\mathrel\\unicode{x2AEB} }}\\)</p>"},{"location":"Probability/Random%20Variables/Covariance/#covariance","title":"Covariance","text":"<p>The covariance of two random variables \\(X,Y\\) is a measure of how much the changes of \\(X\\) and \\(Y\\) are related.</p> <p>The covariance of two random variables \\(X,Y\\), both with finite mean \\(\\E X, \\E Y\\), is given by the following formula.</p> \\[\\large     \\Cov X Y = \\E{ [X - \\E X][Y - \\E Y] } \\] <p>[!tip] Covariance of two Random Variables</p> <p>The covariance of two distributions tell how much the two distributions are related: - if the covariance is positive then there's a proportional trend; - if the covariance is negative then there's an inversely proportional trend.</p>"},{"location":"Probability/Random%20Variables/Covariance/#properties-of-the-covariance","title":"Properties of the Covariance","text":"<ol> <li> <p>\\(\\Cov X Y = \\E{XY} - {\\E X}{\\E Y}\\)</p> </li> <li> <p>\\(\\Cov X X = \\Var X\\)</p> </li> <li> <p>\\(c \\in \\bb R \\Rightarrow \\Cov X c = 0\\)</p> </li> <li> <p>\\(c \\in \\bb R \\Rightarrow \\Cov{cX}{Y} = c \\, \\Cov X Y\\)</p> </li> <li> <p>\\(c \\in \\bb R \\Rightarrow \\Cov{X + c}{Y} = \\Cov X Y\\)</p> </li> <li> <p>\\(X,Y,Z \\text{ r.v.} \\Rightarrow \\Cov{X + Z}{Y} = \\Cov X Y + \\Cov Z Y\\)</p> </li> <li> <p>\\(\\Var{X + Y} = \\Var X + \\Var Y + 2 \\Cov X Y\\)</p> </li> <li> <p>\\(X,Y \\text{ indep.} \\Rightarrow \\Cov X Y = 0\\)</p> </li> </ol>"},{"location":"Probability/Random%20Variables/Joint%20Distribution/","title":"Joint Distribution","text":"<p>\\(\\def \\P#1{{ \\mathbb{P} \\left(#1\\right) }}\\)</p>"},{"location":"Probability/Random%20Variables/Joint%20Distribution/#joint-distribution","title":"Joint Distribution","text":"<p>Given two random variables \\(X : \\Omega \\rightarrow S_X\\) and \\(Y : \\Omega \\rightarrow S_Y\\), their joint distribution is defined as the collection of probabilities</p> \\[\\large     \\P{X = x, Y = y} \\quad \\forall (x,y) \\in S_X \\times S_Y \\] <p>The marginal distributions can be recovered by the total probability law.</p> \\[\\large     \\P{X = x} = \\sum_{y \\in S_Y} \\P{X = x, Y = y} \\] \\[\\large     \\P{Y = y} = \\sum_{x \\in S_X} \\P{X = x, Y = y} \\]"},{"location":"Probability/Random%20Variables/Probability%20Distributions/","title":"Probability Distributions","text":"<p>\\(\\def\\P#1{{ \\mathbb{P} \\left(#1\\right) }}\\) \\(\\def\\bb#1{{ \\mathbb{#1} }}\\) \\(\\def\\cal#1{{ \\mathcal{#1} }}\\)</p>"},{"location":"Probability/Random%20Variables/Probability%20Distributions/#probability-distributions","title":"Probability Distributions","text":"<p>TK !uniform</p> <p>A probability distribution, or simply distribution, is a probability measure \\(\\bb{P}\\) on some sample space and sigma algebra \\((\\Omega, \\cal{F})\\).</p> <p>[!abstract] Discrete Distribution A probability distribution is called discrete if the sample space \\(\\Omega\\) is a discrete set.</p> <p>A distribution defines a weight \\(p_\\omega\\) for each outcome, which can also be seen as the probability of that single outcome occurring; the sequence of weights \\((p_\\omega)_{\\omega \\in \\Omega}\\) uniquely defines \\(\\bb{P}\\).</p> \\[\\large     p_\\omega = \\P{\\set\\omega} \\] <p>[!note] Sum of weights Note that all the outcomes are disjoint between themselves and, by definition of probability measure \\(\\P{\\Omega} = 1\\), hence</p> \\[\\large   \\P{\\Omega} = \\sum_{\\omega \\in \\Omega} p_\\omega = 1 \\]"},{"location":"Probability/Random%20Variables/Probability%20Distributions/#bernoulli-distribution","title":"Bernoulli Distribution","text":"<p>Take \\(\\Omega = \\set{0,1}\\) and define \\(\\bb{P}\\) to be the probability distribution given by the weights</p> \\[\\large     p_0 = 1-p, \\quad p_1 = p \\] <p>\\(\\bb P\\) is called Bernoulli distribution of parameter \\(p\\), it is denoted by \\(Bernoulli(p)\\).</p> <p>[!example] Biased Coin Toss</p> <p>\\(\\bb P\\) models the number of heads obtained in one biased coin toss: heads with probability \\(p\\) and tails with probability \\(1-p\\).</p>"},{"location":"Probability/Random%20Variables/Probability%20Distributions/#geometric-distribution","title":"Geometric Distribution","text":"<p>Let \\(\\Omega = \\set{1,2,\\ldots}\\), define \\(\\bb P\\) to be the probability distribution given by the weights</p> \\[\\large     p_k = (1-p)^{k-1} p, \\quad k \\ge 1. \\] <p>\\(\\bb P\\) is called Geometric distribution of parameter \\(p\\), it is denoted by \\(Geometric(p)\\).</p> <p>[!example] Biased Coin Tosses until Head</p> <p>\\(\\bb P\\) models the number of biased coin tosses up to (and including) the first head; the weight \\(p_k\\) indicates the probability of getting tails from the first toss to the \\((k-1)^\\text{th}\\) and head at the \\(k^\\text{th}\\).</p>"},{"location":"Probability/Random%20Variables/Probability%20Distributions/#binomial-distribution","title":"Binomial Distribution","text":"<p>Fix an integer \\(N \\ge 1\\) and let \\(\\Omega = \\set{1,2,\\ldots,N}\\). Define \\(\\bb P\\) to be the probability distribution given by the weights</p> \\[\\large     p_k = \\binom{N}{k} p^k (1-p)^{N-k},     \\quad 0 \\le k \\le N. \\] <p>\\(\\bb P\\) is called Binomial distribution of parameters \\(N, p\\), it is denoted by \\(Binomial(N,p)\\).</p> <p>[!example] Biased Coin Tosses</p> <p>\\(\\bb P\\) models the number of heads obtained in \\(N\\) biased coin tosses; the weight \\(p_k\\) indicates the probability of getting \\(k\\) heads.</p>"},{"location":"Probability/Random%20Variables/Probability%20Distributions/#poisson-distribution","title":"Poisson Distribution","text":"<p>Let \\(\\Omega = \\bb N\\), and for a fixed parameter \\(\\lambda &gt; 0\\) let \\(\\bb P\\) be the probability distribution given by the weights</p> \\[\\large     p_k = \\frac{ e^{-\\lambda} \\lambda^k }{ k!}, \\quad k \\ge 0. \\] <p>\\(\\bb P\\) is called Poisson distribution of parameter \\(\\lambda\\), it is denoted by \\(Poisson(\\lambda)\\).</p> <p>[!tip]</p> <p>This distribution arises as the limit of a Binomial Distribution with parameters \\(N, \\frac{N}{\\lambda}\\) as \\(N \\rightarrow \\infty\\).</p> <p>[!example]</p> <p>A small business receives, on average, 12 customers per day. What is the probability that the business will receive exactly 8 customers in day?</p> <p>The question models a Poisson Distribution, where: - the days are the basis for sampling, i.e. \\(N \\rightarrow \\infty\\); - the average of customers is the parameter \\(\\lambda\\); - the exact number we're looking for is the parameter \\(k\\).</p> \\[\\large   \\P{customers=8} =   \\frac{ e^{-\\lambda} \\lambda^k }{ k!} =   \\frac{ e^12 \\lambda^8 }{ 8! } \\approx   0.065523 \\]"},{"location":"Probability/Random%20Variables/Random%20Variables%20Inequalities/","title":"Random Variables Inequalities","text":"<p>\\(\\def \\P#1{{ \\mathbb{P} \\left(#1\\right) }}\\) \\(\\def \\E#1{{ \\mathbb{E} \\left(#1\\right) }}\\) \\(\\def \\bb#1{{ \\mathbb{#1} }}\\)</p>"},{"location":"Probability/Random%20Variables/Random%20Variables%20Inequalities/#random-variables-inequalities","title":"Random Variables Inequalities","text":""},{"location":"Probability/Random%20Variables/Random%20Variables%20Inequalities/#markovs-inequality","title":"Markov's Inequality","text":"<p>Let \\(X\\) be a non-negative random variable, \\(\\lambda \\in (0, \\infty)\\) a parameter.</p> \\[\\large     \\P{X \\ge \\lambda} \\le \\frac{\\E X}{\\lambda} \\]"},{"location":"Probability/Random%20Variables/Random%20Variables%20Inequalities/#chebyshevs-inequality","title":"Chebyshev's Inequality","text":"<p>Let \\(X\\) be a random variable with finite mean \\(\\E X\\), \\(\\lambda \\in (0, \\infty)\\) a parameter.</p> \\[\\large     \\P{|X - \\E X| \\ge \\lambda} \\le \\frac{\\Var X}{\\lambda^2} \\]"},{"location":"Probability/Random%20Variables/Random%20Variables%20Inequalities/#cauchy-schwarz-inequality","title":"Cauchy-Schwarz Inequality","text":"<p>Let \\(X,Y\\) be any random variables, then</p> \\[\\large      \\E{|XY|} \\le \\sqrt{\\E{X^2}\\E{Y^2}} \\] <p>Note that, by setting \\(\\P{Y = c} = 1\\) for a \\(c \\in \\bb R\\), then</p> \\[\\large      \\E{|X|} \\le \\sqrt{\\E{X^2}} \\]"},{"location":"Probability/Random%20Variables/Random%20Variables%20Inequalities/#jensens-inequality","title":"Jensen's Inequality","text":"<p>Let \\(X\\) be an integrable random variable, \\(f : R \\rightarrow R\\) a function convex on \\(S_X\\), then</p> \\[\\large     f(\\E X) \\le \\E{f(X)} \\] <p>[!tip]</p> <p>To remember the direction of the inequality: - pick \\(f(x) = x^2\\) - then \\(\\bb E^2 (X) \\le \\E{X^2}\\) - recall that \\(\\Var X = \\E{X^2} - \\bb E^2 (X)\\) is always non-negative.</p>"},{"location":"Probability/Random%20Variables/Random%20Variables/","title":"Random Variables","text":"<p>\\(\\def\\bb#1{{ \\mathbb{#1} }}\\) \\(\\def\\bbP{{ \\mathbb P }}\\) \\(\\def\\bbId{{ \\mathbb 1 }}\\) \\(\\def\\calF{{ \\mathcal F }}\\) \\(\\def\\P#1{{ \\bb{P} \\left( {#1} \\right) }}\\) \\(\\def\\Pid#1#2{{ \\bb{1}_{#1} \\left( {#2} \\right) }}\\) \\(\\def \\seq#1#2{{ {#1}_1, {#1}_2, \\ldots, {#1}_{#2} }}\\) \\(\\def \\seqf#1#2#3{{ {#1}_1({#2}_1), {#1}_2({#2}_2), \\ldots, {#1}_{#3}({#2}_{#3}) }}\\)</p>"},{"location":"Probability/Random%20Variables/Random%20Variables/#random-variables","title":"Random Variables","text":"<p>Random Variables are functions of the outcome of an experiment.</p> <p>A random variable on \\((\\Omega, \\cal F)\\) taking values in a discrete set \\(S\\) is a function \\(X : \\Omega \\rightarrow S\\).</p> <p>[!example] Toss two dice, then \\(\\Omega = \\set{ (i,j) : 1 \\le i,j \\le 6 }\\).</p> <p>Define \\(X(\\omega) = X(i,j) = i+j\\) \\(\\quad \\Longrightarrow \\quad\\) \\(X : \\Omega \\rightarrow \\set{2, \\ldots, 12}\\)</p> <p>\\(X\\) is the sum of the outcome of the two dice.</p>"},{"location":"Probability/Random%20Variables/Random%20Variables/#probability-distribution-of-a-rv","title":"Probability Distribution of a RV","text":"<p>Since \\(X\\) takes values in a discrete set \\(S\\), let</p> \\[\\large     p_x = \\P{X = x} \\quad \\forall x \\in S. \\] <p>The collection \\((p_x)_{x \\in S}\\) is referred to as the probability distribution of \\(X\\).</p> <p>[!info] Distribution Notation If the distribution of \\(X\\) is well known (e.g. Bernoulli), then we can also write \\(X \\sim Bernoulli(p)\\).</p> <p>The function \\(F_X : \\bb R \\rightarrow [0,1]\\) given by</p> \\[\\large     F_X(x) = \\P{X \\le x} \\] <p>is called distribution function of \\(X\\).</p> <p>[!note] Properties of Distribution Function Note that: 1. \\(F_X\\) is piecewise constant, non-decreasing, right-continuous 2. The jumps are given by the weights of the distribution 3. \\(\\large \\lim\\limits_{x \\rightarrow -\\infty}{ F_X(x) } = 0\\) 4. \\(\\large \\lim\\limits_{x \\rightarrow +\\infty}{ F_X(x) } = 1\\)</p> <p>Knowing the probability distribution function \\(F_X\\) is equivalent to knowing the collection of weights \\((p_x)_{x \\in S}\\) such that \\(p_x = \\P{X = x}\\), hence it is equivalent to knowing the probability distribution of \\(X\\).</p>"},{"location":"Probability/Random%20Variables/Random%20Variables/#independence-of-rv","title":"Independence of RV","text":"<p>Two random variables \\(X,Y\\), taking values in \\(S_X,S_Y\\) respectively, are said to be independent if</p> \\[\\large     \\P{X = x, Y = y} = \\P{X = x} \\P{Y = y}     \\quad \\forall (x,y) \\in S_X \\times S_Y. \\] <p>In general, the random variables \\(X_1, X_2, \\ldots, X_n\\) taking values in \\(S_1, S_2, \\ldots, S_n\\) are said to be independent if</p> \\[\\large \\displaylines{     \\P{X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n}     = \\prod_{i=1}^n \\P{X_i = x_i} \\\\     \\forall (x_1, x_2, \\ldots, x_n) \\in \\bigtimes_{i=1}^n S_i. }\\] <p>[!info] Sub-independence If \\(\\sigma = \\set{X_1, X_2, \\ldots, X_n}\\) is a set of independent random variables, then any subset \\(\\sigma' \\subseteq \\sigma\\) with \\(|\\sigma'| \\ge 2\\) is a set of independent random variables.</p> <p>Moreover, given any number of independent random variables \\(\\seq X n\\) and functions \\(\\seq f n\\) such that \\(f_i : S_i \\rightarrow S_i'\\), then the random variables \\(\\seqf f X n\\) are independent.</p>"},{"location":"Probability/Random%20Variables/Random%20Variables/#identity-rv","title":"Identity RV","text":"<p>For a given probability space \\(( \\Omega, \\calF, \\bbP )\\) and \\(A \\in \\calF\\), define</p> \\[\\large     \\Pid{A}{\\omega} = \\cases{         1 \\quad \\text{if } \\omega \\in A \\\\         0 \\quad \\text{otherwise}     } \\] <p>\\(\\bbId_A : \\Omega \\rightarrow \\set{0, 1}\\) tells us whether the outcome is in \\(A\\) or not.</p> <p>[!note] Properties of \\(\\Pid{}{\\omega}\\) 1. \\(\\large \\bbId_{A^c} = 1 - \\bbId_A\\) 2. \\(\\large \\bbId_{A \\cap B} = \\bbId_A \\bbId_B\\) 3. \\(\\large \\bbId_{A \\cup B} = 1 - (1-\\bbId_A) (1-\\bbId_B)\\)</p>"},{"location":"Probability%20and%20Statistics/","title":"Probability and Statistics","text":"<p>Probability and Statistics are both tools which can be used to understand and study uncertainty in events and experiments. Probability is about the likelihood of events happening, while statistics is about collecting and analysing data to make inferences about the events.</p>"},{"location":"Probability%20and%20Statistics/#table-of-contents","title":"Table of Contents","text":"<p>Probabilistic Introduction</p> <ol> <li>Probability Space</li> <li>Combinatorial Analysis</li> <li>Independence</li> <li>Inclusion-Exclusion</li> <li>Conditional Probability</li> <li>Total Probability Law</li> <li>Bayes' Theorem</li> </ol> <p>Statistic Introduction</p> <ol> <li>Sample and Population</li> <li>Variables</li> <li>Data Graphs</li> </ol> <p>Distribution Properties</p> <p>Common Distributions</p> <p>Random Variables</p> <ol> <li>Probability Distributions</li> <li>Random Variables</li> <li>Expectation</li> <li>Variance</li> <li>Covariance</li> <li>Joint Distribution</li> <li>Conditional Distribution</li> <li>Random Variables Inequalities</li> </ol> <p>Continuous</p> <ol> <li>Continuous Probability</li> <li>Continuous Probability Distributions</li> <li>Continuous Random Variables</li> </ol>"},{"location":"Probability%20and%20Statistics/Distribution%20Properties/Expectation/","title":"Expectation","text":"<p>\\(\\def\\P#1{{ \\mathbb{P} \\left(#1\\right) }}\\) \\(\\def\\E#1{{ \\mathbb{E} \\left(#1\\right) }}\\) \\(\\def\\X#1{{ \\mathbb{X} \\left(#1\\right) }}\\) \\(\\def\\bb#1{{ \\mathbb{#1} }}\\) \\(\\def\\cal#1{{ \\mathcal{#1} }}\\) \\(\\def\\seq#1#2{{ #1_1, #1_2, \\ldots, #1_#2 }}\\) \\(\\def\\seqf#1#2#3{{ #1_1(#2_1), #1_2(#2_2), \\ldots, #1_#3(#2_#3) }}\\) \\(\\def\\indep{{ \\mathrel\\unicode{x2AEB} }}\\)</p>"},{"location":"Probability%20and%20Statistics/Distribution%20Properties/Expectation/#expectation","title":"Expectation","text":"<p>The expectation of a random variable \\(X\\), often referred to as expected or mean value, is the average of the values taken by \\(X\\), averaged by the weights of the values.</p> <p>There are various ways to compute the expectation:</p> <ol> <li>Indexed by the weights and values of the distribution</li> </ol> \\[\\large \\begin{aligned}     \\E X &amp;= \\sum_{x \\in S} x \\P{X=x} \\\\     &amp;= \\sum_{x \\in S} x p_x \\end{aligned} \\] <ol> <li>Indexed by the events of the random variable</li> </ol> \\[\\large     \\E X = \\sum_{\\omega \\in \\Omega} X(\\omega) \\P{\\set\\omega} \\] <ol> <li>Uniform distribution</li> </ol> \\[\\large     \\E X = \\frac{1}{n} \\sum_{i=1}^n x_i \\] <p>[!cite]- Converting from events-index to values-index</p> \\[\\large \\begin{aligned}   \\sum_{\\omega \\in \\Omega} X(\\omega) \\P{\\set\\omega}   &amp;= \\sum_{x \\in S_X} \\sum_{\\omega \\in \\Omega : X(\\omega) = x} X(\\omega) \\P{\\set\\omega}   \\\\\\\\   &amp;= \\sum_{x \\in S_X} x   \\underbrace{ \\sum_{\\omega \\in \\Omega : X(\\omega) = x} \\P{\\set\\omega} }_{ \\P{X = x} }   \\\\\\\\   &amp;= \\sum_{x \\in S_X} x \\P{X = x} \\end{aligned} \\]"},{"location":"Probability%20and%20Statistics/Distribution%20Properties/Expectation/#properties-of-expectations","title":"Properties of Expectations","text":"<ol> <li>Non-negative Random Variables</li> </ol> <p>A non-negative random variable will always have non-negative expectation.</p> \\[\\large     X \\ge 0     \\quad \\Rightarrow \\quad     \\E{X} \\ge 0 \\] <p>The only way for a non-negative random variable to have expectation zero is for the random variable to have zero as the only result.</p> \\[\\large     X \\ge 0 \\land \\E{X} = 0     \\quad \\Leftrightarrow     \\P{X = 0} = 1 \\] <ol> <li>Scalar multiplication</li> </ol> <p>Let \\(c \\in \\bb R\\) be a constant, then</p> \\[\\large \\begin{aligned}     \\E{c} &amp;= c \\\\     \\E{cX} &amp;= c \\, \\E{X} \\end{aligned} \\] <ol> <li>Sum of Random Variables</li> </ol> <p>For any random variables \\(X,Y\\), the expectation of the sum of the two is the sum of the expectations.</p> \\[\\large     \\E{X+Y} = \\E{X} + \\E{Y} \\] <p>[!note] Linearity of Expectation</p> <p>By generalizing properties 2 and 3, it can be seen that the expectation is a linear function.</p> \\[\\large   \\E{ \\sum_{k=1}^n c_k X_k }   = \\sum_{k=1}^n c_k \\E{ X_k } \\] <ol> <li>Function of Random Variables</li> </ol> <p>Given any function \\(g : S \\rightarrow S'\\), \\(g(X) : \\Omega \\rightarrow S'\\) is a random variables taking values in \\(S'\\). Its expectation is given by the following formula.</p> \\[\\large     \\E{ g(X) } = \\sum_{x \\in S} g(x) \\P{X = x} \\] <ol> <li>TK</li> </ol> <p>If \\(X\\) takes non-negative integer values, then the expectation is also given by the following formula.</p> \\[\\large     X \\subseteq \\bb N     \\Rightarrow     \\E{X} = \\sum_{k=1}^\\infty \\P{X \\ge k} \\] <ol> <li>Product of Independent Random Variables</li> </ol> <p>If \\(X,Y\\) are two independent random variables, then the expectation of their product is given by the product of their expectations. More generally, if \\(\\seq X n\\) are independent random variables, the expectation of their product is given by the product of their expectations.</p> \\[\\large     \\E{X_1, X_2, \\ldots, X_n} = \\prod_{k=1}^n \\E{X_k} \\]"},{"location":"Probability%20and%20Statistics/Distribution%20Properties/Expectation/#expectations-of-common-distributions","title":"Expectations of Common Distributions","text":"<p>TK move in respective distribution pages $$\\large \\begin{aligned}     X &amp;\\sim Bernoulli(p) &amp;\\Longrightarrow \\E{X} &amp;= p     \\     X &amp;\\sim Binomial(N,p) &amp;\\Longrightarrow \\E{X} &amp;= Np     \\     X &amp;\\sim Geometric(p) &amp;\\Longrightarrow \\E{X} &amp;= \\frac{1}{p}     \\     X &amp;\\sim Poisson(\\lambda) &amp;\\Longrightarrow \\E{X} &amp;= \\lambda \\end{aligned} $$</p>"},{"location":"Probability%20and%20Statistics/Distribution%20Properties/Variance/","title":"Variance","text":"<p>\\(\\def \\E#1{{ \\mathbb{E} \\left(#1\\right) }}\\) \\(\\def \\Var#1{{ \\text{Var} \\left(#1\\right) }}\\) \\(\\def \\Cov#1{{ \\text{Cov} \\left(#1\\right) }}\\) \\(\\def \\bb#1{{ \\mathbb{#1} }}\\) \\(\\def \\seq#1#2{{ {#1}_1, {#1}_2, \\ldots, {#1}_{#2} }}\\) \\(\\def \\seqop#1#2#3{{ {#2}_1 #1 {#2}_2 #1 \\cdots #1 {#2}_{#3} }}\\)</p>"},{"location":"Probability%20and%20Statistics/Distribution%20Properties/Variance/#variance","title":"Variance","text":"<p>The variance of a random variable \\(X\\), often referred to as \\(\\sigma^2\\), is a measure of how much the values of \\(X\\) deviate from its mean, i.e. how close they are to the mean:</p> <ul> <li>a high variance means that the values are mostly far from the mean;</li> <li>a low variance means that the values are mostly close to the mean.</li> </ul> <p>In a probabilistic context, the variance of a random variable \\(X\\) with finite mean \\(\\E X\\) is given by the following formula.</p> \\[\\large     \\Var X = \\E{ [ X - \\E{X} ]^2 } \\] <p>In a statistic context, the variance of a discrete distribution \\(X\\) with cardinality \\(n\\) and mean \\(\\mu\\) is given by the following formula.</p> \\[\\large     \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2 \\] <p>[!note] Bias</p> <p>Smaller samples may not be as accurate as bigger samples, making the statistical inference not as correct as it might be (what it is referred to as bias).</p> <p>To ease this problem the sample variance is inferred to be a bit higher than the population variance by dividing the sum by \\(n-1\\) instead of \\(n\\). This makes it such that datasets with high cardinality will be almost unaffected, while datasets with a lower cardinality will have an higher variance.</p> \\[\\large   \\sigma^2 = \\frac{1}{n-1} \\sum_{i=1}^{n}( x_i - \\mu )^2 \\]"},{"location":"Probability%20and%20Statistics/Distribution%20Properties/Variance/#properties-of-the-variance","title":"Properties of the Variance","text":"<ol> <li>Alternative formula</li> </ol> \\[\\large     \\Var X = \\E{X^2} - [\\E X]^2 \\] <p>[!quote]- Proof</p> \\[\\large \\begin{aligned}   \\Var X &amp;= \\E{ [X - \\E{X}]^2 } \\\\   &amp;= \\E{ X^2 - 2 \\cdot \\E{X} \\cdot X + [\\E X]^2 } \\\\   &amp;= \\E{X^2} + \\E{-2 \\cdot \\E{X} \\cdot X} + [\\E X]^2 \\\\   &amp;= \\E{X^2} - 2 \\cdot \\E{X} \\cdot \\E{X} + [\\E X]^2 \\\\   &amp;= \\E{X^2} - 2 [\\E X]^2 + [\\E X]^2 \\\\   &amp;= \\E{X^2} - [\\E X]^2 \\end{aligned} \\] <ol> <li>Alternative formula when zero expectation</li> </ol> \\[\\large     \\E X = 0     \\quad \\Rightarrow \\quad     \\Var X = \\E{X^2} \\] <ol> <li>Variance of a constant</li> </ol> \\[\\large     \\Var X = 0     \\quad \\Leftrightarrow \\quad     \\P{X = c} = 1 \\] <ol> <li>Scaling of Random Variable</li> </ol> \\[\\large     \\Var{cX} = c^2 \\Var X \\] <p>[!quote]- Proof</p> \\[\\large \\begin{aligned}   \\Var{cX} &amp;= \\E{[cX]^2} - [\\E{cX}]^2 \\\\   &amp;= \\E{c^2 X^2} - [c \\E X]^2 \\\\   &amp;= c^2 \\, \\E{X^2} - c^2 [\\E X]^2 \\\\   &amp;= c^2 \\, [\\E{X^2} - \\bb E^2(X)] \\\\   &amp;= c^2 \\, \\Var X \\end{aligned} \\] <ol> <li>Displacement of Random Variable</li> </ol> \\[\\large     \\Var{X + c} = \\Var X \\] <p>[!quote]- Proof</p> \\[\\large \\begin{aligned}   \\Var{X + c} &amp;= \\E{ [ (X + c) - \\E{X + c} ]^2 } \\\\   &amp;= \\E{ [ X + c - \\E{X} - c ]^2 } \\\\   &amp;= \\E{ [ X - \\E{X} ]^2 } \\\\   &amp;= \\Var X \\end{aligned} \\] <ol> <li>Sum of independent Random Variables</li> </ol> \\[\\large \\displaylines{     \\seq Xn \\Rightarrow \\\\     \\Var{\\seqop+Xn} = \\Var{X_1} + \\Var{X_2} + \\cdots \\Var{X_n} } \\]"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Bayes%27%20Theorem/","title":"Bayes' Theorem","text":"<p>\\(\\def\\P#1{{ \\mathbb{P} \\left(#1\\right) }}\\)</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Bayes%27%20Theorem/#bayes-theorem","title":"Bayes' Theorem","text":"<p>From conditional probability, if \\(A,B\\) are two events with positive probabilities, we have</p> \\[\\large     \\P{A \\cap B} = \\P{A|B} \\P{B} = \\P{B|A} \\P{A} \\] <p>Bayes' theorem states how to reverse the conditioning, i.e. how compute \\(\\P{A|B}\\) from \\(\\P{B|A}\\):</p> \\[\\large     \\P{A|B} = \\frac{ \\P{B|A} \\P{A} }{ \\P{B} } \\] <p>[!tip] Proof \\(\\(\\large \\begin{aligned}   \\P{A|B} &amp;= \\frac{ \\P{A \\cap B} }{ \\P{B} } \\\\   &amp;= \\frac{ \\P{B|A} \\P{A} }{ \\P{B} } \\end{aligned}\\)\\)</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Combinatorial%20Analysis/","title":"Combinatorial Analysis","text":"<p>To compute the probability in a probability space with equally likely outcomes, it is important to know the number of outcomes included in an event.</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Combinatorial%20Analysis/#multiplication-rule","title":"Multiplication Rule","text":"<p>Take \\(N\\) finite sets \\(\\Omega_1, \\Omega_2, \\ldots, \\Omega_N\\) with cardinalities \\(|\\Omega_k| = n_k\\). How many ways are there to pick exactly one element from each set?</p> <p>Logically, there are \\(n_1\\) choices from the first set. Chosen the first element, there are other \\(n_2\\) choices from the second set, leading to \\(|\\Omega_1 \\times \\Omega_2| = n_1 n_2\\). The same logic can then be applied iteratively to the other sets, giving the so called multiplication rule:</p> \\[\\large     | \\Omega_1 \\times \\Omega_2 \\times \\ldots \\times \\Omega_N |     = n_1 n_2 \\ldots n_N \\]"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Combinatorial%20Analysis/#permutations","title":"Permutations","text":"<p>A permutation of a set is a bijection from the set to another ordering of the same set. How many permutations are there for a set of cardinality \\(n\\)?</p> <p>There are \\(n\\) possible elements for the first choice, then \\(n-1\\) possible elements for the second choice (because one has been selected as first choice), \\(n-2\\) possible elements for the third choice and so on. So, by the multiplication rule,</p> \\[\\large     n (n-1) (n-2) \\cdots 1 = n! \\] <p>Hence, there are \\(n!\\) different orderings, or permutations, for a set of \\(n\\) elements. Equivalently, there are \\(n!\\) different bijections from any set of length \\(n\\) to another set of the same length.</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Combinatorial%20Analysis/#subsets","title":"Subsets","text":"<p>How many ways there are to choose \\(k\\) elements from a set of \\(n\\) elements? Remember that when choosing elements, one could care for the order in which the elements are being chosen or not.</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Combinatorial%20Analysis/#subsets-with-ordering","title":"Subsets with Ordering","text":"<p>A formula can be obtained using the same logic as the permutations: there are \\(n\\) possibile elements for the first choice, \\(n-1\\) possibile elements for the second choice, until we get to the \\(k^{th}\\) choice with \\(n-(k-1) = n-k+1\\) possible elements.</p> \\[\\large     n (n-1) \\cdots (n-k+1) = \\frac{ n! }{ (n-k)! } \\]"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Combinatorial%20Analysis/#subsets-without-ordering","title":"Subsets without Ordering","text":"<p>To count how many ways there are to choose \\(k\\) unordered elements from \\(n\\), one could first choose \\(k\\) ordered elements and then forget about the ordering.</p> <p>There are \\(\\frac{n!}{(n-k)!}\\) ways to choose \\(k\\) ordered elements from \\(n\\), and we know there are \\(k!\\) permutations (orderings) for these elements. Hence,</p> \\[\\large     \\binom{n}{k} = \\frac{ n! }{ k! (n-k)! } \\] <p>[!info] Binomial Coefficient The notation \\(\\binom{n}{k}\\) is called binomial coefficient, and it is read \\(n\\) choose \\(k\\). Literally, it indicates the number of possibile ways there are to choose \\(k\\) unordered elements from \\(n\\) elements.</p> <p>Moreover, a more general question can be posed: how many ways there are to partition \\(n\\) elements into unordered groups of sizes \\(n_1, n_2, \\ldots, n_k\\) (with \\(\\sum{n_i} = n\\))?</p> \\[\\large     \\binom{n}{n_1 \\quad n_2 \\quad \\ldots \\quad n_k}     = \\frac{ n! }{ n_1! n_2! \\cdots n_k!} \\] <p>[!info] Multinomial Coefficient The notation TK is called multinomial coefficient, it is read the same as the binomial coefficient.</p> <p>[!tip] Choosing and Partitioning $$ \\frac{ n! }{ k! (n-k)! } = \\binom{n}{k} = \\binom{n}{n-k} = \\binom{n}{ k \\quad n-k } $$ In fact, choosing \\(k\\) unordered elements from \\(n\\) is the same as choosing the remaining \\(n-k\\), which is the same as partitioning \\(n\\) elements into two groups of sizes \\(k\\) and \\(n-k\\).</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Combinatorial%20Analysis/#subsets-with-repetition","title":"Subsets with Repetition","text":"<p>How many ways there are to choose \\(k\\) elements from a set of \\(n\\) elements, allowing repetitions?</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Combinatorial%20Analysis/#subsets-with-repetition-and-ordering","title":"Subsets with Repetition and Ordering","text":"<p>Simply, there are \\(n\\) possibilities for the first choice, \\(n\\) possibilities for the second and so on,</p> \\[\\large     \\underbrace{n \\cdot n \\cdots n}_{k \\text{ times}} = n^k \\]"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Combinatorial%20Analysis/#subsets-with-repetition-without-ordering","title":"Subsets with Repetition without Ordering","text":"<p>TK</p> \\[\\large     \\binom{n+k-1}{k} = \\frac{ (n+k-1)! }{ k! (n-1)! } \\]"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Combinatorial%20Analysis/#combinatorics-formulas","title":"Combinatorics Formulas","text":"Multiplication Rule Permutations \\(\\lvert \\bigtimes_{k=1}^N \\Omega_k \\rvert = \\prod_{k=1}^N n_k\\) \\(n!\\) Subsets <code>Ordering</code> <code>No ordering</code> <code>No repetitions</code> \\(\\frac{ n! }{ (n-k)! }\\) \\(\\binom{n}{k} = \\frac{ n! }{ k! (n-k)! }\\) <code>Repetitions</code> \\(n^k\\) \\(\\binom{n+k-1}{k} = \\frac{ (n+k-1)! }{ k! (n-1)! }\\)"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Conditional%20Probability/","title":"Conditional Probability","text":"<p>\\(\\def\\P#1{{ \\mathbb{P} \\left(#1\\right) }}\\) \\(\\def\\indep{{ \\mathrel{\\unicode{x2AEB}} }}\\)</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Conditional%20Probability/#conditional-probability","title":"Conditional Probability","text":"<p>Conditional Probability is a concept closely related to event independence.</p> <p>Let \\(A,B\\) be two events with \\(\\P{B}&gt;0\\). Then, the conditional probability of \\(A\\) given \\(B\\) is</p> \\[\\large     \\P{A|B} = \\frac{ \\P{A \\cap B} }{ \\P{B} } \\] <p>The interpretation of \\(\\P{A|B}\\) is the probability of the event \\(A\\), knowing that the event \\(B\\) occurred first.</p> <p>[!tip] Independent Events</p> <p>If \\(A \\indep B\\), $$\\large   \\P{A|B} = \\frac{ \\P{A \\cap B} }{ \\P{B} }   = \\frac{ \\P{A}\\P{B} }{ \\P{B} }   = \\P{A} $$ Indeed, if \\(A,B\\) are independent, then \\(B\\) occurring first shouldn't change the likelihood of \\(A\\) happening.</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Inclusion-Exclusion/","title":"Inclusion Exclusion","text":"<p>\\(\\def\\P#1{{ \\mathbb{P} \\left({#1}\\right) }}\\) \\(\\def \\seq#1#2{{ {#1}_1, {#1}_2, \\ldots, {#1}_{#2} }}\\)</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Inclusion-Exclusion/#inclusion-exclusion","title":"Inclusion-Exclusion","text":"<ul> <li>For any two events \\(A,B\\),</li> </ul> \\[\\large     \\P{A \\cup B} = \\P{A} + \\P{B} - \\P{A \\cap B} \\] <ul> <li>For any three events \\(A,B,C\\),</li> </ul> \\[\\large \\begin{aligned}     \\P{A \\cup B \\cup C}     &amp;= \\P{A} + \\P{B} + \\P{C} \\\\     &amp;- \\P{A \\cap B} - \\P{A \\cap C} - \\P{B \\cap C} \\\\     &amp;+ \\P{A \\cap B \\cap C} \\end{aligned} \\] <ul> <li>In general, for any \\(n\\) events \\(\\seq A n\\),</li> </ul> \\[\\large     \\P{ \\bigcup_{i=1}^n A_i }     = \\sum_{k=1}^n{ (-1)^{k+1} }     \\sum_{ 1 \\le i_1 &lt; \\cdots &lt; i_k \\le n }{ \\P{ \\bigcap_{i_k} A_{i_k}} } \\]"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Independence/","title":"Independence","text":"<p>\\(\\def\\P#1{{ \\mathbb{P} \\left( {#1} \\right) }}\\) \\(\\def\\indep{{ \\mathrel{\\unicode{x2AEB}} }}\\)</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Independence/#independence","title":"Independence","text":"<p>Independence is a property of two or more events, it indicates whether the probability of the events is influenced by one of the events happening first.</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Independence/#pairwise-independence","title":"Pairwise Independence","text":"<p>Two events \\(A,B\\) are said to be independent if</p> \\[\\large     \\P{A \\cap B} = \\P{A}\\P{B} \\] <p>It can be proven that, if \\(A \\indep B\\), then \\(A \\indep B^c\\), \\(A^c \\indep B\\) and \\(A^c \\indep B^c\\).</p> \\[\\large     \\P{A \\cap B^c} = TK \\]"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Independence/#general-independence","title":"General Independence","text":"<p>A notion of general independence can also be defined: \\(n\\) events \\(A_1, A_2, \\ldots, A_n\\) are said to be independent if, for any \\(k \\ge 2\\) and any collection of distinct indices \\(1 \\le i_1 &lt; \\cdots &lt; i_k \\le n\\),</p> \\[\\large     \\P{ \\bigcap_{i_k}{ A_{i_k} } } = \\prod_{i_k}{ \\P{ A_{i_k} }} \\] <p>[!warning] General and Pairwise Independence If the \\(n\\) events are independent, then every subset of the \\(n\\) events must be independent. Beware, though, that the converse does not apply: if three events are all pairwise-independent, then the three events could and could not be independent.</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Probability%20Space/","title":"Probability Space","text":"<p>\\(\\def\\P#1{{ \\mathbb{P} \\left(#1\\right) }}\\) \\(\\def\\bb#1{{ \\mathbb{#1} }}\\) \\(\\def\\cal#1{{ \\mathcal{#1} }}\\)</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Probability%20Space/#sample-space","title":"Sample Space","text":"<p>\\(\\Omega = \\set{ \\omega_1, \\omega_2, \\ldots }\\) is called the sample space, while the elements are called outcomes. \\(\\Omega\\) denotes the set of all possible outcomes for an action.</p> <p>[!example] Sample Space Let the action be: - the toss of a coin, then \\(\\Omega = \\set{H, T}\\); - the toss of two coins, then \\(\\Omega = \\set{HH, HT, TH, TT}\\).</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Probability%20Space/#countable-spaces","title":"Countable Spaces","text":"<p>A sample space \\(\\Omega\\) is said countable if there exists a bijection between \\(\\Omega\\) and a subset of \\(\\bb{N}\\).</p> <p>Examples of countable spaces are:</p> <ul> <li>The set of outcomes of throwing a die;</li> <li>The set of natural numbers;</li> <li>The set of integer numbers;</li> <li>The set of outcomes of throwing a coin infinitely many times.</li> </ul> <p>Examples of uncountable spaces are:</p> <ul> <li>The set of numbers between 0 and 1.</li> <li>The set of real numbers;</li> </ul> <p>[!tip] Integers Bijection TK</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Probability%20Space/#events","title":"Events","text":"<p>A subset \\(A \\subseteq \\Omega\\) is called an event.</p> <p>TK operations with events</p> <p>[!example] Events Let \\(\\Omega = \\set{1,2,\\ldots,6}\\) be the outcomes of throwing a die. Then, examples of events are: - \\(A = \\set\\text{N is even} = \\set{2,4,6}\\) - \\(B = \\set\\text{N is 5} = \\set{5}\\) - \\(C = A \\cup B = \\set\\text{N is even or N is 5} = \\set{2,4,5,6}\\)</p> <p>For every event can be defined a complementary event such that: - \\(A \\cup A^c = \\Omega\\) - \\(A \\cap A^c = \\emptyset\\) - \\(\\Omega \\setminus A = A^c\\)</p> <p>[!example] Complementary Events Let \\(\\Omega\\) denote the outcomes of throwing a die. Then \\(\\Omega = \\{1, 2, \\ldots, 6\\}\\).</p> <p>Examples of events are: - \\(A^c = \\set\\text{N is odd} = \\set{1,3,5}\\) - \\(B^c = \\set\\text{N is not 5} = \\set{1,2,3,4,6}\\) - \\(C^c = (A \\cup B)^c = \\set\\text{N is neither even nor 5} = \\set{1,3}\\)</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Probability%20Space/#observable-events","title":"Observable Events","text":"<p>Let \\(\\cal{F} = \\set{ A : A \\subseteq \\Omega}\\), then \\(\\cal{F}\\) contains all the possible events of a sample space. \\(\\cal{F}\\) is said to be the collection of observable events. If an event \\(A \\in \\cal{F}\\), then \\(\\P{A}\\) is said to be the probability of the event.</p> <p>[!note] Properties of \\(\\cal{F}\\) - \\(\\Omega \\in \\cal{F}\\) - \\(\\emptyset \\in \\cal{F}\\) - \\(A \\in \\cal{F} \\Leftrightarrow A^c \\in \\cal{F}\\) - for any sequence of events \\((A_n)_{n \\ge 1} \\in \\cal{F}\\), it holds \\(\\bigcup\\limits_{n=1}^\\infty A_n \\in \\cal{F}\\)</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Probability%20Space/#probability-measure","title":"Probability Measure","text":"<p>A probability measure is a function that maps events to a value \\(\\in [0, 1]\\), it indicates the likely of a certain outcome or event to happen.</p> <p>Formally, a function \\(\\bb{P} : \\cal{F} \\rightarrow [0, 1]\\) is called a probability measure if: - \\(\\P{\\Omega} = 1\\) - For any sequence of disjoint events \\((A_n)_{n \\ge 1}\\) it holds \\(\\(\\P{ \\bigcup\\limits_{n=1}^\\infty{ A_n } } = \\sum_{n=1}^\\infty \\P{A_n}\\)\\)</p> <p>[!note] Properties of Probability Measures - \\(0 \\le \\P{A} \\le 1 \\quad \\forall A \\in \\cal{F}\\) - \\(\\P{A^c} = 1 - \\P{A}\\) - \\(\\P{\\emptyset} = 0\\) - \\(\\P{A \\cup B} = \\P{A} + \\P{B} - \\P{A \\cap B}\\) - \\(\\P{A \\cup B} = \\P{A} + \\P{B} \\Leftarrow A \\cap B = \\emptyset\\) - \\(\\P{A \\cup B} \\le \\mathbb{P}(A) + \\mathbb{P}(B)\\) - \\(A \\subseteq B \\Rightarrow \\P{A} \\le \\P{B}\\)</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Probability%20Space/#probability-space_1","title":"Probability Space","text":"<p>A probability space is defined by the triple \\((\\Omega, \\cal{F}, \\bb{P})\\), where:</p> <ul> <li>\\(\\Omega\\) is the sample space;</li> <li>\\(\\cal{F}\\) is the set of all observable events;</li> <li>\\(\\bb{P}\\) is the probability measure.</li> </ul>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Probability%20Space/#uniform-probability-space","title":"Uniform Probability Space","text":"<p>The simplest case of probability space is that of a finite sample space \\(\\Omega\\) and equally likely outcomes:</p> \\[     \\bb{P}( \\set\\omega ) = \\frac{1}{ |\\Omega| }     \\quad \\forall \\omega \\in \\cal{F} \\] <p>By definition of probability measure, because every set \\(\\set{\\omega_1}\\) is disjoint from every other set \\(\\set{\\omega_2}\\), this implies that</p> \\[     \\P{A} = \\frac{ |A| }{ |\\Omega| }     \\quad \\forall A \\in \\cal{F} \\]"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Total%20Probability%20Law/","title":"Total Probability Law","text":"<p>\\(\\def\\P#1{{ \\mathbb{P} \\left(#1\\right) }}\\) \\(\\def\\seq#1#2{{ \\left( #1_#2 \\right)_{#2 \\ge 1} }}\\)</p>"},{"location":"Probability%20and%20Statistics/Probabilistic%20Introduction/Total%20Probability%20Law/#law-of-total-probability","title":"Law of Total Probability","text":"<p>From conditional probability, it can be seen that the probability of two events happening simultaneously can be derived calculating two consecutive probabilities: the probability that \\(B\\) occurs, then the probability that \\(A\\) occurs given that \\(B\\) has occurred.</p> \\[\\large     \\P{A \\cap B} = \\P{A|B} \\P{B} \\] <p>The same formula applies to \\(A\\) and \\(B^c\\),</p> \\[\\large     \\P{A \\cap B^c} = \\P{A|B^c} \\P{B^c} \\] <p>Then, since \\(B\\) and \\(B^c\\) are disjoint,</p> \\[\\large\\begin{aligned}     \\P{A}     &amp;= \\P{A \\cap B} + \\P{A \\cap B^c} \\\\     &amp;= \\P{A|B} \\P{B} + \\P{A|B^c} \\P{B^c} \\end{aligned}\\] <p>More generally, let \\(\\seq{B}{n}\\) be a sequence of disjoint events of positive probability, whose union is the sample space \\(\\Omega\\). Then, for all events \\(A\\)</p> \\[\\large     \\P{A} = \\sum_{n \\ge 1}{ \\P{A|B_n} \\P{B_n} } \\]"},{"location":"Probability%20and%20Statistics/Statistic%20Introduction/Data%20Graphs/","title":"Graphs","text":"<p>Graphs are useful tools that represent data. They give a shape to the data, which makes finding out further information possible, such as the distribution of the data.</p>"},{"location":"Probability%20and%20Statistics/Statistic%20Introduction/Data%20Graphs/#pie-chart","title":"Pie Chart","text":"<p>The pie chart is a graph for qualitative data. It is a circle where each category is represented as a weighted slice of pie, where the weight is the proportion of the category.</p> <p>[!tip]</p> <p>The pie chart makes it easy to compare one category with the others, but it may be unsuitable for too many categories.</p> <p></p>"},{"location":"Probability%20and%20Statistics/Statistic%20Introduction/Data%20Graphs/#bar-chart","title":"Bar Chart","text":"<p>The bar chart is a graph for qualitative data. It is a canvas with multiple vertical detached bars, each bar represents a category and the bar's height can either be the value or the relative frequency of the category.</p> <p></p>"},{"location":"Probability%20and%20Statistics/Statistic%20Introduction/Data%20Graphs/#dot-plot","title":"Dot Plot","text":"<p>The dot plot is a graph for quantitative discrete data. It is composed of a horizontal ordered axis which represents the possible modalities of an observation, for each observation a dot is placed above the axis on the respective value.</p> <p></p>"},{"location":"Probability%20and%20Statistics/Statistic%20Introduction/Data%20Graphs/#histogram","title":"Histogram","text":"<p>The histogram is a graph for quantitative continuous data. It is similar to a mix of a bar chart and a dot plot, it is composed of bars of equal width with no gap between them, over an horizontal ordered axis representing the modalities of the variable.</p> <p>Each bar represents an interval of values, called class, its height can either be the frequency of the values in the class or the relative frequency of the class.</p> <p></p>"},{"location":"Probability%20and%20Statistics/Statistic%20Introduction/Data%20Graphs/#single-time-series","title":"Single Time Series","text":"<p>The single time series is a graph used to display quantitative data over a span of time. It's a plane where the horizontal axis represents a point in time, the vertical axis represents the quantitative values of a variable.</p> <p>Over time, data is collected in intervals of time, then the observations \\(\\text{(x=time, y=variable)}\\) are mapped on the plane and joined by a segment in order of time. Common patterns over times can be noted, and they're called trends.</p> <p></p>"},{"location":"Probability%20and%20Statistics/Statistic%20Introduction/Sample%20and%20Population/","title":"Sample and Population","text":"<p>When studying a certain characteristic of an entire set, the whole set should be studied, but when the cardinality of the set is huge this is clearly impossible. So, a smaller number of elements in the set are studied, hoping that the results won't vary much from the whole reality.</p> <p>[!note] Population</p> <p>The population is the entire set of possible elements in which a study is interested, e.g. individuals, schools, rats, countries, days, widgets.</p> <p>[!note] Sample</p> <p>A sample is a subset of the population from which information is actually collected.</p> <p>To conduct a study, samples are observed and the result is inferred to the whole population, with a certain margin of error. The margin of error is proportional to the difference of cardinalities between the population and the sample: the smaller the difference, the smaller the margin of error.</p> <p>[!example] Predicting an Election</p> <p>9.5 million people voted an election, of which 3889 have been interviewed. The goal of the study is to predict the results of the election.</p> <p>The population is the 9.5 million people that voted, the sample is the 3889 people that have been interviewed.</p>"},{"location":"Probability%20and%20Statistics/Statistic%20Introduction/Sample%20and%20Population/#parameters-and-statistics","title":"Parameters and Statistics","text":"<p>The concept of parameters and statistics is derived from the sample and population.</p> <p>[!note] Parameter</p> <p>A parameter is a numerical summery of the population, e.g. people that voted a certain person in an election.</p> <p>[!note] Statistic</p> <p>A statistic is a numerical summary of a sample, e.g. randomly selected people that voted a certain person in an election out of all the people that voted.</p>"},{"location":"Probability%20and%20Statistics/Statistic%20Introduction/Sample%20and%20Population/#descriptive-and-inferential-statistics","title":"Descriptive and Inferential Statistics","text":"<p>A statistic either defines the characteristics a sample or infers the characteristics of a population.</p> <p>[!note] Descriptive Statistics</p> <p>Techniques of describing data in ways to capture the essence of the information. Summaries consist, for example, of graphs and numbers such as averages and percentages.</p> <p>[!note] Inferential Statistics</p> <p>Conclusions drawn from data taken from a sample about the population.</p> <p>[!example] Descriptive Statistics In the previous study, \\(24\\%\\) of the people voted candidate A.</p> <p>[!example] Inferential Statistics From the previous study, we are \\(95\\%\\) confident that the percentage of the people who voted for candidate A falls between \\(22.7\\%\\) and \\(25.3\\%\\).</p>"},{"location":"Probability%20and%20Statistics/Statistic%20Introduction/Variables/","title":"Variables","text":"<p>Variables are characteristics that are observed on the subjects of a study.</p> <p>[!example] Variables</p> <p>Examples of variables are height and names in people, mileage in cars.</p> <p>[!tip] Modalities</p> <p>The set of all possible values a variable could take is called modalities (modalities of a variables).</p> <p>Variables are divided in two types: qualitative and quantitative.</p>"},{"location":"Probability%20and%20Statistics/Statistic%20Introduction/Variables/#qualitative-variables","title":"Qualitative Variables","text":"<p>Qualitative (or categorical) variables take all non-(pure)-numerical values, e.g. names, yes and no, school year grade.</p> <p>Qualitative variables are sub-categorised in:</p> <ul> <li>Binary, when there are only two values;</li> <li>Ordinal, when there's a natural order (hierarchy) between the values;</li> <li>Nominal, for all other cases.</li> </ul> <p>[!example] Qualitative Variables</p> <ul> <li>Binary: gender (male or female)</li> <li>Ordinal: language certification type (Cambridge levels)</li> <li>Nominal: religion</li> </ul>"},{"location":"Probability%20and%20Statistics/Statistic%20Introduction/Variables/#quantitative-variables","title":"Quantitative Variables","text":"<p>Quantitative variables take numerical values, that represent the different magnitudes the variable can take.</p> <p>Quantitative variables are sub-categorised in:</p> <ul> <li>Discrete, when the set of values isn't dense;</li> <li>Continuous, when the set of values is dense.</li> </ul> <p>[!example] Quantitative Variables</p> <p>Discrete: number of family members Continuous: height</p>"},{"location":"Probability%20and%20Statistics/Statistic%20Introduction/Variables/#relative-frequencies","title":"Relative Frequencies","text":"<p>The proportion and percentage are two measures useful to compare categories of observations.</p> <p>[!note] Proportion</p> <p>The proportion of a category is the frequency (or count) of the observations of the category over the whole number of observations.</p> \\[\\large   \\text{proportion} =   \\frac{\\text{frequency of category}}{\\text{number of observations}} \\] <p>[!note] Percentage</p> <p>The percentage is the proportion multiplied by 100. Symbolically it's the same thing, but the percentage offers a more concrete feeling: how many units of observation out of 100 fall under the specified category.</p>"},{"location":"Statistics/","title":"Statistics","text":"<p>Statistics is the art and science of designing studies, analysing the data produced by these studies and translating the data into knowledge to understand the world around us.</p> <p>[!note] What statistician do</p> <ul> <li>Gather data, e.g. draw a random sample of students. The sample size depends on how accurate you need your inference to be and the margin of error you can tolerate;</li> <li>Summarize data from the sample, e.g. mean or standard deviation;</li> <li>Analyse data, i.e. analyse the date through statistical techniques and make inference through confidence interval and hypothesis testing;</li> <li>Draw conclusions and report the results of their analysis.</li> </ul>"},{"location":"Statistics/#table-of-contents","title":"Table of Contents","text":"<p>Chapter 1</p> <ul> <li>Sample and Population</li> </ul> <p>Chapter 2</p> <ul> <li>Variables</li> <li>Data Graphs</li> <li>Shapes of Distributions</li> <li>Centres of Distributions</li> <li>Z-Score</li> </ul>"},{"location":"Statistics/Correlation/","title":"Correlation","text":"<p>Correlation is a statistical relationship between variables, where a change in one variable is associated with a change in another variable.</p> <p></p>"},{"location":"Statistics/Correlation/#pearsons-correlation-coefficient","title":"Pearson's Correlation Coefficient","text":"<p>Pearson's correlation coefficient is a measure of the correlation between two variables, ranging from -1 to 1:</p> <ul> <li>a coefficient of 1 represents a perfect positive correlation;</li> <li>a coefficient of 0 represents no correlation at all;</li> <li>a coefficient of -1 represents a perfect negative correlation.</li> </ul> <p>Pearson's correlation coefficient for two variables \\(X,Y\\) can be computed using the following formula,</p> \\[\\large     \\rho_{X,Y} = \\frac{ \\text{cov}(X,Y) }{ \\sigma_X \\sigma_Y }     = \\frac{\\mathbb{E}[(X-\\mu_X)(Y-\\mu_Y)]}{\\sigma_X \\sigma_Y} \\] <p>where: - \\(\\text{cov}(X,Y)\\) is the covariance of the two variables; - \\(\\sigma_X\\) is the standard deviation of the variable \\(X\\); - \\(\\sigma_Y\\) is the standard deviation of the variable \\(Y\\).</p> <p>The formula could be rewritten in the following way</p> <p>[!note] Correlation Formula</p> <p>The correlation formula could be rewritten in the following way.</p> \\[\\large   r = \\frac   { \\sum (x_i - \\overline x) (y_i - \\overline y) }   { \\sqrt {   \\sum (x_i - \\overline x)^2 \\sum (x_i - \\overline x)^2   } } \\]"},{"location":"Statistics/Z-Score/","title":"Z-Score","text":"<p>The Z-Value, Z-Score or just Z represents the number of standard deviations an observation is from the mean.</p> <p>The </p> <p>The z-score for a particular observation is calculated as</p>"},{"location":"Statistics/Distributions/Centres%20of%20Distributions/","title":"Centre of Distributions","text":"<p>The centre of a distribution can be defined in multiple ways, depending on the needs and on the kind of information we're looking for.</p>"},{"location":"Statistics/Distributions/Centres%20of%20Distributions/#mean","title":"Mean","text":"<p>The arithmetic mean is the sum the observations normalized by the number of observations.</p> \\[\\large     \\mu = \\frac{1}{n} \\sum_{i=1}^n x_i     = \\frac{x_1 + x_2 + \\cdots + x_n}{n} \\] <p>Example of Mean </p> <p>[!warning] Extreme Values</p> <p>The mean is affected by extreme values: changing one of the first/last values into a much lower or higher value will greatly affect the mean.</p>"},{"location":"Statistics/Distributions/Centres%20of%20Distributions/#properties-of-the-mean","title":"Properties of the Mean","text":"<ol> <li>The mean value is internal, i.e. \\(x_1 \\le \\mu \\le x_n\\)</li> <li>The mean value is the fair value in the distribution, i.e. \\(\\sum_{i=1}^n x_i = \\sum_{i=1}^n \\mu\\)</li> <li>The sum of all deviations is zero, i.e. \\(\\sum_{i=1}^n (x_i - \\mu) = 0\\)</li> <li>The mean function is a linear function</li> </ol>"},{"location":"Statistics/Distributions/Centres%20of%20Distributions/#median","title":"Median","text":"<p>The median is the value in the middle of the ordered set of all observations, i.e. the \\(j\\)-th term in the ordered sequence \\((x_i)_{i=1}^n\\), where \\(j = \\frac{n+1}{2}\\).</p> <p>[!tip] Middle Values</p> <p>If the cardinality of the sample is odd, then the median is the middle value; but if the cardinality is even, then the median is the average (mean) of the two middle values, which may or may not be an observed value.</p> <p>Example of Median</p> <p>[!warning] Extreme Values</p> <p>The median isn't affected by extreme values: changing one of the first/last values into something much lower/higher will not affect the mean, as the mean is only affected by the middle value(s).</p>"},{"location":"Statistics/Distributions/Centres%20of%20Distributions/#mode","title":"Mode","text":"<p>The mode of a distribution is the value that occurs the most often.</p> <p>The mode can be used with both quantitative and qualitative data, but it is the most useful with qualitative data, because mean and median can't be applied to qualitative data.</p> <p>A single mode isn't guaranteed to exist, there could exist more than one mode (more than one group of categories with the same frequency) or no mode at all (groups of same frequency are evened out).</p>"},{"location":"Statistics/Distributions/Properties%20of%20Distributions/","title":"Properties of Distributions","text":"<p>TK variability (or spread)</p>"},{"location":"Statistics/Distributions/Properties%20of%20Distributions/#range","title":"Range","text":"<p>The range of a distribution is the span of the interval in which the distribution takes value, it can be computed as the difference between the largest and the smallest values in the dataset.</p> \\[\\large     \\text{range}(X) = \\max(X) - \\min(X) \\] <p>The range is an easy to compute value, but it is greatly affected by extreme values.</p>"},{"location":"Statistics/Distributions/Properties%20of%20Distributions/#variance","title":"Variance","text":""},{"location":"Statistics/Distributions/Properties%20of%20Distributions/#standard-deviation","title":"Standard Deviation","text":"<p>The standard deviation is a measure that tells how much the values in a distribution are close to the mean.</p> <ul> <li>A low standard deviation indicates that the values are close to the mean;</li> <li>A high standard deviation indicates that the values are more spread out. </li> </ul> <p>The standard deviation, expressed by \\(\\text{Std}(X)\\) or simply \\(\\sigma\\), is defined as follows.</p> \\[\\large     \\sigma = \\sqrt{         \\frac{1}{n}         \\sum\\limits_{i=1}^n (x_i - \\mu)^2     }     = \\sqrt{\\text{Var}(X)} \\] <ul> <li>\\(n\\) is the cardinality of the dataset</li> <li>\\(x_i\\) is the \\(i\\)-th value in the dataset</li> <li>\\(\\mu\\) is the mean of the dataset</li> </ul> <p>[!note] Bias</p> <p>TK</p> \\[\\large   \\sigma = \\sqrt{       \\frac{1}{n-1} \\sum_{i=1}^{n}( x_i - \\mu )^2   } \\]"},{"location":"Statistics/Distributions/Shapes%20of%20Distributions/","title":"Shapes of Distributions","text":"<p>Given a statistical study's graph, graphical properties can be noted.</p>"},{"location":"Statistics/Distributions/Shapes%20of%20Distributions/#modality-of-a-distribution","title":"Modality of a Distribution","text":"<p>The modality of a distribution of data is the general shape that the graph assumes when using graphs such as dot plots and histograms.</p> <p>A graph can</p> <ul> <li>Be unimodal: the data is distributed such that there's a single mound. The highest point is at the mode;</li> <li>Be bimodal: the data is distributed such that there are two mounds;</li> <li>Have no modality:  there are no distinct mounds.</li> </ul> <p>[!note] Mound</p> <p>A mound, or peak (TK confirm), is a collection of classes where the frequencies collectively increase and then decrease.</p> <p>Images of some distribution modalities</p>"},{"location":"Statistics/Distributions/Shapes%20of%20Distributions/#symmetry-and-skewness","title":"Symmetry and Skewness","text":"<p>The shape of a distribution can either be symmetric or skewed to one side.</p> <p>[!note] Symmetric Distribution</p> <p>A distribution is said to be symmetric if the distribution under the central value (that is, the distribution on the left side of the middle value) is a mirrored image of the distribution above the central value.</p> <p>[!note] Skewed Distribution</p> <p>A distribution is said to be skewed if the distribution isn't symmetric. It is said to be skewed on one side if the frequencies on that side decrease faster w.r.t. the other side.</p> <p>Images of distribution symmetries</p>"},{"location":"Systems%20and%20Networking/Systems%20and%20Networking/","title":"Systems and Networking","text":""},{"location":"Systems%20and%20Networking/Systems%20and%20Networking/#creating-a-process","title":"Creating a Process","text":"<p>A process may create other processes through System Calls, the creator process is called parent while the created process is called child. Each new process is given a unique id (PID) and stores the parent process' id (PPID).</p> <p>Regarding resources, there are two possibilities: - The child is a perfect copy of the parent, sharing the same text and data segments but in a different address space. This is the behaviour of the <code>fork</code> system call. - The child is a brand new process, with a different program loaded into the address space. This can be done, after a fork, by using the <code>exec</code> system call.</p> <p>The parent process, after creating the child, has two options: - Wait for the child process to terminate and then continue executing. A process may wait for another process with the <code>wait</code> system call. - Run concurrently with the child without being blocked.</p>"},{"location":"Systems%20and%20Networking/Systems%20and%20Networking/#systems-and-networking","title":"Systems and Networking","text":"<p>Systems and Networking is a course that studies the hardware of machines and the interconnection of those machines.</p> <p>The course is divided in two courses: - Unit 1, centred around the hardware and software of the single machine; - Unit 2, centred around networks of multiple machines.</p> <p>Unit 1 is focused more and the hardware/software of the single machine, while unit 2 is centred around networks and connectivity.</p>"},{"location":"Systems%20and%20Networking/Systems%20and%20Networking/#table-of-contents","title":"Table of Contents","text":""},{"location":"Systems%20and%20Networking/Systems%20and%20Networking/#unit-1","title":"Unit 1","text":"<ul> <li>High-Level Computer Architecture</li> <li>Operating System</li> </ul>"},{"location":"Systems%20and%20Networking/Systems%20and%20Networking/#unit-2","title":"Unit 2","text":"<ul> <li>Networks</li> <li>Network Core</li> <li>Transport Means</li> <li>Application Architecture</li> <li>Security</li> </ul> <p>Internet Protocol</p> <ul> <li>Packet</li> <li>ISO-OSI Model</li> <li>Application Layer</li> <li>Transport Layer</li> <li>Network Layer</li> </ul>"},{"location":"Systems%20and%20Networking/Unit%201/High-Level%20Computer%20Architecture/","title":"High-Level Computer Architecture","text":""},{"location":"Systems%20and%20Networking/Unit%201/High-Level%20Computer%20Architecture/#architecture","title":"Architecture","text":"<p>Today's computers are based on Von Neumann's Architecture.</p> <p>![[assets/Diagram - Von Neumann Architecture.jpg]]</p>"},{"location":"Systems%20and%20Networking/Unit%201/High-Level%20Computer%20Architecture/#cache","title":"Cache","text":"<p>TK</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/CPU/","title":"Central Processing Unit","text":"<p>The Central Processing Unit is the component that executes actual instructions and computations. In modern architectures, there can be multiple processors that work parallelly in order to increase computing power.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/CPU/#components","title":"Components","text":"<p>A CPU is composed of many parts, here's a list of the main macro-components: - Control Unit - Arithmetic Logical Unit - Address Generation Unit - Registers - Cache</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/CPU/#control-unit","title":"Control Unit","text":"<p>The Control Unit is the unit that manages the entire CPU components and behaviours. It tells the other components (e.g. ALU) what to do and how to respond to instructions.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/CPU/#arithmetic-logical-unit","title":"Arithmetic Logical Unit","text":"<p>Also called Combinatorial Unit, the ALU is the core that executes arithmetic and bitwise logic operations.</p> <p>It takes in input two operands, which can come from registers or literals.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/CPU/#address-generation-unit","title":"Address Generation Unit","text":"<p>TK</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/CPU/#instruction-cycle","title":"Instruction Cycle","text":"<p>To execute an instruction, the CPU performs at least the following 3 steps. 1. Fetch: retrieve an instruction from the central memory, whose address is contained in the program counter; 2. Decode: interpret the fetched instruction; 3. Execute: execute the decoded instruction.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Cache/","title":"Cache","text":""},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Cache/#cache","title":"Cache","text":"<p>TODO</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/IO%20Devices/","title":"I/O Devices","text":"<p>Input/Output Devices are devices that let the user interact with the machine.</p> <p>We can find multiple categories of I/O Devices: - Storage, e.g. USB sticks - Communications, e.g. headsets - User Interface, e.g. monitors - etc.</p> <p>Each I/O Device is composed of two parts: the physical device and the device controller.</p> <p>TODO: OS Software Abstraction diagram</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/IO%20Devices/#physical-device","title":"Physical Device","text":"<p>A physical device is the physical circuit/component of an I/O device, it directly interacts with the user.</p> <p>A physical device does not necessarily need to communicate with a machine, because the device controller acts as a medium.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/IO%20Devices/#device-controller","title":"Device Controller","text":"<p>A device controller is a circuit (chip or set of chips) that controls one or more physical devices.</p> <p>The behaviour of the device controller is independent of the OS, instead it's the OS that adapts to the controller.</p> <p>Like the CPU, every controller has some registers used to configure/interact with the device, which can be used by the CPU to communicate with it.</p> <ul> <li>Status Registers: provide information to the CPU about the status of the device (e.g. ready, busy, error);</li> <li>Configuration/Control Registers: can be used by the CPU to configure and control the device;</li> <li>Data Registers: can be used to share (read/write) data between the CPU and the device.</li> </ul>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/IO%20Devices/#device-driver","title":"Device Driver","text":"<p>I/O Devices and Operating Systems might speak different languages, so there has to be an intermedium that translates.</p> <p>This medium is the device driver, a light middleware (software) that translates communications between devices and OS's.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/IO%20Devices/#communicating-with-devices","title":"Communicating with Devices","text":"<p>TODO: port vs memory mapped IO</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/IO%20Devices/#io-tasks","title":"I/O Tasks","text":"<p>// TODO</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Instructions/","title":"Instructions","text":"<p>The collection of all the instructions the CPU can execute is called Instruction Set.</p> <p>The CPU's architecture can be abstracted using only the instruction set and instruction cycle: the physical processor is then created by implementing the abstract architecture (e.g. Intel/AMD processors from x86, phones/Raspberry Pi processors from ARM).</p> <p>An Instruction is composed of two parts: - An operator (opcode) - Zero or mode operands (representing either registers, memory addresses or literals)</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Instructions/#protected-instructions","title":"Protected Instructions","text":"<p>TK</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Memory/","title":"Memory","text":"<p>Memory is a kind of storage, whether digital or analogic. There are different kinds of memory, each with its own properties.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Memory/#properties","title":"Properties","text":""},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Memory/#volatile-non-volatile","title":"Volatile / Non-volatile","text":"<p>A volatile memory loses all of its contents when powered off, it isn't persistent between different machine boots.</p> <p>A non-volatile memory, on the other hand, keeps its contents when powered off, so it persists data across different machine boots.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Memory/#access-time","title":"Access time","text":"<p>Different storage types have different access times. These are dependent on the internal structure of the storage.</p> <p>Usually, it is inversely proportional to the storage capacity.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Memory/#storage-type","title":"Storage type","text":"<p>Based on its use, we can define three types of storages.</p> <ol> <li>Primary: used directly by the CPU</li> <li>Secondary: used as mass storage</li> <li>Tertiary: nowadays deprecated</li> </ol>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Memory/#hierarchy","title":"Hierarchy","text":"<p>A hierarchy of some types of memories, ranked by increasing access time and decreasing cost.</p> Memory Type Volatile Access time Capacity Register Primary yes 1 cc 32 or 64 bit Cache Primary yes 2 to 10 cc 100 KB to 10 MB Main Memory Primary yes 50 to 100 cc 4 to 128 GB Solid-State Drive Secondary no ? 128 GB to 2 TB Hard-Disk Drive Secondary no ~40M cc 256 GB to 10 TB Optical Disk Tertiary no ? ? Magnetic Tapes Tertiary no ? ? <p>[!info] cc stands for Clock Cycle.</p> <p>[!warning] Note Note: some cells are marked as unknown because the values between two different physical supports aren't equal. But the hierarchy still applies, so an average value should be found between the values of the previous and next rows.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Memory/#representation","title":"Representation","text":"<p>A memory can be seen a a sequence of cells, each cell stores a byte.</p> <p>Every cell has an address, which CPUs and I/O devices can use to access (read/write) the memory.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Registers/","title":"Registers","text":"<p>Registers are small storages located in the CPU, usually the size of a word, used to store values the processor will use in a short time.</p> <p>They can be of two types: - General purpose (e.g. eax, ebx, ecx for the x86 arch.) - Special purpose</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Registers/#general-purpose","title":"General Purpose","text":"<p>TK</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Registers/#special-purpose","title":"Special Purpose","text":"<ul> <li>Stack Pointer: contains the address of the latest entry inserted in the stack;</li> <li>Program Counter: contains the address of the next instruction to execute;</li> <li>Return Address: contains the next instruction to execute when exiting a function;</li> <li>Zero: a register whose value will always be zero.</li> </ul>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/System%20Buses/","title":"System Buses","text":"<p>A system bus is a connection that transfers bit from a component to another.</p> <p></p> <p>The primary system buses are: - Data bus, carries actual data - Address bus, carries the addresses to access memory (where to read/write) -  Control bus, carries operations and control data</p> <p>Today, more dedicated buses have been introduces, each with specific functions (e.g. PCI, SATA, USB).</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Timer/","title":"Timer","text":"<p>A timer is an hardware component which enables CPU scheduling.</p> <p>Physically it's just a clock with microseconds accuracy, but at every fixed interval of time (e.g. 100 microseconds) it generates an interrupt that notifies the scheduler to take over and decide which process to execute next.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Virtual%20Memory/","title":"Virtual Memory","text":"<p>Virtual memory is an abstraction of the physical memory. Its reason is to give each process the illusion that the physical memory is a contiguous address space, called Virtual Address Space.</p> <p>It allows programs to be run without them being entirely loaded in the physical memory, instead loading them piece by piece when needed. Note, though, that in the virtual memory the program is considered loaded in its entirety (as a single block) in a single memory space.</p> <p>A virtual memory is implemented by both hardware (MMU) and software (OS).</p> <p>An advantage of virtual memory is the range of the memory: the CPU can address \\(2^{64}\\) addresses, equivalent to \\(2^{64}\\) bytes (16 exbibytes)! Approximately, this is a billion times more than the physical memory capacity.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Virtual%20Memory/#page","title":"Page","text":"<p>Pages are blocks of memory that partition the virtual address space, they have the same size (e.g. 4 KiB).</p> <p>They are either loaded in main memory or stored on disk: - if the page is being used then it's loaded in the main memory, mapped to an address different than the one in the virtual address space; - if the page isn't being used then it's stored on disk, to free up space in the main memory. If needed, the page can be loaded again in the main memory.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Virtual%20Memory/#memory-management-unit","title":"Memory Management Unit","text":"<p>A physical unit that converts virtual addresses to physical ones using a page table, which is managed by the OS. The OS must manage the loading and unloading of the pages.</p> <p>For quick lookups, the MMU uses a cache called Translation Look-aside Buffer, instead of searching the page in the page table.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Virtual%20Memory/#virtual-address-space","title":"Virtual Address Space","text":"<p>The virtual address space is a virtual memory block used by processes. Every process can use a fixed contiguous amount of virtual addresses in the memory, which are translated into real addresses by the MMU.</p> <p>The amount of addresses user programs can use are dependant on the hardware and on the OS. For example, on a 32-bit architecture, the virtual addresses range from \\(0\\) to \\(2^{32}-1\\).</p> <p>The layout is as follows (from top to bottom): 1. Kernel Space 2. Stack (growing towards bottom) 3. Heap (growing towards top) 4. BSS 5. Data 6. Text</p> <p>TK</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Virtual%20Memory/#text","title":"Text","text":"<p>Contains executable instructions.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Virtual%20Memory/#data","title":"Data","text":"<p>Static variables.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Virtual%20Memory/#bss","title":"BSS","text":"<p>Global variables (non-initialized).</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Virtual%20Memory/#heap","title":"HEAP","text":"<p>Used for dynamic allocation.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Architecture/Virtual%20Memory/#stack","title":"Stack","text":"<p>LIFO structure used to store all the data needed by a function call (stack frame)</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Deadlock/","title":"Deadlock","text":"<p>Whenever two or more thread are competing for a fixed number of resources, some threads may block waiting for a resource to be freed. If all threads block while waiting for resources occupied by the other threads, no thread can continue: it's a deadlock.</p> <p>A deadlock can happen if all of the following conditions hold,</p> <ol> <li>Mutual Exclusion: at least one thread must hold a non-shareable resource;</li> <li>Hold and Wait: at least one thread is holding a non-shareable resource and it's waiting for other resource(s) to become available;</li> <li>No Preemption: a thread can only release a resource voluntarily, neither another thread nor the OS can force it to release the resource;</li> <li>Circular Wait: there's a set of threads \\(t_1, \\ldots, t_n\\), where \\(t_i\\) is waiting on resource(s) held by \\(t_{(i+1)\\%5}\\).</li> </ol>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Deadlock/#deadlock-detection","title":"Deadlock Detection","text":"<p>To detect deadlock, a certain type of graph (Resource Allocation Graph) can be used.</p> <p>Let \\(G=(V,E)\\) be a directed graph, where - \\(V\\) is the set containing both the resources \\(\\set{r_1, \\ldots, r_m}\\) and the threads \\(\\set{t_1, \\ldots, t_n}\\); - \\(E\\) is the set of edges connecting threads and resources.</p> <p>Edges can be of two types, - Request Edge: a directed edge \\((t_i, r_j)\\), it indicates that the thread \\(t_i\\) has requested the resource \\(r_j\\), but hasn't acquired it yet; - Assignment Edge: a directed edge \\((r_j, t_i)\\), it indicates that the OS has assigned the resource \\(r_j\\) to thread \\(r_i\\).</p> <p></p> <p>A deadlock might exists if the graph contains cycles.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Deadlock/#deadlock-prevention","title":"Deadlock Prevention","text":"<p>A deadlock might happen only if all the previous conditions hold: a deadlock may be prevented by ensuring that at least one of the conditions doesn't hold anymore.</p> <ol> <li>Mutual Exclusion: make resources shareable;</li> <li>Hold and Wait: a thread may not make sequential requests for resources while holding some, requests must all be made at once;</li> <li>No Preemption: if a thread makes a request for a resource that cannot be satisfied yet, all the resources held by the thread will be temporary released;</li> <li>Circular Wait: impose an ordering on resources and enforce to request them in such an order.</li> </ol>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Deadlock/#deadlock-avoidance","title":"Deadlock Avoidance","text":"<p>TK (&amp; Banker)</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Synchronization/","title":"Synchronization","text":"<p>Even though processes and threads cooperate to reach a common goal, each one of them (usually) executes its task by itself, with no known state of the other threads; executing threads with no logic regulation could bring bugs and problems to any program.</p> <p>[!example] Problem Example Take the case where a thread is trying to count the number of elements in a linked list.</p> <p>While it's in the midst of counting, another thread adds a new element at the start of the list. The total count should have been increased by one, but the thread that counts has no way of knowing that, so the total count will have a missing element.</p> <p>Often, in multi-threading programs, there are some critical sections where it is important that the section isn't disturbed by actions of other threads. To ensure this, we need to synchronize the threads so that they act properly.</p> <p>Any type of synchronization must satisfy three properties,</p> <ul> <li>Mutual Exclusion: only one process/thread at a time may execute the critical section;</li> <li>Liveness: if any process/thread wants to get in the critical section when nobody is executing it, then it must be able to do so;</li> <li>Bounded Waiting: a process/thread requesting access to the critical access will eventually get a turn and there must be a limit to how many others can go first.</li> </ul>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Synchronization/#locks","title":"Locks","text":"<p>Only one lock exists for critical section. As the word implies, whenever a thread gets to a critical section it locks it (acquires the lock), and the current thread is the only one that can execute and unlock (release the lock) the section. If the thread find the section locked, then the thread has to wait for the lock.</p> <p>The lock policy implements two atomic primitives:</p> <ul> <li><code>Lock.acquire()</code> - Wait until the lock is free, then grab it;</li> <li><code>Lock.release()</code> - Unlock and wake up any thread waiting in <code>acquire()</code>.</li> </ul> <p>[!tip] When using a Lock - Always acquire the lock before accessing shared data; - Always release the lock after finishing with shared data;</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Synchronization/#implementing-locks-by-disabling-interrupts","title":"Implementing Locks by disabling interrupts","text":"<p>By disabling/enabling interrupts when acquiring/releasing a lock, the current thread could execute the entire critical section while the other threads wait, assuming a single-core architecture. Beware, that the thread holding the lock cannot go into a waiting state, otherwise interrupts wouldn't be enabled again.</p> <p>On a multi-core, though, the technique becomes unfeasible, because disabling interrupts for all processes and threads would, for sure, make things not work as intended. Moreover, the overhead generated by calling the primitives needed to manage interrupts isn't small.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Synchronization/#implementing-locks-by-atomic-instructions","title":"Implementing Locks by atomic instructions","text":"<p>[!example] The problem Assume two threads \\(t_1, t_2\\) want to acquire the lock, so \\(t_1\\) checks if the lock is free; \\(t_1\\) reads the value, sees that the lock is free and begins acquiring it (it doesn't mark the lock as acquired yet).</p> <p>Meanwhile, there's a context switch, \\(t_2\\) sees the lock as free and acquires it (marks it as acquires). Another context switch follows and \\(t_1\\), which knew that the lock was free, acquires the lock.</p> <p>This situation is a bug, both threads saw the lock as free and acquired it. The reasons this happened is because reading and writing memory are two separated operations.</p> <p>To solve the problem, the only thing needed is an atomic instruction (execution is ensured from start to finish) that lets the thread check that the lock is free and then mark it as acquired. Most architectures have a <code>test&amp;set</code> instruction, which reads the memory location and immediately sets it to <code>1</code>.</p> <p>The <code>test&amp;set</code> instruction can be used to implement locks: - if it reads <code>0</code>, then the lock is free, the thread can acquire it; - if it reads <code>1</code>, then the lock isn't free, the thread can be put to sleep.</p> <p>The value can always be set to <code>1</code>, because: - if it reads <code>0</code>, then the thread is acquiring the lock, so it'd need to set it as acquired; - if it reads <code>1</code>, then there's no problem.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Synchronization/#semaphores","title":"Semaphores","text":"<p>Semaphores are a generalization of synchronization like locks, but permit more than one thread to execute a single critical section.</p> <p>If a semaphore permits just one thread then it's called binary semaphore, otherwise if it permits more than one thread (but less than a fixed number) then it's called counting semaphore.</p> <p>Semaphores can be used for the following synchronization applications: - Ensure mutually exclusion like locks (binary semaphore); - Ensure limited access to certain resources (counting semaphores); - Enforce scheduling constraints so as to execute threads according to some order.</p> <p>TK semaphore methods</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Synchronization/#monitors","title":"Monitors","text":"<p>A monitor is a construct implemented by some programming languages (e.g. Java, C++) that controls access to shared data. The language provides some syntax to implement logic synchronizations, the actual synchronization (primitives/code) is added at compile time by the compiler.</p> <p>Formally, a monitor defines a lock and zero or more condition variables for managing concurrent access to shared data. Using the lock guarantees mutual exclusion at any time.</p> <p>[!info] Synchronized Usually, monitors use the <code>synchronized</code> keyword to mark a code block as a critical section. When entering the critical sections, compilers automatically try to acquire the lock, waiting if necessary, and release it when leaving the section.</p> <p>Condition variables, as the name implies, are variables that let threads wait for a certain condition before resuming execution of a critical section. Condition variables define some methods: - <code>wait()</code>, give up the lock and wait inside the critical section; - <code>notify()</code>, wake up a thread waiting inside the critical section; - <code>notifyAll()</code>, wake up any thread waiting inside the critical section (only one will acquire the lock).</p> <p>[!tip] Semaphores vs Conditional Variables Semaphores and conditional variables may have the same methods, but their meanings differ.</p> Method Semaphore Conditional Variable <code>wait()</code> Wait to enter the critical section Give up the lock and wait for a condition <code>signal/notify()</code> Tell a waiting thread that access is available Tell a waiting thread that something changed <p>[!abstract] Mesa vs Hoare Monitors There are two types of monitors, - Mesa: returning from <code>wait()</code> is only a hint that something changed, the conditional variable must be checked again (in a <code>while</code> loop); - Hoare: returning from <code>wait()</code> guarantees that the conditional variable holds true, no need to check again (it's just an <code>if</code>).</p> <p>Mesa is the most implemented one, while Hoare is the most theorical one.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/","title":"Threads","text":"<p>A \"thread of control\" is the basic unit of CPU utilization, it represents a stream of execution of a sequence of instructions; meaning that every thread will have its own resources needed to execute instructions (i.e. program counter, stack, set of registers), while the associated process sets up everything else (i.e. address space, text, data, resources like files).</p> <p>Implicitly, in single-threaded OS's, every process is a thread. In multi-thread OS, every process can have multiple threads that execute code while sharing the process' resources.</p> <p>Threads can collaborate between themselves in the same process, so no system calls are required to cooperate. Also, context-switch between threads is faster than between processes.</p> Traditional Processes Multi-threaded Processes Only 1 PC, stack, set of registers 1 PC, stack, set of registers per thread Can carry out one sequence of instructions Each thread can execute one sequence of instructions Processes cannot share certain resources (e.g. files) Threads in a process can share certain resources (e.g. files) <p></p> <p>[!tip] Multitasking Multi-thread programming is useful whenever a process has multiple independent tasks to perform, especially whenever some of the tasks are blocking. Blocking a process' thread does not preclude the concurrent execution of the other threads of the process.</p> <pre><code>Example: Word processor\n- Check for spelling and grammar errors\n- Handle user input\n- Save file periodically\n(TK: move somewhere other)\n</code></pre> <p>Each task could be executed independently using different CPUs to reach best efficiency and performance.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/#advantages-of-threads","title":"Advantages of Threads","text":"<ul> <li>Responsiveness: a process' main thread could communicate with different processes while the other threads are blocked or slowed down by intensive computations;</li> <li>Resource Sharing: threads share code, data and address space, inter-thread communication isn't restrictive;</li> <li>Economy: creating, managing and context-switching threads is faster that performing the same for processes;</li> <li>Scalability: on multi-processor architectures, a single threaded process can be run on one CPU only, whereas a multi-threaded process may execute the threads parallelly on any available processor/core.</li> </ul>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/#thread-pools","title":"Thread Pools","text":"<p>Creating threads is less expensive than creating processes, but continuously creating threads in a single process might lead to problems:</p> <ul> <li>Overhead when creating each thread;</li> <li>No limit to the possible number of threads.</li> </ul> <p>Thread pools resolve these problems. When the process starts up, a fixed number of threads is created (usually no more than logic units in a CPU) and are put in a waiting state. Whenever it is needed, a thread is picked up from the pool to execute what it is needed to do, and then put it back in the pool.</p> <p>[!info] Thread Pool Scheduling Thread Pools operate with a FCFS scheduling policy.</p> <p>If there are no threads available when a new task comes in, the new task will be appended to a queue. When a thread frees up, the first task in the queue will be assigned to that thread.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/#types-of-threads","title":"Types of Threads","text":"<p>Support for multi-threading can be provided in three ways: - Kernel Threads at kernel level, where the OS itself manages the threads; - User Threads at user level, where threads are managed in the user space with a thread library, which doesn't require OS intervention; - Lightweight Processes, a hybrid mix of both kernel and user threads.</p> <p>[!info] Lightweight Processes Both threading methods have their pros and cons, so in reality threads are implemented using a mix of both.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/#kernel-threads","title":"Kernel Threads","text":"<p>A kernel thread is actually the smallest unit of execution that can be scheduled by the OS. The OS is responsible for managing all threads, so system calls are provided to create and manage the threads from user space.</p> <p>Kernel threads are schedulable by the CPU, so in addition to the PCB a TCB (Thread Control Block) is associated to each thread.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/#pros","title":"PROs","text":"<p>TK</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/#cons","title":"CONs","text":"<p>TK</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/#user-threads","title":"User Threads","text":"<p>User threads are threads which are managed entirely by a user-level library, which means the OS doesn't know anything about these threads.</p> <p>The OS runs these threads as if they were single-threaded processes, meaning most of the advantages of threads relative to the OS are lost</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/#pros_1","title":"PROs","text":"<p>TK</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/#cons_1","title":"CONs","text":"<p>TK</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/#lightweight-processes","title":"Lightweight Processes","text":"<p>A better method to implement threads is a hybrid of both kernel and user threads: mapping user threads to kernel threads.</p> <p>A number of kernel threads is created by the OS, while user processes create user threads using a thread library; these user threads are then mapped to kernel threads, so that the CPU can actually run the user threads in a (true) multi-threading environment.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/#one-to-one","title":"One to One","text":"<p>Distinct kernel threads handle one user thread each.</p> <p>This model uses all the advantages of threads, but isn't really different from using all thread kernels.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/#many-to-one","title":"Many to One","text":"<p>Many user threads are assigned to a single kernel thread.</p> <p>A process' user threads are assigned to the same single kernel thread, so a single process can't use the real parallelism of multi-threading. Moreover, if a blocking system call is made, the executing user thread blocks the kernel thread, blocking all the other associated user threads.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/#many-to-many","title":"Many to Many","text":"<p>Many user threads are multiplexed between an equal or smaller amount of kernel threads.</p> <p>The many to many model is an advantageous one that enables processes to most (if not all) features of multi-threading processing. User threads from the same process can be split among multiple processors, and blocking system calls do not block the entire process.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/#two-level","title":"Two Level","text":"<p>The two level model is a variant of the many to many model with the restriction that certain kernel threads are mapped to a single user thread (one to one).</p> <p>The only advantage this method provides is an increased flexibility of scheduling policies, so that certain threads have ensured execution time.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Concurrency/Threads/#contention-scope","title":"Contention Scope","text":"<p>Contention Scope how threads will compete with each other for the use of physical CPUs.</p> <p>[!abstract] Process Contention Scope Competition occurs between threads of the same process.</p> <p>It is the case of user threads mapped to a single kernel thread, or threads managed by a thread library (e.g. many-to-one, many-to-many).</p> <p>[!abstract] System Contention Scope Competition occurs between kernel threads and involves the system scheduler.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Address%20Binding/","title":"Address Binding","text":"<p>A program is composed of instructions and data, which are hardcoded into the program. For the program to be executed, it must be translated (converting all the symbolic names of the language into binary code) and loaded into the main memory, so that the CPU can access it.</p> <p>[!info] Compiling and Loading Translation is done by compilers, assemblers and linkers, loading is done by the OS loader.</p> <p>If the CPU has to access a certain variable in a program, it must use the physical address for that variable: the variable must have an associated physical address (address binding).</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Address%20Binding/#compile-time-address-binding","title":"Compile Time Address Binding","text":"<p>Assuming the program knows the exact address where it will reside in the main memory, then the compiler can translate every variable to absolute physical addresses, that the CPU will use to access the variables.</p> <p>The code is called absolute code, because the physical addresses will be equal to the logical addresses (addresses used by the program), as there's no translation from the address the program uses to the final one the CPU uses.</p> <p>This type of address binding poses a lot of problems, e.g. if the program has to be loaded in a different memory location than the code has to be compiled again to relocate the addresses.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Address%20Binding/#load-time-address-binding","title":"Load Time Address Binding","text":"<p>Just like compile time address binding, the compiler assumes that the program is gonna be in a certain memory location, so all the addresses will be relative to that specific memory location. When the program is loaded into memory, the loader adds the offset from the actual memory location to the addresses, so that the calculated addresses are the real physical ones.</p> <p>The code is called statically relocatable code, because the program can be run in any memory location, but it has to be restarted each time it needs to be moved. The logical addresses are still considered equal to the physical addresses, because a simple offset matches them up.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Address%20Binding/#execution-time-address-binding","title":"Execution Time Address Binding","text":"<p>With execution time address binding, the logical addresses used by the program are translated in runtime in physical addresses. This requires special a hardware support called MMU.</p> <p>The logical addresses (also called virtual addresses) aren't equal to the physical addresses anymore, because each virtual address could be any physical address.</p> <p>[!note] Dynamically Relocatable Code The code is called Dynamically Relocatable Code, because the program can be moved around in the main memory during execution, without any problem.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Memory%20Allocation/","title":"Memory Allocation","text":"<p>There are different ways to allocate memory to processes, from simpler ones to the more complex ones.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Memory%20Allocation/#contiguous-memory-allocation","title":"Contiguous Memory Allocation","text":"<p>One of the most reasonable ways to allocate memory, is to keep track of the unused memory spaces (called holes) and allocate holes to processes that need memory. Contiguous means that the process memory space isn't broken into smaller pieces, but the process is assigned a single continuous block of memory.</p> <p>[!question] Allocation Algorithms Between the following algorithms, simulations show that the ranking of efficiencies is as follows (from most efficient to least): 1. First-Fit 2. Best-Fit 3. Worst-Fit</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Memory%20Allocation/#first-fit-memory-allocation","title":"First-Fit Memory Allocation","text":"<p>Scan the list of holes until one big enough for the process is found, then allocate the hole to the process. Subsequent requests for memory could start scanning from the start or from the position of the last hole.</p> <p>[!tip] Complexity First-Fit allocation has a time complexity of \\(O(n)\\). \\(n\\) is the number of holes.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Memory%20Allocation/#best-fit-memory-allocation","title":"Best-Fit Memory Allocation","text":"<p>Allocate the smallest hole that is big enough to satisfy the request, this way larger holes could be used for processes that may require more memory.</p> <p>[!tip] Complexity Best-Fit allocation has a time complexity of \\(O(n)\\) normally, but could be \\(O(\\log n)\\) if the list of holes is kept sorted. \\(n\\) is the number of holes.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Memory%20Allocation/#worst-fit-memory-allocation","title":"Worst-Fit Memory Allocation","text":"<p>Allocate the largest hole available, which may seem counterintuitive but increase the likelihood that the remaining portion from the allocation could be used in future requests.</p> <p>[!tip] Complexity Worst-Fit allocation, like Best-Fit, has a time complexity of \\(O(n)\\) normally, but could be \\(O(\\log n)\\) if the list of holes is kept sorted. \\(n\\) is the number of holes.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Memory%20Allocation/#fragmentation","title":"Fragmentation","text":"<p>Memory allocation suffers a big problem: fragmentation. When allocating a memory segment, a smaller segment might remain that can't be useful to other processes because it's too small.</p> <p>There are two types of fragmentation, - External Fragmentation: when there's enough space to load a process in memory, but the space isn't contiguous (it's composed of smaller sparse holes); - Internal Fragmentation: when it's more efficient to allocate a whole hole to a process rather than keeping track of a tinier hole (e.g. some bytes).</p> <p>[!cite] Wasted Memory Simulations show that for every \\(2N\\) allocated blocks, \\(N\\) are lost due to external fragmentation, i.e. one third of memory space is wasted on average because of fragmentation.</p> <p>Fragmentation could be solved by either - Full Compaction: moving all processes \"up\", removing any hole between allocated memory spaces and making one big hole; - Partial Compaction: moving as less processes as possible, so to make a hole big enough for the new process to fit; - Swapping: processes currently not in execution (i.e. in waiting state) could be swapped out of the memory and saved in the disk to free space, then when they go back to a ready state they are swapped in the memory.</p> <p>Both fragmentation and swapping are good management techniques that unfortunately require much overhead, so if the possibility arises it'd be better to turn to more efficient techniques (e.g. paging).</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Multiprogramming%20Memory/","title":"Multiprogramming Memory","text":"<p>The term Multiprogramming Memory means a memory that can host multiple programs that are executing. Some rules must regulate the memory, otherwise programs could corrupt themselves or even the OS.</p> <p>Sharing Goals: - Several processes can coexist in the main memory at the same time; - Cooperating processes can share parts of the address space;</p> <p>Transparency Goals: - Processes shouldn't be aware that the memory is shared; - Processes shouldn't know which part of the physical memory is them assigned.</p> <p>Security Goals: - Processes must not be able to corrupt each other or the OS; - Processes must not be able to read data of other processes.</p> <p>Efficiency Goals: - CPU and memory performance should not degrade too much due to sharing: - Keep memory fragmentation low.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Multiprogramming%20Memory/#static-relocation","title":"Static Relocation","text":"<p>Static relocation uses Load Time Address Binding. The OS is loaded at the highest memory address, while other processes are loaded contiguously below the OS (<code>memory_size</code> - <code>os_size</code> - 1).</p> <p>[!warning] Static Relocation Problems With static relocation no HW support is needed, but there are many problems: - Processes could corrupt other processes or the OS; - Processes are to be allocated contiguously, which implies assuming worst case stack and heap; - Processes can't be moved around in the memory once in has been allocated.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Multiprogramming%20Memory/#dynamic-relocation","title":"Dynamic Relocation","text":"<p>Dynamic Relocation uses Execution Time Address Binding. On each memory access request, the MMU can check if the physical address associated to the virtual address belongs to the requesting process and block the request if needed.</p> <p></p> <p>Dynamic Relocation offers many advantages that Static Relocation doesn't: - Read/Write protection across processes' address spaces; - Possibility to move processes during execution; - Possibility to let processes' address space grow over time; - Simple and fast hardware implementation (MMU), no software overhead;</p> <p>but still has problems on its own: - Contiguous allocation in physical memory (possible memory waste); - No partial sharing of address space yet; - The entirety of the program must be allocated in memory.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Paging/","title":"Paging","text":"<p>Paging is a memory management technique that provides the illusion of a contiguous logical address space, divided into fixed-size block called pages. These pages can then be mapped to non-contiguous physical frames (physical blocks in the memory).</p> <p>[!tip] Fragmentation With paging, external fragmentation is eliminated because pages have a fixed size, even though internal fragmentation may still occur (the whole page has to be allocated).</p> <p>Paging is a combination of efforts from both the OS and the HW: - the OS must map logical pages to physical frames; - the HW must translate virtual addresses to physical ones.</p> <p>The OS can map pages to frames using a Page Tabe, which is a table with just two columns: the page number and the associated frame number.</p> <p></p> <p>Processes still use virtual/logical addresses to refer to memory, the paging construct is invisible to processes. The virtual address space created by the paging construct is a contiguous address space that stars from 0.</p> <p>[!note] Virtual to Physical Address Addresses are divided into two parts: - The LSBs are the offset, used to identify cells inside pages and frames - The remaining MSBs are the page/frame identifier</p> <p>The page's and frame's offset bust be the same size, because pages and frames have the same size. Identifiers, though, don't need to be the same size, because the only requirement is for pages to have a corresponding frame.</p> <p>[!tip] Page/Frame Size The usual page/frame size is a power of 2, usually between 512B and 8192B (i.e. 8KiB). This is because there's no need for <code>div</code> and <code>mod</code> operations is powers of 2, the results are MSBs and LSBs, respectively.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Paging/#translation-look-aside-buffer","title":"Translation Look-aside Buffer","text":"<p>The Page Table must still be stored somewhere, caches are too small to fit it and the main memory is too slow to permit fast access. To solve this problem, Translation Look-aside Buffers are born.</p> <p>The TLB is (generally) a L1 cache that stores a part of the page/frame mappings for fast access, while the full page table resides in memory (Page Table Base Register).</p> <p>When dealing with multiple processes, the TLB needs to reflect the mappings of the current process. This can be done in two ways: - The TLB is reset at every context-switch, possibly losing time later (the first accesses after a context-switch are all misses): - The content of the TLB is saved on each context-switch and then loaded back.</p> <p>[!tip] Memory Access Time With no TLB, access to memory would take \\(T_\\text{MA} = 2 \\cdot t_\\text{MA}\\) (one access to translate the virtual address, one access to retrieve data).</p> <p>With TLB, access to memory would take  $$\\large   T_\\text{MA} = p ( t_\\text{MA} + t_\\text{TLB} )   + (1-p) ( 2 \\cdot t_\\text{MA} + t_\\text{TLB} ) $$</p> <p>where: - \\(T_\\text{MA}\\) is the total time to access the corresponding physical address from a virtual address; - \\(t_\\text{MA}\\) is the physical memory access time; - \\(t_\\text{TLB}\\) is the lookup time on TLB; - \\(p\\) is the probability of a TLB cache hit (or hit ratio).</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Paging/#memory-handling-tk","title":"Memory Handling TK","text":"<p>Whenever a process starts:</p> <ol> <li>The process asks for \\(N\\) Bytes (equivalent to \\(k\\) pages);</li> <li>If \\(k\\) frames are free then those are allocated to the process, otherwise frames no longer needed are swapped-out;</li> <li>Each page is associated to one of the frames, the PTBR is updated accordingly;</li> <li>The TLB is flushed and ready to use for the process;</li> <li>As the process runs, the OS loads the missed lookups into the TLB, possibly replacing existing entries if full.</li> </ol> <p>Each process will have a different TLB and page table, so the address of the PTBR and a copy of the TLB will be saved in the PCB.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Paging/#page-sharing","title":"Page Sharing","text":"<p>If two or more processes share the same code, the pages with the code could be shared among the processes, to save up space in memory. This can be done by pointing the pages (supposedly) holding the code to the same frames.</p> <p>[!warning] Reentrant Code Pages could be shared only if the code is reentrant: - The code doesn't change nor it overwrites itself; - Each process running the code has its own set of registers and data.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Segmentation/","title":"Segmentation","text":"<p>Segmentation is a memory management technique applied by some (if not most) programming languages (e.g. C). With segmentation, the code is divided into few memory segments, each with a specific logic meaning and memory location (memory is supposed contiguous and fixed).</p> <p>[!example] C Segmentation A C compiler usually generates 5 segments: code, libraries, static variables, stack, heap.</p> <p>Segmentation is invisible to the program: the logic address is divided into two parts: - A few bits which indicate the segment; - The rest of the address is the offset in the segment.</p> <p>The segment index is translated, using a segment table, into the base address of the segment in the memory; then the offset is added to the base address to get the actual memory address. The segment table, because it has few entries, could be stored in normal hardware registers.</p> <p>[!note] Base-Limit Segmentation implements the base-limit registers security feature: each segment's entry in the segmentation table has a base and a limit addresses.</p> <p>Before accessing the actual memory address, the Control Unit checks if <code>base</code> + <code>offset</code> &gt; <code>limit</code>, if it is then a trap is issued.  </p> <p>[!warning] Fragmentation Normal segmentation supposes that the segments are allocated in a contiguous block of memory, which means that internal and external fragmentations are still a problem.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Memory%20Management/Segmented%20Paging/","title":"Segmented Paging","text":"<p>TK</p> <p>\\(p(t_\\text{FAULT}) + (1-p)t_\\text{MA}\\)</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Operating%20System/","title":"Operating System","text":"<p>There isn't a universal definition for an Operating System, but one could say that it is an implementation of a virtual machine which should be easier to program than bare hardware.</p> <p>An OS stands between users (or application programs) and the underlying hardware, and it is the only mean to access the hardware.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Operating%20System/#design","title":"Design","text":"<p>An OS must be designed with some goals in mind.</p> <p>It could be oriented towards the user (easy to use) or towards the system itself (easy to design and implement).</p> <p>It is crucial to separate policies (what the OS will do) from mechanisms (how the OS will do something): - (flexibility) addition and modification of policies can be easily supported; - (reusability) existing mechanism can be reused to implement new policies; - (stability) adding a new policy shouldn't destabilize the whole system.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Operating%20System/#features","title":"Features","text":"<p>The features of an OS are many: - Resource Manager: manage physical resources (CPU, memory, I/O, etc.) with efficiency; - Virtual Machine: virtualize any physical resource, to give the user and programs the illusion of infinite resources (and for efficiency reasons); - Separate HW/SW: provide a set of API's, to control the access to physical hardware.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Operating%20System/#services","title":"Services","text":"<p>Which services an Operative System can offer is dictated by the underlying structure.</p> OS Service HW Support Protection and Security Kernel/User Mode, Protected Instructions, Base/Limit Registers System Calls Traps, Interrupt Vectors Exception Handling Traps, Interrupt Vectors I/O Operations Traps, Interrupt Vectors, Memory Mapping Scheduling Timer Synchronization Atomic Instructions Virtual Memory Translation Look-Aside Buffer (TLB)"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Operating%20System/#structure","title":"Structure","text":"<p>Many OS have different structures, with a mixture of languages and techniques. An OS should be partitioned into separate subsystems, each with carefully defined tasks, inputs, outputs and performance characteristics.</p> Structure PROs CONs Simple Structure easy to implement rigidity, security Traditional Monolithic easy to implement, efficiency rigidity, security Layered easy to implement and debug, modularity, portability communication overhead, duplicated functionalities Microkernel security, reliability, extendibility efficiency (message passing)"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Operating%20System/#simple-structure","title":"Simple Structure","text":"<p>The main example being MS-DOS, it is the most simple OS structure, there is no modular subsystem whatsoever and no separation between kernel and user mode. Every functionality is implemented directly without decoupling of policies and mechanisms.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Operating%20System/#traditional-monolithic-kernel","title":"Traditional Monolithic Kernel","text":"<p>The main example being UNIX systems, it's one huge program run as one process in the same address space.</p> <p>Even though it's one big program, it's still kind of modular: the protected functionalities are run in kernel mode (e.g. I/O, virtual memory, scheduling), while the other programs (e.g. shells, commands) are run in user mode.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Operating%20System/#layered-structure","title":"Layered Structure","text":"<p>The OS is divided in \\(N\\) layers, with the hardware being layer 0. Each layer L uses the functionalities implemented by the layer \\(L-1\\) to expose new functionalities to layer \\(L+1\\).</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Operating%20System/#microkernel-structure","title":"Microkernel Structure","text":"<p>The microkernel opposes to the monolithic kernel because the kernel contains very basic functionalities, usually what's needed to communicate with the hardware (e.g. scheduling, memory management), the rest (e.g. application programs, file system) runs in user mode.</p> <p>In this structure there is a clear distinction between policy (user mode) and mechanism (microkernel) and it is very modular, but this modularity comes at a cost: processes use message passing to communicate, which can slow down processing speed.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Operating%20System/#loadable-kernel-modules","title":"Loadable Kernel Modules","text":"<p>Many modern systems implement this design for its flexibility, which is similar to the layered and microkernel structures: - The main focus is on object-oriented for reusability; - Each functionality is a module that can talk to others through some interface; - Each module can be loaded when needed within the kernel space (memory space of the kernel).</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Process/","title":"Process","text":"<p>A process is a particular instance of a program, it differs from a program because a program is static, saved in the mass memory, while a process is the program that's running in the main memory. Many processes may run the same program, but each process instance will have its own state!</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Process/#process-control-block","title":"Process Control Block","text":"<p>The Process Control Block is the main structure used by the OS to keep track of all the processes.</p> <p>Every process has its own PCB. Every time a new process is created, a corresponding PCB is created and placed into a queue containing all the other PCBs. As soon as the corresponding process is terminated, the PCB is deallocated.</p> <p>The PCB contains the following information: - Execution State - PID (process identifier, a unique number) - Registers' values (e.g. program counter, stack pointer, general purpose) - Scheduling information (priority, state) - Memory management information (page tables) - I/O status (list of open files/devices) - General information (CPU time consumed, owner process)</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Process/#process-execution-state","title":"Process Execution State","text":"<p>Each process has an execution state, dictated by program actions (e.g. system calls), OS actions (e.g. scheduling) and external actions (e.g. interrupts)</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Process/#new","title":"New","text":"<p>In new state, the process has just been successfully set up and it's waiting to be picked up for the first time by the scheduler.</p> <p>From here, the process can go in the following states: - Ready (admitted)</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Process/#ready","title":"Ready","text":"<p>In ready state, the process is ready to be executed by the CPU.</p> <p>From here, the process can go in the following states: - Running (dispatched by scheduler)</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Process/#running","title":"Running","text":"<p>In running state, the process is actually being executed by the CPU.</p> <p>From here, the process can go in the following states: - Ready (interrupted by scheduler) - Waiting (interrupted by I/O or event) - Terminated (exit)</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Process/#waiting","title":"Waiting","text":"<p>In waiting state, the process is suspended while waiting for (usually) I/O events. Once the event is done, the process goes back to ready state to be resume execution.</p> <p>Most of the events are blocking: the process can't do anything until the system call returns. Meanwhile, the OS sets the process to the waiting state and schedules another process right away, so to avoid the CPU being idle.</p> <p>From here, the process can go in the following states: - Ready (I/O or event completion)</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Process/#terminated","title":"Terminated","text":"<p>In terminated state, the process has finished executing and all the resources used by it have to be freed.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Protection%20and%20Security/","title":"Protection and Security","text":"<p>An Operating System must provide mechanisms to control which users and processes have access to which resources, especially nowadays that everything is connected to a network.</p> <p>These mechanisms may be implemented by hardware (e.g. Base-Limit) or software (e.g. System Calls)</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Protection%20and%20Security/#memory-protection","title":"Memory Protection","text":"<p>To protect user programs from accessing the Operating System's and other programs' memory space, some processors offer a simple security feature.</p> <p>These processors have two dedicated registers: - Base, which contains the address where the program's memory block starts; - Limit, which contains the address where the program's memory block ends.</p> <p>At program startup, the OS saves the base and limit values; the processor then, every time the program tries to access the memory, checks if the cells the program is trying to access to fall between the base and limit values.</p> <p>[!info] Note This strategy is applied real-time by the processor, so no software overhead is generated.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Protection%20and%20Security/#kernel-user-mode","title":"Kernel-User Mode","text":"<p>To offer security and stability, the processor has two modalities: Kernel and User.</p> <p>In Kernel mode, the Operating System is unrestricted, and can execute any instruction, including privileged ones.</p> <p>On the other hand, in User mode the CPU doesn't allow the execution of some privileged instructions (TK), to prevent the user from compromising the system. Some of actions the user cannot execute are: - Address I/O directly - Manipulate the content of the main memory - Switch to kernel mode - Halt the machine</p> <p>Programs in user mode can sometimes execute these instructions through System Calls.</p> <p>Kernel and User mode are implemented by the CPU via a hidden register, which uses one bit <code>{0: kernel, 1: user}</code> to distinguish between modalities.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/System%20Calls/","title":"System Calls","text":"<p>User programs cannot run privileged instructions directly, but they can ask the Operating System to run the instructions on their behalf in Kernel mode.</p> <p>A System Call is an interface to a service provided by the OS, typically written in C/C++. Most programs, though, access these services via a high-level API rather a direct system call.</p> <p>Examples of high-level API for system calls are: - GNU C library - Win32 API - Java API</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/System%20Calls/#categories","title":"Categories","text":"<p>There are 6 main categories of system call, distinguished by the type of service they offer: - Process Control - File Management - Device Management - Information Management - Communications - TK</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/System%20Calls/#process-control","title":"Process Control","text":"<p>Process control system calls may be used to execute all those actions needed to control processes, such as create/execute/terminate processes or get/set process attributes.</p> <p>It may also be used to allocate/deallocate memory, because memory is a resource needed by processes.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/System%20Calls/#file-management","title":"File Management","text":"<p>File management system calls may be used to execute all those actions needed to operate on files, such as open/read/write/close files, create/move/delete files or get/set file attributes; file management also includes management of directories.</p> <p>A program does not need to know how the File System works or how it is implemented, because file management system calls provide an abstract way to operate on it.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/System%20Calls/#device-management","title":"Device Management","text":"<p>Device management system calls may be used to execute all those actions needed to operate on devices other than the CPU, such as request/read/write/release a device or get/set device attributes.</p> <p>The devices may be physical (e.g. disk drives) or virtual (e.g. files, partitions). An OS may attach these devices as special files in the File System, so that the File Management system calls may be used too.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/System%20Calls/#information-management","title":"Information Management","text":"<p>Information management system calls may be used to execute actions needed to get/set information about the system, such as time, date, system data and attributes.</p> <p>Information management system calls may also be used to execute debugging: executing a program in such a way to halt execution after every instruction, to trace the operation and logic of the program.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/System%20Calls/#communications","title":"Communications","text":"<p>Communications system calls may be used to communicate with other devices by sending/receiving messages and transfer status information.</p> <p>There are two main methods to communicate: message passing and shared memory.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/System%20Calls/#message-passing","title":"Message Passing","text":"<p>The message passing method works by opening a connection between the processes/hosts and then transmitting messages along that connection.</p> <p>This method must implement system calls to: - Identify the process/host to communicate with; - Establish a connection; - Open and close the connection as needed; - Transmit messages along the connection; - Wait for incoming messages (either by blocking or non-blocking); - Delete the connection.</p> <p>Between the two methods, the message passing method is easier and generally more appropriate for small amounts of data.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/System%20Calls/#shared-memory","title":"Shared Memory","text":"<p>The shared memory method works by writing and reading from the same memory space. This implies some restrictions, such: - Communicating processes and threads must be able to access the same memory space; - There must be a mechanism to restrict simultaneous access to memory locations; - Allocate and deallocate memory as needed.</p> <p>This method is a bit more tricky, but it's more appropriate for large amounts of data.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/System%20Calls/#system-call-handler","title":"System Call Handler","text":"<p>TK</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/System%20Calls/#parameter-passing","title":"Parameter Passing","text":"<p>TK</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/System%20Calls/#common-system-calls","title":"Common System Calls","text":"Name Parameters Return Notes fork none New process' PID execlp Path to executable, arguments TK waitpid PID TK sleep # of seconds TK"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Trap/","title":"Trap","text":"<p>A trap is a signal that prompts the CPU/OS to switch to kernel mode.</p> Trap Reason Synchronous Generator System Call Request OS service Yes Software Exception Report some fault Yes Software Interrupt Notify an event No Hardware"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Trap/#system-call","title":"System Call","text":"<p>A System Call is a trap that is used to request a service from the OS (e.g. read a file).</p> <p>It's synchronous, in the sense that it is dependent on instructions (e.g. <code>ecall</code>), it's generated from user programs.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Trap/#exception","title":"Exception","text":"<p>An exception is a trap that is used to report faults in the execution of a program (e.g. division by zero).</p> <p>It's synchronous, in the sense that it is dependent on instructions (e.g. <code>div TK</code>), it's generated from both OS and user programs.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Operating%20System/Trap/#interrupt","title":"Interrupt","text":"<p>An interrupt is a trap that is used to notify of events external to the program (e.g. network packet arrived),</p> <p>It's asynchronous, in the sense that it is independent of instructions and depends on external factors, in fact it's generated by the hardware when said events happen.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Process%20Creation/","title":"Process Creation","text":"<p>Processes are created from other processes via system calls, which results in a hierarchy of processes. The process that has been created is called child process, while the creator is called parent process.</p> <p>Each new process is given a new PID, but it also stores the parent's PID (PPID).</p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Process%20Creation/#resources-handling","title":"Resources Handling","text":"<p>Regarding the new process' resources, there are two possibilities: copying and spawn.</p> <ul> <li>Copying</li> </ul> <p>The child is a perfect copy of the parent, the text and data segments are copied into a new address space. </p> <p>This is the behaviour of the <code>fork</code> system call.</p> <ul> <li>Spawn</li> </ul> <p>The child is a brand new process, with a different program loaded into the address space.</p> <p>This is the behaviour of the <code>fork</code> and then <code>exec</code> system calls, or Window's <code>spawn</code> system call.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Process%20Creation/#parents-behaviour","title":"Parent's Behaviour","text":"<p>After creating the child process, the parent process can procede in two ways.</p> <p>One way is to wait for the child process (or processes) to exit (with the <code>wait</code> system call) and then proceed; the other is to run concurrently to the child without being blocked.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Process%20Scheduling/","title":"Process Scheduling","text":"<p>A crucial job of the Operating System is to keep the CPU busy at all time to maximize execution efficiency. Furthermore, it must execute processes in such a way to deliver acceptable response times for all programs, particularly interactive ones.</p> <p>The process scheduler must implement these objectives paying attention to any excessive overhead generated by swapping processes in and out of the CPU.</p> <p>[!warning] Overhead Overhead is the time used by doing routine actions, like context-switching. Swapping processes in the CPU too frequently generates too much overhead, which could be used by processes ready for execution.</p> <p>Scheduling decisions take place under one of four conditions:</p> Condition Cause Options A process switches from running to waiting I/O request or <code>wait</code> system call Select a new process A process switches from running to ready Interrupt (e.g. Timed Context Switch) Either select a new process or continue with the current one A process switches from waiting to ready I/O or <code>wait</code> completion Either select a new process or continue with the current one A process is created or terminated <code>fork</code> system call Select a new process"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Process%20Scheduling/#cpu-vs-io","title":"CPU vs IO","text":"<p>Processes can either be - CPU-bound (uses more CPU than I/O operations) - I/O-bound (uses more I/O operations than CPU) - Both</p> <p>Most of the programs are both, they alternate in CPU-bursts and I/O-bursts. A burst is a small program section in which either one of CPU or I/O operations are needed.</p> <p></p> <p>[!tip] Efficient Scheduling An efficient scheduling system must mix CPU-bound and I/O-bound processes in such a way to execute CPU-bound processes when the I/O-bound ones are waiting. </p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Process%20Scheduling/#process-state-queues","title":"Process State Queues","text":"<p>The OS has a queue for each process state, processes' PCB's are saved in the queue of the respective state. When the OS changes the status of a process, the process' PCB is moved from its previous queue to the queue of the current state.</p> <p>[!tip] This method to manage processes is very flexible, because the OS may use different (scheduling) policies to manage the processes in each queue.</p> <p></p> <p>Furthermore, the waiting queue may be subdivided into one queue for each I/O device, so that processes wait in the queue of the device they're waiting for.</p> <p>[!info] State Queues State queues aren't necessarily FIFO queues. On the contrary, most state queues are implemented with other types of queue, to maximize scheduling efficiency.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Process%20Scheduling/#process-scheduler","title":"Process Scheduler","text":"<p>The process scheduler is that part of the OS that decides which processes to execute and when. It is its job, then, to invoke the dispatcher whenever the CPU must change the process to execute.</p> <p>There are three types of scheduler:</p> Type Frequency Description / Advantages Long-Term Scheduler Infrequent Typical of batch/heavily loaded systems, it doesn't generate unnecessary overhead. Short-Term Scheduler Very frequent Performs context switches frequently, decreases response time of processes at the cost of more overhead. Medium-Term Scheduler Frequency is proportional to the load of the system Reduces overhead when there isn't a high load and allows small jobs to execute when there is a high load."},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Process%20Scheduling/#dispatcher","title":"Dispatcher","text":"<p>The dispatcher is the module that gives the process chosen by the process scheduler control of the CPU, by performing the needed actions.</p> <p>Its functionalities include: - Context switching; - Switching to user mode; - Set the PC to the right address after a new program is loaded.</p> <p>[!info] Dispatch Latency The dispatcher is run whenever a context switch is needed, so the time it uses (dispatch latency) must me as short as possibile.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Process%20Scheduling/#context-switch","title":"Context Switch","text":"<p>Context Switch is the procedure used by the CPU to suspend a currently executing process in order to run a ready one.</p> <p>It's not a trivial operation, because when performing a context switch the CPU must: 1. Update the PCB of the current process with the internal state of the process; 2. Load the internal state of the ready process from the PCB to the CPU.</p> <p>[!info] Internal State The internal state of a process includes everything needed by the CPU to run the process, such as special registers (PC, SP, etc.) and ordinary registers.</p> <p>A context switch may occur due to any incoming trap (i.e. system calls, exceptions, HW interrupts). Whenever this happens, the CPU must context-switch to kernel mode to handle the interrupt.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Process%20Scheduling/#timed-context-switch","title":"Timed Context Switch","text":"<p>To avoid CPU-bound processes from hogging the CPU, interrupts may be triggered whenever a time slice has passed using a timer. The time slice is defined as the maximum amount of time between two consecutive context switches.</p> <p>Timed context-swtich is the mean through which time-sharing multi-tasking OS's can implement pseudo-parallelism (apparently run two or more processes at the same time).</p> <p>[!tip] Time Slice A context switch takes around 10 microseconds. With a time slice between 10 and 100 milliseconds (typical of a modern OS), the overhead is relatively small (with a ratio between 0.1% and 0.01%).</p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Process%20Scheduling/#preemptive-scheduling","title":"Preemptive Scheduling","text":"<p>Scheduling (and context switch) can happen following one of two policies: Preemptive Scheduling or Non-preemptive Scheduling.</p> <p>Non-preemptive scheduling lets the process run until it either blocks voluntarily or it finishes; it is the case of non-timed context switch.</p> <p>Preemptive scheduling decides whether and when to stop a running process and put it in the ready state; it is the case of timed context switch</p> <p>[!failure] Issues Preemptive scheduling is imperative for real-time OS's, but it could lead to problems such as: - The kernel is busy executing a critical system call, which is interrupted; - Two processes share data, but one in interrupted while it's updating the data.</p> <p>TK countermeasures</p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Process%20Termination/","title":"Process Termination","text":"<p>A process usually terminates on its own using the <code>exit</code> system call.</p> <p>The <code>exit</code> syscall takes in input an integer, which is the code of termination. The value 0 should indicate a successful completion of the process, while non-zero values indicate that the process has terminated with some kind of problem.</p> <p>A process may also be terminated if the OS cannot provide any more resources needed by the process.</p> <p>A process may also be killed: - by the parent, if no longer needed; - by another process entirely (e.g. the <code>kill</code> command).</p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Process%20Termination/#on-process-termination","title":"On Process Termination","text":"<p>When a process terminates, all the system resources used by it are freed up (e.g. open files are flushed and closed).</p> <p>When a parent process is terminated, it's up to the Operating System to decide what to do with the child processes; the child processes may or may not be allowed to continue. Child processes without a parent are called orphaned processes.</p> <p>[!tip] Orphaned Processes on UNIX On UNIX Operative Systems, orphaned processes are generally inherited by <code>init</code>, which proceeds to kill them.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Scheduling%20Algorithms/","title":"Scheduling Algorithms","text":"<p>Scheduling processes is no easy task, there are multiple variables to keep track of and many criteria to account for.</p> TK Goal Description CPU utilization \ud83d\udcc8 Maximize Ideally, the CPU should be busy 100% of the time. On real systems it should range from 40% (lightly loaded) to 90% (heavily loaded). Throughput \ud83d\udcc8 Maximize The number of processes is completed in a time unit. It is highly volatile (based on the job length). Turnaround time \ud83d\udcc9 Minimize Time required for a process to complete from submission to completion, including waiting time (in the queue). Waiting time \ud83d\udcc9 Minimize Time spent by the processes in the ready queue waiting to run. Response time \ud83d\udcc9 Minimize Usually applicable to interactive processes, it's the time measured from the issuance of a command to the begin of a response to the command. <p>[!warning] Waiting time The waiting time mentioned above is not the waiting state! The scheduler doesn't have control over processes in waiting state, it can only manage the ones in ready and running states.</p> <p>Some of these variables are dependent on other, so they can be computed through some formulas.</p> <ul> <li>\\(T_\\text{arrival}=\\) arrival time (in the scheduling system as a ready process)</li> <li>\\(T_\\text{completion}=\\) completion time (when the process completes its execution)</li> <li>\\(T_\\text{burst}=\\) burst time (time required by a process for CPU execution)</li> <li>\\(T_\\text{turnaround} = T_\\text{completion} - T_\\text{arrival}\\)</li> <li>\\(T_\\text{waiting} = T_\\text{turnaround} - T_\\text{burst}\\)</li> </ul>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Scheduling%20Algorithms/#list-of-scheduling-algorithms","title":"List of Scheduling Algorithms","text":"<ol> <li>First Come First Serve</li> <li>Round Robin</li> <li>Shortest Job First</li> <li>Priority</li> <li>Multilevel Queue</li> <li>Multilevel Feedback Queue</li> <li>Lottery Scheduling</li> </ol>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Scheduling%20Algorithms/#scheduling-policies","title":"Scheduling Policies","text":"<p>The goals mentioned above can't be reached together and there must be some kind of trade-off, based on the scheduling policy chosen.</p> <p>The scheduling policy of an interactive system says to: - Minimize the average response time, to provide output to the user as quickly as possible; - Minimize the maximum response time, to relieve the worst-case scenario of a non-responding program; - Minimize the variance of response time, because users prefer a predictable system rather than an inconsistent one.</p> <p>The scheduling policy of a batch system says to: - Maximize throughput, by minimizing overhead (i.e. context switches) and using system resources (CPU and I/O devices) in the most efficient way; - Minimize waiting time, by giving time slices of the same size to all processes.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Scheduling%20Algorithms/#first-come-first-serve","title":"First Come First Serve","text":"<p>FCFS is a simple scheduling algorithm composed of just one FIFO queue.</p> <p>When the currently running process changes states, the scheduler looks for the first process in the queue. If it is ready it is scheduled for execution (the dispatcher is called for this process), otherwise it is put at the end of the queue (e.g. if it's in waiting state).</p> <p>When the currently running process changes states, two things can happen: - it completes execution, the scheduler won't have to do anything; - it goes into waiting state, the scheduler will place it back at the end of the queue.</p> <p>[!info] Non-preemptive A process may continue running indefinitely until it waits or terminates by itself, which is why FCFS is a non-preemptive scheduling algorithm.</p> <ul> <li>PROs</li> <li> <p>Very simple</p> </li> <li> <p>CONs</p> </li> <li>Waiting time is highly variable (short jobs may sit behind long ones)</li> <li>CPU-bound jobs will force I/O-bound ones to wait (convoy effect)</li> </ul>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Scheduling%20Algorithms/#round-robin","title":"Round Robin","text":"<p>Round Robin is a simple scheduling algorithm that works just like FCFS, but CPU-bursts are assigned with a time slice (or time quantum) limit.</p> <p>If the current job the CPU is executing finishes before the time slice ends, then the job is replaced with another one just like FCFS. If instead the time slice ends first, the job is switched with the next one and it is put in the ready queue again.</p> <p>This scheduling algorithm makes extensive use of timers: every time the time slice ends, the timer generates an interrupt that the dispatcher uses to switch the jobs.</p> <p>[!info] Preemptive Unless the time it takes to complete the job is less than the time slice, processes are interrupted at least one during execution, which is why Round Robin is a preemptive scheduling algorithm.</p> <p>Round Robin is a fair scheduling algorithm, because it shares the CPU equally among the jobs, even though the average waiting time can be longer than other algorithms; once a job has been given a time slice, the dispatcher must give all the other jobs a time slice before giving it back to the initial job.</p> <p>[!warning] Performance The performance of Round Robin is dependant on the length of the time slice. - A large time slice turns RR to FCFS, because jobs will finish on their own and won't be ever interrupted out of the CPU. - A small time slice will result in more context switch, which is more overhead and less effective CPU utilization (low throughput).</p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Scheduling%20Algorithms/#shortest-job-first","title":"Shortest Job First","text":"<p>Shortest Job First is a scheduling algorithm that orders the jobs to be scheduled by the expected amount of work (CPU-burst) until the next I/O operation or completion, in an ascending way.</p> <p>[!info] Non-preemptive SJF is a non-preemptive scheduling algorithm, because it lets jobs block by themselves when needed. There's also a preemptive variant called SRTF: whenever a new job with a shorter CPU-burst than the current job arrives in the ready queue, that job is immediately scheduled, blocking the current one. </p> <p>It is optimal to minimize the average waiting time, but the difficulty is is found in predicting the amount of CPU utilization of a job. It could be simple if programs were linear, but most programs include conditional statements and jumps, which makes it close to impossible to compute.</p> <p>[!warning] Starvation CPU-bound jobs with long CPU utilization could go into starvation (won't get scheduled), because new short jobs will get scheduled before them.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Scheduling%20Algorithms/#priority","title":"Priority","text":"<p>Priority is a scheduling technique in which every job is assigned a priority, jobs with higher priorities get scheduled first. These priorities can be assigned either internally by the OS or externally by the user.</p> <p>Priorities are implemented using integers, but there is no convention for the ordering. Usually, low numbers mean higher priority (with 0 being the highest one).</p> <p>[!info] (Non-)preemptive Priority scheduling can be both preemptive and non-preemptive: either schedule jobs with the same priority preemptively using another scheduling algorithm such as round robin, or wait for the current job to block or terminate by itself.</p> <p>[!tip] Shortest Job First SJF can be seen as a priority scheduling algorithm: priorities correspond to the expected CPU time and lower priorities (shortest times) get scheduled first.</p> <p>[!warning] Starvation Jobs with low priority can go in starvation, i.e. jobs won't get scheduled because new jobs with higher priority get scheduled first. This problem can be solved with aging: priority of jobs is increased proportionally to the time they wait in queue, so that eventually they get scheduled.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Scheduling%20Algorithms/#multilevel-queue","title":"Multilevel Queue","text":"<p>Multilevel Queue is a scheduling algorithm that divides jobs into multiple queues, each queue is distinguished by a job category. Once a job is inserted into a queue in cannot change queue.</p> <p>So, there are two levels of scheduling: a higher level scheduling to select the queue, and a lower level scheduling to select which process will be given the CPU. Each queue can implement whichever scheduling algorithm it's best suit for the jobs.</p> <p>[!abstract] Higher Level Scheduling Usually, the higher level scheduling is one of the following. - Strict Priority: each queue is assigned different priorities, higher priorities get scheduled first. - Round Robin: each queue gets a time slice, which can be of different sizes too.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Scheduling%20Algorithms/#multilevel-feedback-queue","title":"Multilevel Feedback Queue","text":"<p>Multilevel Feedback Queue is a scheduling algorithm similar to MLQ, except the fact that jobs can change queue.</p> <p>This is most appropriate when a process instructions may change from CPU-intensive to I/O-intensive, going from a higher to a lower priority. Aging can also be applied easier, by moving the job from a lower to a higher priority queue.</p> <p>Higher priority (i.e. lower integers) queues have a larger time slice.</p> <ol> <li>By default, jobs start in the highest priority queue;</li> <li>If the job's time slice expires (not enough time), increase the priority;</li> <li>If the job's time slice doesn't expire (too much time), decrease the priority.</li> </ol> <p>MLFQ is the most flexible scheduling algorithm, but it's also the most complex to implement. There are many parameters that can influence it: - Number of queues - Scheduling algorithm for each queue - Method used to move jobs across queues - Method used to choose the initial queue for a job</p> <p>[!info] Fairness MLFQ tries to mimic the optimal behaviour of SJF in terms of waiting time; it explicitly promotes short jobs, but it could be unfair to longer jobs.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Process%20Handling/Scheduling%20Algorithms/#lottery-scheduling","title":"Lottery Scheduling","text":"<p>Every job is given a certain number of lottery tickets. On each time slice, a uniformly random winning ticket is selected, CPU time will be assigned to the owner of the ticket.</p> <p>The best policy would be to give more tickets to short running jobs and less tickets to long running jobs (similar to SJF). To avoid starvations, each job gets at least one ticket, so it can be selected anytime. Adding and deleting jobs affects the other jobs proportionally and not with a fixed rate.</p> <p>[!info] Randomized Scheduler This is the only example of randomized scheduler, rather than deterministic.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Storage%20Management/Disk%20Scheduling/","title":"Disk Scheduling","text":"<p>When using magnetic disks, the factor that most impacts I/O time isn't the time taken to read a single sector, but the delay to get the head from where it is to the correct sector (i.e. seek time).</p> <p>To help reduce the seek time, different algorithms to schedule incoming I/O requests can be applied:</p> <ul> <li>First Come First Served</li> <li>Shortest Seek Time First</li> <li>SCAN</li> <li>C-SCAN</li> <li>LOOK</li> <li>C-LOOK</li> </ul> <p>[!warning] Arm Speed Note that the arm speed isn't linear, but has acceleration and deceleration. The arm has to stop in a particular track if the sector isn't under the head. This means that the arm speed is a factor to consider when comparing disk scheduling algorithms.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Storage%20Management/Disk%20Scheduling/#first-come-first-served","title":"First Come First Served","text":"<p>The First Come First Served (FCFS) disk scheduling algorithm serves requests in the order they arrive.</p> <p>It's easy to implement, but performance may degrade if requests are sparce between them.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Storage%20Management/Disk%20Scheduling/#shortest-seek-time-first","title":"Shortest Seek Time First","text":"<p>The Shortest Seek Time First (SSTF) disk scheduling algorithm greedily serves a set of requests based on the distance of the requests from the head. The algorithm will serve first the request that's the closest to the current position of the head.</p> <p>It can be implemented via a sorted list, and the overhead needed to keep the list sorted can be negligible because it'll take less time than the actual seek time.</p> <p>In a global context, SSTF can increase the distance travelled by the head and could cause starvation (if one request is far from the head and closer requests keep coming).</p> <p>[!example] Global Inefficiency Take, as example, the set of requests {10, 20, 30, 40, 61} and the starting position of the head is 50. SSTF will first get all the requests other than 61, then in the end get to track 61, having travelled 91 tracks.</p> <p>If it were to get to track 61 first and then get to the others, the head would have travelled only 62 tracks.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Storage%20Management/Disk%20Scheduling/#scan","title":"SCAN","text":"<p>The SCAN disk scheduling algorithm continuously moves the head back and forth (0 to 100 and viceversa), visiting all the tracks in the meanwhile. Requests are served while the head is visiting the requested track.</p> <p>Like SSTF, SCAN can implemented via a sorted list, where the head or the tail (depending on the current direction) contains the next request to serve, but unlike SSTF there's no risk of starvation, even though it's unfair to requests with close track but in the opposite direction (e.g. head at 50 going to 0, request for 51 comes in).</p>"},{"location":"Systems%20and%20Networking/Unit%201/Storage%20Management/Disk%20Scheduling/#look","title":"LOOK","text":"<p>The LOOK disk scheduling algorithm works just like SCAN, but instead of going from end to end, the head will turn back once the last request close to the end has been served.</p> <p>[!example] Head is at 20, going to track 0. The closes request to the end is 18: after having served the request on track 18, the head turns back in the opposite direction.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Storage%20Management/Disk%20Scheduling/#c-scan","title":"C-SCAN","text":"<p>The SCAN disk scheduling algorithm works just like SCAN, but instead of going back and forth serving requests, the head gets reset back every time it reaches the end, without serving requests when going back.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Storage%20Management/Disk%20Scheduling/#c-look","title":"C-LOOK","text":"<p>The C-LOOK disk scheduling algorithm works just like LOOK TK</p>"},{"location":"Systems%20and%20Networking/Unit%201/Storage%20Management/Magnetic%20Disk/","title":"Magnetic Disk","text":"<p>The magnetic disk is a mass storage device, used to store large amount of data; the drawback, though, is that it is much slower than the main memory.</p> <p>With magnetic disks, two categories can be distinguished: - Hard Disk, composed of metal disks (higher capacity); - Floppy Disk, composed of flexible disks (lower capacity but portable).</p>"},{"location":"Systems%20and%20Networking/Unit%201/Storage%20Management/Magnetic%20Disk/#structure","title":"Structure","text":"<p>The magnetic disk drives are composed of multiple disks, attached to a spindle that rotates them. An arm assembly controls some arms (two per disk, as each disk has two working surfaces) with a magnetic head that does the reading and writing.</p> <p>Each surface is composed of multiple tracks (concentric rings), the set of all tracks at the same radius from the spindle is called cylinder. The track, then, is further divided into sectors, which is the smallest addressable unit, with a memory size of 512 bytes.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%201/Storage%20Management/Magnetic%20Disk/#capacity","title":"Capacity","text":"<p>The overall capacity of the drive is given by</p> \\[\\large     \\text{Capacity} = B \\cdot S \\cdot T \\cdot H \\] <p>where:</p> <ul> <li>\\(B\\) is the byte capacity of every sector;</li> <li>\\(S\\) is the number of sectors per track;</li> <li>\\(T\\) is the number of tracks per surface;</li> <li>\\(H\\) is the number of heads, i.e. number of surfaces.</li> </ul> <p>[!tip] Sectors Density Old magnetic disks had the same number of sectors in each track, meaning the inner track had a much higher density of sectors compared to the outer ring. </p> <p>Nowadays, magnetic disks have different numbers of sectors per track, so as to keep the density the same.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Storage%20Management/Magnetic%20Disk/#referencing","title":"Referencing","text":"<p>Each block of data in a magnetic disk can be referenced using a triple:</p> <ul> <li>Head</li> <li>Cylinder / Track</li> <li>Sector</li> </ul> <p>[!note] Cylinder Index Cylinders are indexed starting from the outer layer, i.e. the outermost track has index 0.</p>"},{"location":"Systems%20and%20Networking/Unit%201/Storage%20Management/Magnetic%20Disk/#data-transfer","title":"Data Transfer","text":"<p>Data transfer of magnetic disks is composed of three steps:</p> <ul> <li>Positioning Time</li> <li>Rotational Delay</li> <li>Transfer Time</li> </ul> <p>[!note] Positioning Time Also called Seek Time or Random Access Time, it's the time that's needed for the heads to get to a specific track, including the time to stop. It's the step which takes the most amount of time.</p> <p>[!note] Rotational Delay It's the time that's needed for the disk to rotate to get a specific sector under the head. It ranges from 0 to a full revolution, which for a disk with average speed (7200 rpm or 120 rps) takes slightly more than 8 ms.</p> <p>[!note] Transfer Time It's the time taken by the head to actually read the contents of the entire sector.</p> <p>The transfer rate of a magnetic disk is given by the capacity of a sector (512B) over the sum of all the steps needed to read data from one sector (i.e. seek time, rotational delay, transfer time).</p>"},{"location":"Systems%20and%20Networking/Unit%202/Application%20Architecture/","title":"Application Architecture","text":"<p>Developing an application nowadays is significantly easier, considering that layers other than the application layer are already implemented. The only thing a developer should think of are: - Application logic; - Application architecture; - Which transport protocol to use.</p> <p>The application architecture dictates the role of each host and the relation between them.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Application%20Architecture/#client-server-architecture","title":"Client-Server Architecture","text":"<p>The Client-Server is an architecture paradigm where some hosts, namely the servers, provide services to the end users, called clients.</p> <p>Servers and clients each have their characteristics.</p> <p>Server: - Always on - (Usually) owned by a service provider - Provides services to clients - Could collaborate with other servers - Has a well-known address (either IP or DNS)</p> <p>Client: - It's the end user - Requests services from servers - Does not communicate directly with other clients</p> <p>Often, just one server cannot satisfy the request of all clients. To fix this problem, more than one servers are installed in specialized data centres, so that the servers can share requests and load of work.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Application%20Architecture/#peer-to-peer-architecture","title":"Peer-to-Peer Architecture","text":"<p>A.K.A. P2P, its an architecture paradigm with no distinction between clients and servers. Each peer (host) provides functionalities as both client and server, and peers can communicate freely and directly between each other. Peers are not machines owned by service providers, but are instead machines owned by end users.</p> <p>Like a client connects to a known server, a peer must know other peers to communicate with them. There are some ways to discover peers: - Use a centralized system (server) which can list connected peers; - Register known peers by hand; - Scan the network for other peers.</p> <p>P2P is more \"efficient\" than Client-Server in bandwidth usage and workload sharing: peers don't connect to one server only (huge bandwidth usage) that needs to handle all the requests (high computational power needed), but peers themselves handle requests. This means that there is a lower bandwidth usage (peers are sparse) and workload is shared (between peers), at the cost of the initial setup for peers discovery and other drawbacks (mainly security and reliability).</p>"},{"location":"Systems%20and%20Networking/Unit%202/Application%20Architecture/#communication","title":"Communication","text":"<p>An application/program that runs on a machine is just a process, so applications that need to communicate can do so like processes: by exchanging messages.</p> <p>These messages can be exchanged over the network too, thanks to the layered structure of packets.</p> <p>Processes over the network can communicate through sockets: a software interface, a virtual channel used by the machine to transfer and receive packets. In a communication there are two sockets involved, one for each host.</p> <p>A socket is distinguished by two things: - An IP address, which identifies an host; - A port, which identifies a service (or more generally, a \"channel\") on the host.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Network%20Core/","title":"Network Core","text":""},{"location":"Systems%20and%20Networking/Unit%202/Network%20Core/#network-core","title":"Network Core","text":"<p>A network core is a smaller network of connected routers. Multiple network cores are interconnected to provide long-distance connections.</p> <p>Routers in a network core are connected in a partial mesh to provide reliability. To send a packet from a source host to a destination host, routers forward packets to the next router across an optimal path.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Network%20Core/#forwarding","title":"Forwarding","text":"<p>Forwarding is an action that consists in moving arriving packets to other Router. The goal of this action is to move the packet towards its destination.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Network%20Core/#routing","title":"Routing","text":"<p>Routing is an action that consists in finding the optimal path along the connected network cores to send packets in the most efficient way from source to destination.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Network%20Core/#packet-switching","title":"Packet Switching","text":"<p>With packet switching, packets don't have a fixed path to go through. Instead, routers compute an optimal path of routers to transfer a packet from the sending host to the receiving host. In this way, go through one router to another, without having to propagate to all the network.</p> <p>If a packet has length \\(L\\ [bits]\\) and walks through a channel with transmission rate \\(R\\ [\\frac{\\text{bits}}{\\text{bits/s}}]\\), a packet will take \\(\\frac{L}{R}\\) seconds to be fully transmitted.</p> <p>TK Before forwarding packets to the next link, a router must fully receive the packet (store and forward). Internally, routers have a queue for packets. If the arrival rate exceeds the transmission rate, packets are put in this queue. If more packets than the queue can contain arrive, these packets are dropped (lost).</p>"},{"location":"Systems%20and%20Networking/Unit%202/Network%20Core/#circuit-switching","title":"Circuit Switching","text":"<p>In circuit switching, routers themselves don't process packets. Instead, they open a connection between the incoming link and the destination link, so that the hosts can communicate directly.</p> <p>It is like a direct mean between the hosts, with no sharing of resources with third parties. Old telephone networks relied on this method.</p> <p></p> <p>There are two methods to share the same channel: FDM and TDM.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Network%20Core/#frequency-division-multiplexing","title":"Frequency Division Multiplexing","text":"<p>FDM divides the frequency into bands, each communication instance can happen between the bounds of an assigned band. The trade-off in respect to TDM is that communications can happen anytime.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Network%20Core/#time-division-multiplexing","title":"Time Division Multiplexing","text":"<p>TDM divides time into slots, each communication instance is assigned slots when hosts can communicate. The trade-off in respect to FDM is that communications can allocate all the bandwidth.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Networks/","title":"Networks","text":"<p>A network is an interconnection of machines whose goal is communication of these devices. There are various types of networks, some of the most used are in the table below.</p> Network Type Goal Home private Medium download speed Enterprise private High download an upload speed Mobile public Oriented towards long-range wireless Internet public Connects all the previous networks <p>A network is composed of many things which permit connection and communication: devices, transport means, protocols.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Networks/#internet","title":"Internet","text":"<p>Internet is a network that connects them all. Access is granted through an ISP.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Networks/#internet-service-provider","title":"Internet Service Provider","text":"<p>Companies that provide access to the Internet through a contract, they sell either wired and wireless access.</p> <p>Usually ISPs work on a national and international level, but smaller regional ISPs should provide the same services.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Networks/#internet-exchange-point","title":"Internet Exchange Point","text":"<p>IXPs are hubs that interconnect various ISPs.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Networks/#content-provider-network","title":"Content Provider Network","text":"<p>A CPN is a private (usually enterprise) network that companies use to provide services and host servers that end user can use/connect to. To provide speed and efficiency, often CPN are on the same level of ISPs.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Security/","title":"Security","text":"<p>Nowadays, people could use anything for their personal gains, Internet included. So, it is imperative to learn what malicious people could do and how to counteract.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Security/#attacks","title":"Attacks","text":""},{"location":"Systems%20and%20Networking/Unit%202/Security/#packet-sniffing","title":"Packet Sniffing","text":"<p>People could access the transport mean of packets and read packets that are going through that transport mean (e.g. shared ethernet, wireless)!</p> <p>Malicious users could read the content of these packets and find sensitive information about sender and receiver (including passwords).</p>"},{"location":"Systems%20and%20Networking/Unit%202/Security/#packet-spoofing","title":"Packet Spoofing","text":"<p>Malicious users could send packets with a false source address, to imitate actions needed to further scopes.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Security/#denial-of-service","title":"Denial of Service","text":"<p>(Distributed) Denial of Service consists in cracking other network devices to then uniformly attack a selected target. The goal of this attack is to allocate as many resources to make the target unavailable, unreachable to other hosts, by sending as many useless packets as the cracked devices can.</p> <p>All the cracked network devices are called botnet.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Security/#defense","title":"Defense","text":""},{"location":"Systems%20and%20Networking/Unit%202/Security/#authentication","title":"Authentication","text":"<p>Authentication is a security measure which consists in providing a way to prove that you are who you say you are, so that other users/hardware cannot impersonate you.</p> <p>Phones provide authentication through SIM cards, but in the internet there's no other hardware support, software implementations are needed.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Security/#confidentiality","title":"Confidentiality","text":"<p>Even if other users could catch packets, these packets must not be readable (or at least the message must not be understandable).</p> <p>Confidentiality is implemented via encryption. </p>"},{"location":"Systems%20and%20Networking/Unit%202/Security/#integrity-checks","title":"Integrity Checks","text":"<p>A digital signature is inserted in the packet, so to check nobody tampered with it.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Security/#access-restrictions","title":"Access Restrictions","text":"<p>Devices could connect to password-protected VPNs, so that their devices are not \"talking\" over a public network.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Security/#firewalls","title":"Firewalls","text":"<p>Firewalls are either hardwares or softwares inserted in network cores, specialized in network analysis. The main functionalities are: - Filter incoming packets to restrict senders, receivers and applications; - Detecting and reacting to DOS.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Transport%20Means/","title":"Transport means","text":"<p>If two machines need to communicate, there has to be a mean of transport for the data to travel. Based on the network type, there are different options to consider.</p> <p>Wired (A cable which connects two ends, guided) - Coaxial Cable - Twisted Pair - Optic Fibre</p> <p>Wireless: (Radio waves, unguided) - TK</p>"},{"location":"Systems%20and%20Networking/Unit%202/Transport%20Means/#coaxial-cable","title":"Coaxial Cable","text":"<p>It's one of the oldest options, it consists of two insulated concentric copper conductors.</p> <p></p> <p>This cable can reach high speeds (based on the transmission protocol used), and it finds various usages, e.g.: telephonic transmissions, digital transmissions, television transmissions.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Transport%20Means/#twisted-pair","title":"Twisted Pair","text":""},{"location":"Systems%20and%20Networking/Unit%202/Transport%20Means/#optic-fibre","title":"Optic Fibre","text":""},{"location":"Systems%20and%20Networking/Unit%202/Transport%20Means/#radio-waves","title":"Radio Waves","text":""},{"location":"Systems%20and%20Networking/Unit%202/Internet/Application%20Layer/","title":"Application Layer","text":""},{"location":"Systems%20and%20Networking/Unit%202/Internet/Application%20Layer/#application","title":"Application","text":"<p>The application layer is the layer where application logic and protocols reside. The application layer contains the final message that devices want to exchange.</p> <p>The packet generated by the Application Layers is called message.</p> <p>The application defines message syntax and semantics on when to exchange messages. An application must also implement an application architecture.</p> <p>The application layer makes use of the transport layer, so it must decide which protocol to use (either TCP or UDP) based on needed characteristics, as: - Reliability/Loss - Connection - More throughput</p> <p>Examples of application layer protocols are: - Web: HTTP - E-Mail: SMTP, IMAP, POP3</p>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Layered%20Structure/","title":"Layered Structure","text":"<p>To better organize the packet structure and services, packets follow a layered structure. Each layer follows a Protocol, offers some functionalities, and uses the functionalities of the underlying layer.</p> <p>The protocol layers could be implemented by both hardware and software, depending on the functionalities needed. When the Internet was still in development, a theoretical model was developed: the internet protocol stack, which is similar to and was later deprecated in favor of the ISO/OSI Model.</p> <p>The internet protocol stack is formed by 5 layers, each layer encapsulates the packet that the upper-layer has passed and adds a functionality to it (in the form of header).</p> Layer Packet Name Description Application Layer Message Holds the final message of the packet. Transport Layer Segment Definition of transport protocol. Network Datagram Routing the packet to the destination. Link Frame Forwarding the packet to the next node. Physical N/A Physically transmitting signals. <p></p>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Layered%20Structure/#transport","title":"Transport","text":"<p>The transport layer is the layer that wraps the application layer and initializes the transport inside the application. A transport protocol is chosen (i.e. TCP or UDP), which also decides how the packet will be handled (e.g. reliability in TCP).</p> <p>The packet generated by the Transport Layer is called segment.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Layered%20Structure/#network","title":"Network","text":"<p>The network layer provides the service of delivering the segment to the transport layer of the destination host. It works on the Internet Protocol, which all network devices must understand. It is also called IP Layer, in refer to the fact that the internet protocol is the main protocol keeping the internet connected.</p> <p>The packet generated by the Network Layer is called datagram.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Layered%20Structure/#link","title":"Link","text":"<p>The link layer is in charge of moving the datagram from one node (host or router) to the next. The characteristics of the link layer change over the protocol of the link (e.g. WiFi, ethernet).</p> <p>The packet generated by the Link Layer is called frame.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Layered%20Structure/#physical","title":"Physical","text":"<p>The physical layer is the final layer that is in charge of physically moving the packet from one device to the other. The means of doing so change depending on the actual transport means.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Network%20Layer/","title":"Network Layer","text":"<p>The network layer provides routing and addressing functions, enabling the transfer of data packets across interconnected networks. It operates above the data link layer and below the transport layer in the TCP/IP protocol suite.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Network%20Layer/#ip-packet-structure","title":"IP Packet Structure","text":"<ol> <li>Version: The IP version field indicates the version of IP being used, such as IPv4 or IPv6.</li> <li>Header Length: The header length field specifies the length of the IP header in 32-bit words.</li> <li>Type of Service (ToS): The ToS field defines the quality of service, including priority, delay, throughput, and reliability.</li> <li>Total Length: This field indicates the total length of the IP packet, including the header and data.</li> <li>Identification: The identification field is used for uniquely identifying each IP packet.</li> <li>Flags: The flags field contains control bits for fragmentation and reassembly of IP packets.</li> <li>Fragment Offset: In fragmented IP packets, the fragment offset field specifies the position of each fragment within the original packet.</li> <li>Time to Live (TTL): The TTL field represents the maximum number of hops (routers) an IP packet can traverse before being discarded.</li> <li>Protocol: The protocol field identifies the transport layer protocol (e.g., TCP, UDP) to which the IP packet should be passed after routing.</li> <li>Header Checksum: The header checksum field is used to verify the integrity of the IP header.</li> <li>Source IP Address: This field contains the IP address of the sender (source) of the IP packet.</li> <li>Destination IP Address: The destination IP address field specifies the IP address of the intended recipient of the IP packet.</li> <li>Options: The options field is optional and used for including additional parameters or functionality in the IP packet header.</li> <li>Padding: If needed, the padding field is used to ensure that the IP header aligns to a 32-bit boundary.</li> <li>IP Payload: The IP payload consists of the encapsulated data from the upper-layer protocol (e.g., TCP, UDP).</li> </ol>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Network%20Layer/#ip-addressing","title":"IP Addressing","text":"<p>IP addresses are used to uniquely identify devices (hosts) connected to an IP network. There are two main versions of IP addressing:</p> <ul> <li>IPv4 Addressing: IPv4 addresses are 32 bits long and are typically represented as four decimal numbers separated by dots (e.g., 192.168.0.1). IPv4 allows for approximately 4.3 billion unique addresses.</li> <li>IPv6 Addressing: IPv6 addresses are 128 bits long and are represented as eight groups of hexadecimal numbers separated by colons (e.g., 2001:0db8:85a3:0000:0000:8a2e:0370:7334). IPv6 provides a significantly larger address space, allowing for an enormous number of unique addresses.</li> </ul> <p>IP addresses are divided into network and host portions. The network portion identifies the network to which the device is connected, while the host portion identifies a specific device within that network. IP routing is performed based on the network portion of the IP address.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Network%20Layer/#ip-routing","title":"IP Routing","text":"<p>IP routing is the process of forwarding IP packets from a source host to a destination host across multiple interconnected networks. Routers are responsible for examining the destination IP address in each packet's header and determining the appropriate path for forwarding.</p> <p>Routing tables are used by routers to make forwarding decisions. These tables contain information about network addresses and corresponding next-hop routers. Routing protocols, such as RIP, OSPF, and BGP, help routers exchange routing information and update their routing tables dynamically.</p> <p>During the routing process, routers inspect the destination IP address and use their routing tables to determine the next-hop router that should receive the packet. The packet is then forwarded to the next-hop router until it reaches its destination network.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Network%20Layer/#ip-fragmentation-and-reassembly","title":"IP Fragmentation and Reassembly","text":"<p>IP fragmentation allows large IP packets to be divided into smaller fragments to fit within the maximum transmission unit (MTU) of the underlying network. The receiving host reassembles these fragments back into the original packet.</p> <p>If an IP packet is too large for a network link's MTU, the sender divides the packet into smaller fragments. Each fragment contains a portion of the original packet's data, along with additional IP header fields for fragmentation and reassembly.</p> <p>The destination host uses the identification, flags, and fragment offset fields in the IP header to reassemble the fragments into the original packet. Fragments arriving out of order or lost during transmission can be retransmitted by the sender or discarded by the receiver.</p> <p>Fragmentation introduces overhead and can impact network performance. Therefore, it is generally desirable to avoid fragmentation whenever possible by adjusting the packet size or using Path MTU Discovery to determine the maximum packet size that can be transmitted without fragmentation.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Network%20Layer/#ip-multicasting","title":"IP Multicasting","text":"<p>IP multicasting enables the efficient delivery of IP packets to a group of hosts that have joined a multicast group. Multicast groups are identified by multicast IP addresses, which are a subset of IPv4 and IPv6 addresses reserved for multicast communication.</p> <p>A sender sends a single copy of the IP packet to the multicast group's IP address, and routers in the network replicate and forward the packet only to those interfaces connected to hosts that have joined the multicast group.</p> <p>[!tip] Multicasting Usages</p> <p>Multicasting is used for applications such as video streaming, online gaming, and real-time communication, where the same data needs to be distributed to multiple recipients simultaneously.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Packet%20Delay/","title":"Packet Delay","text":"<p>Packets sent with packet switching can be lost and suffer delay:</p> <ul> <li>Packets lost need to be sent again;</li> <li>Every packet suffers from delay that is given from different sources.</li> </ul> <p>Packet Delay is given from the following sources.</p> Order Type Description 1. Processing Every router checks for errors in the packet and which way it should take. 2. Queueing The packet has to wait for other packets before itself to be sent. 3. Transmission Depends on the length of the packet. Longer packets take more to be transmitted. 4. Propagation Depends on the length and propagation speed of the physical mean. <p>The whole delay a packet can suffer is given from the following sum.</p> \\[\\large d_\\text{tot} = d_\\text{proc} + d_\\text{queue} + d_\\text{transm} + d_\\text{prop} \\ [s]\\]"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Packet%20Delay/#processing-delay","title":"Processing Delay","text":"<p>Processing delay is the time routers take to check for errors in the packet, plus the time it takes to decide where to send it. Usually, it's in the order of microseconds.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Packet%20Delay/#queueing-delay","title":"Queueing Delay","text":"<p>Queueing delay is the time a packet spends in the router's waiting queue, before it is forwarded. It's highly variables, and depends on the level of congestion of the router.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Packet%20Delay/#transmission-delay","title":"Transmission Delay","text":"<p>Transmission delay is the time it takes a whole packet to be sent from the router (output speed). It is dependent on the packet length \\(L\\ [bits]\\) and the transmission rate \\(R\\ [bps=\\frac{bits}{s}]\\), and is given from the following ratio.</p> \\[\\large d_\\text{transm} = \\frac{L}{R}\\ [s]\\]"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Packet%20Delay/#propagation-delay","title":"Propagation Delay","text":"<p>Propagation delay is the time it takes the signal to travel through the whole transport mean. It is dependent on the length of the mean \\(d\\ [m]\\) and the propagation speed \\(s\\ [\\frac{m}{s}]\\), and is given from the following ratio.</p> \\[\\large d_\\text{prop} = \\frac{d}{s}\\ [s]\\]"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Packet%20Delay/#traffic-intensity","title":"Traffic Intensity","text":"<p>Take a queue with 10 packets: the first packet will be sent immediately, while the last packet will have to wait for 9 packets to be sent.</p> <p>Now, imagine an average packet arrival rate \\(a \\ [\\frac{packets}{s}]\\), meaning a rate of how many packets arrive in a second. We can use this variable to compute an average queueing rate, where \\(L,R\\) are the same variables from the Transmission Delay.</p> \\[\\large     \\text{traffic intensity} = \\frac{L \\cdot a}{R}     = \\frac{\\text{bits incoming}}{\\text{bits outgoing}}     = a \\cdot d_\\text{transm} \\] <p>Traffic intensity is the ratio of how many bits are arriving vs. how many bits are being transmitted. Different values of the ratio mean different things:</p> <ul> <li>\\((0 \\le r &lt; 1)\\) The queue is decreasing or packets are being forwarded immediately;</li> <li>\\((r = 1)\\) The average number of packets in the queue is fixed, neither increasing nor decreasing;</li> <li>\\((r &gt; 1)\\) The packets in the queue are increasing and, if full, arriving packets are lost (infinite average queueing delay).</li> </ul>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Packet%20Delay/#throughput","title":"Throughput","text":"<p>Throughput is the rate at which bits are actually delivered from the sender to the receiver, measured in \\(\\frac{bits}{s}\\). It can be either instantaneous (at a given point in time) or average (over a span of time).</p> <p>Throughput is measured from sender to receiver, meaning we need to account for all routers/links in the path. In the path there's always a bottleneck, a link that caps the transfer rate to a certain limit: if in a path there's a link which supports at max 1K bits/s, even though other links could support higher speeds the minimum speed will always be 1K bits/s (meaning, no matter what other higher speeds there are, the transmission delay will always be affected the most by the lower speed).</p>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Packet/","title":"Packet","text":"<p>A packet is a uniform data structure that is sent through the network. Usually, hosts in a network communicate using packets.</p> <p>Packet Delay</p> <p>Layered Structure</p>"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Subnets/","title":"Subnets","text":""},{"location":"Systems%20and%20Networking/Unit%202/Internet/Transport%20Layer/","title":"Transport Layer","text":"<p>The Transport Layer is the fourth layer of the Internet Protocol Stack, its reason is to provide a logical channel for communication between two applications across the network.</p> <p>Transport-layer features are implemented in end hosts only, routers and switches work from the network layer down. To send a message from one host to another:</p> <ol> <li>On the sending host, the transport layer breaks the message in smaller pieces to which is added a transport header. The final packets are called segments;</li> <li>The segments are then passed to the network layer (segments are now called datagrams), which does its job and routes the datagrams to the destination host;</li> <li>On the destination host, the network layer receives the datagrams, unpacks the segments and passes them to the transport layer;</li> <li>The transport layer processes the received segments, builds the message and passes it to the application layer.</li> </ol> <p>The internet works on two transport protocols: TCP and UDP, each with its set of services. Some of the services that both cannot provide, though, are delay guarantee (i.e. maximum delay) and bandwidth guarantee (i.e. minimum throughput).</p> Protocol Reliability Packet Ordering Connection TCP Yes Yes Yes UDP No No No"},{"location":"Systems%20and%20Networking/Unit%202/Internet/Transport%20Layer/#multiplexing-and-demultiplexing","title":"Multiplexing and Demultiplexing","text":"<p>As already said, the transport layer provides a virtual channel between applications that want to communicate. The base of this channel is sockets: the source application writes into the socket and the destination application reads from the socket.</p> <p>An host, though, can hold many connections, each of which can be accessed through socket; when a packet arrives, the transport layer has to redirect the packet through the right socket.</p> <p>Sockets are handled with the following criteria: - UDP sockets (connectionless) are identified through the two-tuple destination address and port, any packet (regardless of source) sent to the receiving host will be forwarded through the socket identified by the specified port; - TCP sockets (connection) are identified through the four-tuple source address and port, destination address an port, any packet sent to the receiving host will be forwarded to the socket identified by the specified tuple. If no socket is identified by that tuple, a new socket is created.</p> <p>[!warning] UDP sockets</p> <p>UDP sockets don't recognize the source (i.e. who sent the packet), so it is possible that applications from multiple hosts could talk to one receiver.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DHCP/","title":"Dynamic Host Configuration Protocol","text":"<p>The Dynamic Host Configuration Protocol (a.k.a. DHCP) is a network protocol that provides automatic IP address allocation and other network configuration parameters to devices on a network. It simplifies the process of network configuration by centrally managing and assigning IP addresses to devices.</p> <p>DHCP operates in a client-server model, consisting of the following components:</p> <ul> <li> <p>DHCP Client: The client is a device that requests network configuration parameters from a DHCP server. It typically sends a DHCP Discover message to locate available DHCP servers on the network.</p> </li> <li> <p>DHCP Server: The server is responsible for managing and allocating IP addresses and network configuration information to DHCP clients. It receives DHCP requests from clients and responds with DHCP offers containing IP address lease information.</p> </li> </ul> <p>The DHCP process involves the following steps:</p> <ol> <li> <p>DHCP Discover: When a DHCP client boots up or connects to a network, it broadcasts a DHCP Discover message to discover available DHCP servers. The message contains information such as the client's MAC address and network configuration requirements.</p> </li> <li> <p>DHCP Offer: DHCP servers that receive the DHCP Discover message respond with a DHCP Offer message. The offer includes an available IP address and other configuration parameters, such as subnet mask, default gateway, DNS server addresses, and lease duration.</p> </li> <li> <p>DHCP Request: The DHCP client selects one of the offered IP addresses and sends a DHCP Request message to the DHCP server. The request indicates the client's acceptance of the offered IP address and configuration parameters.</p> </li> <li> <p>DHCP Acknowledgment: Upon receiving the DHCP Request message, the DHCP server sends a DHCP Acknowledgment message to confirm the IP address lease and provide any additional configuration information. The client can now use the assigned IP address and network settings.</p> </li> <li> <p>DHCP Lease Renewal: DHCP leases are typically temporary, with a predefined lease duration. Before the lease expires, the client can request a lease renewal from the DHCP server. This process helps ensure that IP addresses are efficiently managed and re-allocated when no longer in use.</p> </li> </ol>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DHCP/#benefits-of-dhcp","title":"Benefits of DHCP","text":"<ul> <li> <p>Automatic IP Address Configuration: DHCP eliminates the need for manual IP address assignment, simplifying network administration and reducing the chances of address conflicts.</p> </li> <li> <p>Centralized Management: DHCP servers provide centralized control over IP address allocation and configuration parameters, making it easier to manage and modify network settings.</p> </li> <li> <p>Efficient Address Utilization: DHCP leases allow for dynamic allocation of IP addresses. When devices are no longer connected or their leases expire, the IP addresses can be released and reused by other devices.</p> </li> <li> <p>Reduced Configuration Errors: DHCP reduces human errors associated with manual IP configuration, such as mistyped addresses or incorrect subnet settings.</p> </li> <li> <p>Scalability: DHCP scales well in large networks, allowing efficient IP address management for a large number of devices.</p> </li> <li> <p>Flexibility: DHCP supports additional configuration options beyond IP addresses, such as DNS server addresses, domain names, and other network parameters.</p> </li> </ul> <p>By providing automatic IP address allocation and network configuration, DHCP simplifies network administration, enhances network efficiency, and improves overall usability for connected devices.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DNS/","title":"Domain Name System","text":"<p>Hosts in a network can be identified by their IP address, which are easy to understand for machines. For humans, though, they're harder to remember, especially when there's many to remember.</p> <p>To fix this problem, the Domain Name System has been created. The DNS associates one or more alphanumeric addresses to one IP address, so that humans may use this canonical mnemonic address instead of numeric ones.</p> <p>The DNS is both: 1. a distributed database implemented in a hierarchy of DNS servers; 2. an application-layer protocol that allows hosts to query said database.</p> <p>[!info] The protocol The DNS is an application-layer protocol that can be used parallelly to other application-layer protocols such as HTTP and SMTP. It uses UDP over port 53.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DNS/#services","title":"Services","text":"<ul> <li>Host Aliasing</li> </ul> <p>TK</p> <ul> <li>Mail Server Aliasing</li> </ul> <p>TK</p> <ul> <li>Load Distribution</li> </ul> <p>TK</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DNS/#structure","title":"Structure","text":"<p>The DNS database is a distributed database, the hierarchy is so organized: 1. Root DNS servers 2. Top-Level Domain (TLD) servers 3. Authoritative servers 4. (Local DNS server)</p> <p>Hierarchy of DNS servers</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DNS/#root-dns-server","title":"Root DNS Server","text":"<p>The Root DNS servers are the main entry point for DNS queries, they contain the addresses to the TLD servers.</p> <p>Root servers are managed by 13 different organizations, and there are more than 400 servers scattered all over the world. This is done to reduce traffic volume, response time and to prevent a single point of failure. Moreover, maintenance would be easier too.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DNS/#top-level-domain-server","title":"Top-Level Domain Server","text":"<p>Each TLD (e.g. <code>org</code>, <code>com</code>, <code>it</code>) has an associated server (or cluster of servers), which point to the correlated authoritative servers. Every TLD server is managed by a large company.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DNS/#authoritative-server","title":"Authoritative Server","text":"<p>Authoritative servers are the final leaf of the distributed hierarchy, they contain actual records that bind mnemonic DNS addresses to IP addresses.</p> <p>This type of server is managed by companies, every company/private could have its own authoritative server to route DNS addresses of their public services to IP addresses. If someone decides that they do not want to host a DNS server, they could always pay another company to add DNS records in their stead.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DNS/#local-dns-server","title":"Local DNS Server","text":"<p>A local DNS server doesn't strictly belong to the DNS hierarchy, but it's still central for hosts connecting through ISPs. Each ISP has a local DNS server (also called default name server), which acts as an entry-point to the hierarchy.</p> <p>When an host makes a request using DNS, the default name server acts as a proxy which forwards the request to servers in the hierarchy, without needing to follow the proper order (perhaps because of a cache).</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DNS/#queries","title":"Queries","text":"<p>The DNS works through queries: the host asks the DNS to resolve a hostname, queries can either be iterated or recursive.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DNS/#iterated-queries","title":"Iterated Queries","text":"<p>Iterated queries work like this: 1. The host asks to the local server to resolve an hostname; 2. The local server asks the root server, which sends a list of TLD servers; 3. The local server asks the TLD servers, which returns an authoritative server; 4. The local server asks the authoritative server, which could act as an additional layer to multiple nested authoritative servers. In this case, the authoritative resolves the hostname either directly or through recursive queries (in the case of nested authoritative servers); 5. The local server then returns the resolved hostname to the requesting host.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DNS/#recursive-queries","title":"Recursive Queries","text":"<p>Recursive queries work like this: 1. The host asks to the local server to resolve an hostname; 2. The local server asks the root server to resolve the hostname; 3. The root server asks the right TLD server to resolve the hostname; 4. The TLD server asks the right authoritative server to resolve the hostname; 5. The authoritative server resolves the hostname either directly or through an additional query to the right authoritative server; 6. The resolved hostname then travels back to the requesting host in the reversed order of querying: authoritative, TLD, root, local, host.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DNS/#caching","title":"Caching","text":"<p>Similarly to HTTP, going every time from the root server to the authoritative one increases traffic load on the network. To reduce this strain, DNS server can (and must) cache resolved hostnames.</p> <p>This way, if an host was to ask to the local server about an hostname: - if the local host contains the resolved hostname, it'll return that directly; - otherwise, if the local host contains the resolved address a right TLD server, it'll ask the TLD server; - if even no TLD server address is stored, the local server will ask the root server and the query will be just like the initial examples.</p> <p>[!info] Root DNS This caching system makes it so the root servers are actually a last resource, to contact if not even the right TLD server is known from the local server.</p> <p>DNS mappings, though, could change any moment (e.g. maintenance); this leads to two things: - defining a caching time limit; - defining a way to tell if a resolved hostname is absolute or not.</p> <p>[!abstract] Caching Time Caching time is finite, usually two days, but it can be configured using TTL. The reason is to update regularly DNS mappings.</p> <p>[!abstract] Authoritative Mapping A DNS query response can be marked as <code>Authoritative</code> or <code>Non-Authoritative</code>.</p> <ul> <li><code>Authoritative</code> says that the mapping is coming from an authoritative server, so the mapping is absolute;</li> <li><code>Non-Authoritative</code> says that the mapping is coming from a cache, so the mapping could have changed in the meantime.</li> </ul>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DNS/#dns-records","title":"DNS Records","text":"<p>Every mapping is stored in the distributed database in a four-tuple called Resource Record (RR), constructed like this: <code>(Name, Value, Type, TTL)</code>.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DNS/#ttl","title":"TTL","text":"<p>The Time-To-Live is the amount of time after which a cached mapping should expire, removed from the cache.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DNS/#type","title":"Type","text":"<p>DNS records have more than a type, based on the application of the mapping.</p> Type Name Value Description <code>A</code> A hostname Associated IP address The classic hostname-IP address mapping. <code>NS</code> A domain (e.g. foo.com) Authoritative server's IP address The mapping of a domain to the relative authoritative server. <code>CNAME</code> A hostname A canonical hostname An alias for a hostname to a more mnemonic hostname (e.g. <code>foo.com</code> to <code>www.foo.com</code>) <code>MX</code> A hostname Associated hostname/IP address Canonical name of the mail server. This way, the mail server can have the same hostname as the web server, but with different IPs (e.g. <code>foo.com</code> to <code>mail.bar.foo.com</code>)."},{"location":"Systems%20and%20Networking/Unit%202/Protocols/DNS/#dns-messages","title":"DNS Messages","text":"<p>DNS messages identify both queries and responses, as they have the same structure. A DNS message is composed of a header part, of size 12 bytes, and 4 other sections.</p> <p></p> <p>The header is divided into 6 categories of 2 bytes each: 1. Identification, a number that identifies the query, so the response can then be matched using this; 2. Flags, some 1-bit flags; 3. Number of questions, how many hostnames in the query should be resolved; 4. Number of answer RRs, how many resolved Resource Records are in the answer; 5. Number of authority RRs, how many authority Resource Records are in the answer; 6. Number of additional RRs, how many Resource Records with additional information are in the answer.</p> <p>Not all the bits in the flags section are used, some are reserved for future use. Some of the flags are: - Query/Flag, indicates whether the message is a query or a reply; - Authoritative, indicates whether the server is authoritative; - Recursion desired, indicates whether the clients prefers to perform the query using recursion instead of iterating; - Recursion available, indicates whether the server can execute recursive queries.</p> <p>The other sections are: 1. Question, contains the actual hostnames to resolve. To resolve an hostname, the client must send the name and the type of mapping (e.g. <code>A</code>, <code>NS</code>, <code>MX</code>); 2. Answer, contains the the resolved hostnames in form of Resource Records (i.e. <code>Name, Value, Type, TTL</code>); 3. Authority, contains records of other authoritative servers (e.g. recursive queries); 4. Additional Information, contains other useful records. For example, when querying an <code>MX</code> record that maps to a canonical name, this section could contain the <code>A</code> record of the canonical name.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/Email/","title":"Email","text":"<p>Email was and is one of the most important tools the internet hosts. TK</p> <p>The email technology has three components:</p> Component Description User agents Clients that lets users communicate with mail servers Mail servers Servers that store and send email to other servers SMTP Protocol used to send and exchange email POP3 or IMAP Protocols used to download email <p></p> <p>Each user registered within a mail server has a mailbox, which manages and maintains the messages sent to them, with relative attachments.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/Email/#simple-mail-transfer-protocol","title":"Simple Mail Transfer Protocol","text":"<p>SMTP is the core protocol of email, it's the protocol that lets mail servers exchange email messages. It needs reliability, so SMTP uses TCP as transport protocol.</p> <p>SMTP works like this: 1. A sender uses its user agent to compose a message, set the receiver's email address and send the message; 2. The sender's user agent sends the message to the configured mail server, which places it in a message queue; 3. The sender's mail server (which now acts as a client) opens a TCP connection to the receiver's mail server; 4. After an initial SMTP handshake, the client sends the sender's message to the receiver's mail server; 5. The receiver's mail server places the message in the receiver's mailbox; 6. The receiver can then use his user agent to retrieve the message whenever he wants.</p> <p>TK SMTP port</p> <p>If the receiver's server isn't able to open a connection, the message stays in the sender's server's message queue. The sender's server will try to send it again after some time passed.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/Email/#smtp-commands","title":"SMTP Commands","text":"<p>The core of SMTP works by exchanging human-readable messages. In these messages, the sender server executes some commands, to which the receiver server replies with codes and optional messages.</p> Command Parameter Description <code>HELO</code> Sender server's hostname Short for HELLO, introduces the sender's server. <code>MAIL FROM</code> Sender's address Specifies the sender's address. <code>RCPT TO</code> Receiver's address Specifies the receiver's address. <code>DATA</code> Message The actual message to be transmitted. This can be multi-line, and ends with a line with just a dot. (i.e. <code>CRLF.CRLF</code>) <code>QUIT</code> N/A Tells the receiver's server to close the TCP connection. <p>If the sender has multiple messages to send to the same receiver, a new connection isn't required every time. Once the sender has introduced itself with the <code>HELO</code> command, it can send a new message by opening with the <code>MAIL FROM</code> command and closing it by ending the <code>DATA</code> command (i.e. <code>CRLF.CRLF</code>). After sending all the messages, the connection can be closed with the <code>QUIT</code> command.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/Email/#message-format","title":"Message Format","text":""},{"location":"Systems%20and%20Networking/Unit%202/Protocols/Email/#headers","title":"Headers","text":"<p>Much like HTTP, SMTP messages can include headers. The main difference, though, is that while HTTP headers are useful only to browsers and web servers, SMTP headers are useful only to end users (i.e. receivers).</p> <p>Headers in SMTP messages provide a way for senders to add more information to receivers, like an address to reply to.</p> <p>SMTP headers, like HTTP headers, are key-value pairs that each compose one line at the start of the message, and are separated by an empty line from the actual message. Some are required (<code>From</code>, <code>To</code>, <code>Subject</code>), while others are optional</p> <p>[!warning] Note Headers differ from SMTP commands. While the headers <code>From</code> and <code>To</code> repeat information already provided during the SMTP handshake, this information is actually not used by the servers, but only to the end users. These headers could be different from the information used during the handshake, but the receiving user will still be the one provided with commands.</p> <p>A typical SMTP message (just the message) looks like this:</p> <pre><code>From: alice@crepes.fr\nTo: bob@hamburger.edu\nSubject: Studying SMTP.\n\n(message)\n</code></pre>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/Email/#encoding","title":"Encoding","text":"<p>The SMTP standard works by sending messages encoded with 7-bit ASCII. Any character encoded in any other way will be sent as a 7-bit ASCII character(s) anyway. This includes any binary part and attachments.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/Email/#attachments","title":"Attachments","text":"<p>Attachments are included at the end of the message, encoded as 7-bit ASCII characters.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/Email/#example-of-smtp-exchange","title":"Example of SMTP Exchange","text":"<p><code>S:</code> marks the messages sent by the receiver's server, while <code>C:</code> the ones sent by the sender's server.</p> <pre><code>S: 220 hamburger.edu  \nC: HELO crepes.fr  \nS: 250 Hello crepes.fr, pleased to meet you\nC: MAIL FROM: &lt;alice@crepes.fr&gt;  \nS: 250 alice@crepes.fr ... Sender ok  \nC: RCPT TO: &lt;bob@hamburger.edu&gt;  \nS: 250 bob@hamburger.edu ... Recipient ok  \nC: DATA  \nS: 354 Enter mail, end with \".\" on a line by itself  \nC: Do you like ketchup?  \nC: How about pickles?  \nC: .  \nS: 250 Message accepted for delivery  \nC: QUIT  \nS: 221 hamburger.edu closing connection\n</code></pre>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/Email/#reading-emails","title":"Reading Emails","text":"<p>While a sender can use SMTP to send email messages, these messages will reach the receiver's email server, where they will be stored in the respective mailbox. To read the messages, the receiver has to use other designated protocols: - POP3 - IMAP</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/Email/#pop3","title":"POP3","text":"<p>POP3 is the first mail access protocol created, and is quite simple. This simplicity, though, limits its features.</p> <p>POP3 works like this: 1. Authorization, the user agent uses username and password (sent in cleartext) to log in the mail server; 2. Transaction, the user agent retrieves all the emails and can do actions such as mark messages for deletion, remove marks for deletion and obtain mail statistics; 3. Update, it occurs after the clients issues the <code>quit</code> message, which ends the POP3 session. The mail server deletes all the messages previously marked for deletion.</p> <p>The problem with POP3 is this basic featuring: messages can be stored in folders and categories just once they've been downloaded in the end device. If a user downloads emails on another user agent, such changes won't be seen in this other client.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/Email/#pop3-commands","title":"POP3 Commands","text":"<p>Much like SMTP, POP3 works by exchanging messages. The client can send commands, to which the server can respond in two ways: <code>+OK</code> and <code>-ERR</code>, both can be followed by an additional message.</p> Command Parameters Description <code>user</code> Username Necessary to authenticate, must be the first command. <code>pass</code> Password Necessary to authenticate, must come after <code>user</code>. <code>list</code> N/A Lists the messages stored in the mailbox. <code>retr</code> Message ID Retrieves the content of a message stored in the mailbox. <code>dele</code> Message ID Marks a message for deletion. <code>quit</code> N/A Tells the server to close the connection."},{"location":"Systems%20and%20Networking/Unit%202/Protocols/Email/#example-of-pop3-exchange","title":"Example of POP3 Exchange","text":"<pre><code>S: +OK POP3 server ready  \nC: user bob  \nS: +OK  \nC: pass hungry  \nS: +OK user successfully logged on\nC: list  \nS: 1 498  \nS: 2 912\nS: .  \nC: retr 1  \nS: (blah blah ...  \nS: .................  \nS: ..........blah)  \nS: .  \nC: dele 1  \nC: retr 2  \nS: (blah blah ...  \nS: .................  \nS: ..........blah)  \nS: .  \nC: dele 2  \nC: quit  \nS: +OK POP3 server signing off\n</code></pre>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/Email/#imap","title":"IMAP","text":"<p>To fix POP3's limitations, IMAP has been created. IMAP is more complex that POP3, but offers more functionalities.</p> <p>TK</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/HTTP/","title":"HyperText Transfer Protocol","text":"<p>The HyperText Transfer Protocol is the main application-layer protocol used in the Web. HTTP has been developed to permit the exchange of documents between hosts through the web.</p> <p>Clients ask, through a structured message in the application layer, to servers a web page (document), which is composed of objects; these objects could be any file (e.g. image, html file, video file).</p> <p>A document is uniquely identified by an Uniform Resource Locator (URL):</p> \\[\\large     \\underbrace{\\text{http://}}_{(1)\\ protocol}     \\underbrace{\\text{github.com}}_{(2)\\ hostname}     \\underbrace{\\text{/favicon.png}}_{(3)\\ resource} \\] <ol> <li>Application protocol to be used. In this case, HTTP;</li> <li>Hostname (either IP or DNS address) of the server;</li> <li>Path of the resource to get.</li> </ol> <p>The default port of HTTP is <code>80</code>, which is usually implicit. If a server exposes a website on another port, though, it can be specified in the URL:</p> \\[\\large     \\text{http://github.com}     \\underbrace{\\text{:8080}}_{(4)\\ port}     \\text{/favicon.png} \\] <ol> <li>Explicit port where the website is exposed.</li> </ol> <p>HTTP uses the TCP transport-layer protocol, so thanks to lower layers HTTP does not have to think about reliability and insuring the message got across. Instead, it can think of just sending the documents to clients.</p> <p>HTTP is called a stateless protocol, because servers do not store information about the client. If a client asks for the same file two times in a row, the server will send that same file two times.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/HTTP/#connection","title":"Connection","text":"<p>The HTTP connection can be either non-persistent or persistent, meaning whether the server can transfer more than one file in one open TCP connection.</p> <p>Take for example a document composed of one HTML page <code>index.html</code> and 5 PNG images, all residing on the same server with address <code>site.edu</code>.</p> <ul> <li> <p>Non-persistent</p> </li> <li> <p>The client initiates a TCP connection with the server on port 80 (<code>site.edu:80</code>);</p> </li> <li>The client sends an HTTP request message through the URL <code>http://site.edu/index.html</code>;</li> <li>The server processes the request, retrieves the document and sends it via an HTTP response message;</li> <li>The server tries to close the TCP connection (only closes once the server knows the client got everything right);</li> <li>The client receives the response and extracts the document from the response message, while the connection is terminated. While extracting the document it finds references to the 5 images;</li> <li>The first 4 steps are executed for each image.</li> </ul> <p>To the time it takes to transfer all the files, we'd have to add 2 RTT for each TCP handshake needed to open a new connection. Moreover, looking at both client and server, there's the need to allocate new resources (i.e. socket) every time a new connection is opened. This is crucial especially from the server side, which is processing many requests simultaneously.</p> <ul> <li> <p>Persistent</p> </li> <li> <p>The client initiates a TCP connection with the server on port 80 (<code>site.edu:80</code>);</p> </li> <li>The client sends an HTTP request message through the URL <code>http://site.edu/index.html</code>;</li> <li>The server processes the request, retrieves the document and sends it via an HTTP response message;</li> <li>The server waits an amount of time before closing the connection, so that the client can follow instantly with new requests;</li> <li>The client receives the response and extracts the document from the response message, with the connection still open. While extracting the document it finds references to the 5 images, so it opens requests for each new file;</li> <li>The server receives the requests and tries to send the files concurrently;</li> <li>The connection is closed either by the client, after receiving the files, or by the server, after a timeout.</li> </ul> <p>A persistent connection takes just 1 RTT for each file requested (plus the one initiating the TCP connection), so it is faster than a non-persistent one \\((n+1 \\le 2n)\\), and there's no need to allocate new socket resources for each file.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/HTTP/#format","title":"Format","text":"<p>The HTTP defines two types of messages: request (used to request an object) and response (used to return an object).</p> <p>HTTP Request <pre><code>GET /somedir/page.html HTTP/1.1\nHost: excale.ovh\nConnection: close\nUser-agent: Mozilla/5.0\nAccept-language: it, en\n</code></pre></p> <p>HTTP Response <pre><code>HTTP/1.1 200 OK\nConnection: close\nDate: Thu, 03 Nov 2022 16:40:43 GMT\nServer: Apache/2.2.3 (CentOS)\nLast-Modified: Thu, 03 Nov 2022 16:40:43 GMT\nContent-Length: 6821\nContent-Type: text/html\n\n(entity body...)\n</code></pre></p> <p>An HTTP message is a simple human-readable ASCII message, containing some lines.</p> <p>The first line is the request line or status line, respectively for requests and responses. Then are the headers, key-value pairs that are like variables included in requests and responses.</p> Name Contains Request Line Method, URL, HTTP version Status Line HTTP version, Status code, Status message <p>Each line is terminated by a combination of carriage return <code>cr</code> and line feed <code>lf</code>. Moreover, there's a blank line after the header lines (this too terminated by <code>cr</code> and <code>lf</code>).</p> <p>After the header lines is the space for the entity body: the object that needs to be exchanged between client and server. The server isn't the only one that can send objects; in fact, with some methods, the client can send objects too.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/HTTP/#methods","title":"Methods","text":"<p>The method of an HTTP request tells the server how to interpret the request and what to send as a response.</p> Method Description GET The client requests an object POST TK PUT TK DELETE TK HEAD TK"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/HTTP/#status","title":"Status","text":"<p>TK</p> Code Message Description <code>2XX</code> <code>200</code> OK <code>3XX</code> <code>301</code> Moved Permanently <code>4XX</code> General Client Error <code>400</code> Bad Request <code>404</code> Not Found <code>5XX</code> General Server Error <code>505</code> HTTP Version Not Supported"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/HTTP/#headers","title":"Headers","text":"<p>Headers are a key-value pair and are one of the key information in HTTP messages. Most headers are optional, except some required ones.</p> <p>Except required ones, headers are a way the HTTP offers to customize responses based on the requesting client.</p> Header Host Possible Values Description <code>Host</code> Client Address (either IP or DNS) Destination of the request. <code>Connection</code> Both TK Whether to keep the TCP connection open. <code>User-agent</code> Client Any String Code of the requesting web browser. <code>Accept-language</code> Client Any Locale Tells the server to send the object in a preferred language, if possible. <code>Date</code> Server Any Date Indicate the date at which the response has been created/object retrieved. <code>Server</code> Server <code>Last-Modified</code> Server <code>Content-Length</code> Server <code>Content-Type</code> Server <code>If-modified-since</code> Client Any Date <code>Cookie</code> Client <code>Set-Cookie</code> Server"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/HTTP/#cookies","title":"Cookies","text":"<p>The HTTP is stateless, so no information (either of client and server) is saved across requests. This simplifies the structure of HTTP and permits high-performance web servers, but takes aways customizability and some functionality.</p> <p>These characteristics can be reintroduced with cookies: special key-value pairs that go outside the scope of headers.</p> <p>The cookie technology has three main components: - One or more <code>Cookie</code> header lines in the HTTP request message; - One or more <code>Set-Cookie</code> header line in the HTTP response message; - A cookies file kept on the user's end system, which is managed by the browser.</p> <p>Cookies have a security policy: every website can store and read only its own cookies, while other cookies aren't accessible to it (e.g. <code>google.com</code>'s cookies aren't accessible to <code>facebook.com</code>)</p> <p>The browser stores the cookies in a file; whenever the client makes a request these cookies are sent under a <code>Cookie</code> header in a semicolon-separated list of <code>key=value</code> pairs (e.g. <code>Cookie: id=269; theme=dark</code>).</p> <p>The server can read these cookies to customize the response (or to track a session with the client); it can tell the client to store other cookies with the <code>Set-Cookie</code> header. The same logic of the <code>Cookie</code> header is applied: cookies are sent in a semicolon-separated list of <code>key=value</code> pairs, but there's more. The <code>Set-Cookie</code> header can also specify some attributes of the cookie, namely: - Domain and Path (a more granular security feature) - Expires or Max-Age (respectively date or seconds after which the cookie is deleted) - Secure and HttpOnly (whether the cookie can be read using https only and from scripting languages)</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/HTTP/#proxy-server","title":"Proxy Server","text":"<p>A proxy server is a web server that satisfies HTTP requests on behalf of another web server. It stands in the middle of a client and one or more servers, and can have various functions: - Security checks - Web cache - Load balancing</p> <p>Here we analyse especially the web cache functionality.</p> <p>The client won't make direct HTTP requests to the original server, but it will instead send the request to the proxy. The proxy has its own storage where it keeps frequently-asked files: - if the proxy can find the requested file in its storage, then it will respond directly to the client; - otherwise, the proxy will forward the request to the original server, which will (hopefully) provide the requested object. The proxy will save the object on its storage and forward the original server's response.</p> <p>[!info] Note Note that the proxy is both a server and a client: it is a server for the browser, and a client for the original server.</p> <p>Caches are important for two main reasons:</p> <ol> <li>They can reduce long-distances response time.</li> </ol> <p>Imagine a client in America that sends a request to a server in Europe. The RTT will take some time, because packets have to travel from America to Europe and then back to America.</p> <p>If the server providers were to install a web cache in America, the client would be able to ask to the proxy. If the proxy can provide the requested file, then there's no need for the client to ask the cross-continental server.</p> <p>And this, implicitly, is the other reason caches are important:</p> <ol> <li>They can reduce internet traffic.</li> </ol> <p>Reducing incoming traffic from America, more bandwidth will be available in Europe, especially to the server providers, which will have to answer less requests.</p> <p>Web caching introduces a problem: what if the server updates an object? The cache then will have an outdated version of the file.</p> <p>To solve this problem, before replying to the clients, a proxy performs a Conditional GET: the proxy sends a GET request to retrieve the object requested by the client, and appends a <code>If-modified-since</code> header containing a date. The origin server will: - send the whole object in the response, if the object has been modified after the specified date; - send a <code>304 Not Modified</code> response with an empty body, if the file stored in the cache is up-to-date.</p> <p>Conditional GET <pre><code>GET /fruit/kiwi.gif HTTP/1.1\nHost: excale.ovh\nIf-modified-since: Fri, 4 Nov 2022  12:23:56\n</code></pre></p> <p>Conditional response <pre><code>HTTP/1.1 304 Not Modified\nDate: Fri, 4 Nov 2022 12:25:06\nServer: Apache/1.3.0 (Unix)\n\n(empty entity body)\n</code></pre></p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/RDT/","title":"Reliable Data Transfer","text":"<p>Reliable Data Transfer is a theoretical transport layer protocol for reliably transferring a stream of packets over an unreliable channel; it is the foundations of TCP.</p> <p>There have been many version of RDT, with the protocol getting better in newer versions.</p> <ul> <li>[RDT 1.0] Assumes the underlying channel is reliable, the protocol focuses on creating and forwarding packets.</li> <li>[RDT 2.0] Acknowledges there could be errors in the packets, introduces <code>ACK</code> and <code>NACK</code> packets to either validate or invalidate the packets.</li> <li>[RDT 2.1] Acknowledges the <code>ACK</code> and <code>NACK</code> packets could get corrupt themselves.</li> <li>[RDT 2.2] Removes <code>NACK</code> packets, since the protocol could be simpler without them.</li> <li>[RDT 3.0] Acknowledges the packets could get lost, introduces packet timeout.</li> </ul> <p>RDT uses two finite state machines to manage the sender and the receiver algorithms.</p> <p>[!danger] Diagrams</p> <p>All FSM diagrams in this note are subject to future change, as right now I'm just screenshotting them from somewhere else.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/RDT/#rdt-10","title":"RDT 1.0","text":"<p>The first version of RDT assumes that the underlying channel is reliable, i.e. nothing will get corrupted or changed. The protocol simply focuses on creating and forwarding the packets.</p> <p></p> <p>--- Sender Side ---</p> <ol> <li>Create packet</li> <li>Send packet</li> <li>Repeat</li> </ol> <p>--- Receiver Side ---</p> <ol> <li>Receive packet</li> <li>Extract message</li> <li>Repeat</li> </ol> <p>[!warning] Flaws</p> <p>Keep in mind that with RDT 1.0 (which is really UDT) packets could get corrupted or lost, without none of the hosts knowing.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/RDT/#rdt-20","title":"RDT 2.0","text":"<p>The second version of RDT acknowledges that the packets could get corrupted, which is why it introduces the ACK (acknowledge) and NACK (negative/not acknowledge) packets.</p> <p>After sending a packet, the sender must wait an acknowledgement for the sent packet. If the acknowledgement is positive, then the next packet can be sent, otherwise the current packet must be retransmitted.</p> <p>The receiver, on the other hand, must check that a packet isn't corrupted when it receives it. If the packet is valid then it must send a positive acknowledgment, otherwise it must send a negative acknowledgment.</p> <p></p> <p>--- Sender Side ---</p> <ol> <li>Create packet</li> <li>Send packet</li> <li>Wait ACK</li> <li>If NACK, execute step 2 again</li> <li>Repeat</li> </ol> <p>--- Receiver Side ---</p> <ol> <li>Receive packet</li> <li>If packet corrupted, send NACK and go to step 1</li> <li>Send ACK</li> <li>Extract message</li> <li>Repeat</li> </ol> <p>[!warning] Flaws</p> <p>With RDT 2.0, the ACK and NACK packets could get corrupted or lost, and none of the hosts would know.</p> <p>For example: - An acknowledgement packet could be lost, thus blocking the communication because the sender would be forever waiting for an acknowledgement. - A NACK could turn into an ACK if corrupted, giving the ok to the sender to send the next packet, even though the receiver still didn't get the current packet correctly. - An ACK could turn into a NACK if corrupted, thus the sender would be sending a duplicate packet, which the receiver would think it being the next packet.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/RDT/#rdt-21","title":"RDT 2.1","text":"<p>The first reiteration of the second version of RDT handles the problem that ACK and NACK packets could get lost or corrupted; in particular, it handles duplicates of received packets.</p> <p></p> <p></p> <p>The sender assigns a number to each packet, alternating between 0 and 1, so that the receiver can discard duplicate packets (i.e. if more than one consecutive packets have the same number, keep the first packet only).</p> <p>[!note] Acknowledgement</p> <p>Acknowledge packets don't need to report the packet number back, because the acknowledgement packets always refer to the last sent packet.</p> <p>--- Sender Side ---</p> <ol> <li>Create packet #0</li> <li>Send packet #0</li> <li>Wait ACK</li> <li>If NACK or corrupted, execute step 2 again</li> <li>Create packet #1</li> <li>Send packet #1</li> <li>Wait ACK</li> <li>If NACK or corrupted, execute step 5 again</li> </ol> <p>--- Receiver Side ---</p> <ol> <li>Receive packet (#0)</li> <li>If packet corrupted or packet is #1, send NACK and go to step 1</li> <li>Send ACK</li> <li>Extract message</li> <li>Receive packet (#1)</li> <li>If packet corrupted or packet is #0, send NACK and go to step 5</li> <li>Send ACK</li> <li>Extract message</li> </ol>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/RDT/#rdt-22","title":"RDT 2.2","text":"<p>RDT 2.2 improves the previous version of RDT by removing NACK packets and identifying ACK packets by the number of the respective data packet.</p> <p>The only difference with RDT 2.1 is that, instead of sending a NACK packet when receiving a corrupted packet, the receiver must send an ACK packet identified by the number of the last correctly received packet (i.e. the previous). When the sender receives an out-of-order ACK packet, it must resend the packet it just sent (i.e. send packet 1, receive ACK 0, resend packet 1).</p> <p></p> <p>--- Sender Side ---</p> <ol> <li>Create packet #0</li> <li>Send packet #0</li> <li>Wait ACK</li> <li>If ACK #1 or corrupted, execute step 2 again</li> <li>Create packet #1</li> <li>Send packet #1</li> <li>Wait ACK</li> <li>If ACK #0 or corrupted, execute step 6 again</li> </ol> <p>--- Receiver Side ---</p> <ol> <li>Receive packet (#0)</li> <li>If packet corrupted or packet is #1, send ACK #1 and go to step 1</li> <li>Send ACK #0</li> <li>Extract message</li> <li>Receive packet (#1)</li> <li>If packet corrupted or packet is #0, send ACK #0 and go to step 1</li> <li>Send ACK #1</li> <li>Extract message</li> </ol> <p>[!warning] Losses</p> <p>RDT 2.2 still doesn't assume that packets could get lost, it only assumes that packets could get corrupted.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/RDT/#rdt-30","title":"RDT 3.0","text":"<p>The third version of the RDT protocol is the last and complete version, it addresses the problem that packets could get lost. Moreover, it introduces pipelining to increase throughput.</p> <p>The sender, after sending a packet, will wait for a given time for an ACK:</p> <ul> <li>if the ACK is received then the next packet can be sent;</li> <li>if the ACK isn't received then the current packet must be retransmitted.</li> </ul> <p>[!tip] Duplicate Packets</p> <p>If a packet (either data or ACK) isn't lost but just delayed, then the sender would be transmitting the same packet two times, thus duplicating it; but this isn't a problem, because RDT already handled duplicates starting from version 2.1.</p> <p></p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/RDT/#pipelining","title":"Pipelining","text":"<p>Pipelining is a technique used to increase the throughput of RDT. Instead of sending a packet and waiting for the ACK (stop and wait), the sender may send a window of packets altogether, removing all the back-and-forth created by acknowledging every packet with stop and wait.</p> <p>To identify and acknowledge them, the packets aren't marked with 0 and 1 only, but a whole range of integers is used.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/RDT/#go-back-n","title":"Go-Back-N","text":"<p>Go-Back-N creates a window of transmittable packets that can be sent at the same time. Instead of acknowledging every packet singularly, packets are acknowledged cumulatively: the receiver will send the ACK for the last packet in a sequence, meaning it received correctly all the previous packets.</p> <p>--- Sender Side ---</p> <ul> <li>Keeps a timer for the oldest non-ACK'ed packet (a.k.a. <code>send_base</code>);</li> <li>Can send all packets in the current window (from <code>send_base</code> to <code>send_base + window_size</code>);</li> <li>If an ACK for packet <code>n &gt;= send_base</code> is received, then slide the window to start at <code>n+1</code>;</li> <li>On <code>send_base</code> timeout, retransmit the whole window.</li> </ul> <p>--- Receiver Side ---</p> <ul> <li>ACK the latest packets such that there aren't any unreceived packets before;</li> <li>If an out-of-order packet is received, it could be either buffered or discarded.</li> </ul>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/RDT/#selective-repeat","title":"Selective Repeat","text":"<p>Selective Repeat creates a window of transmittable packets that can be sent independently, which the receiver has to acknowledge one-by-one.</p> <p></p> <p>--- Sender Side ---</p> <ul> <li>Keeps a timer for every non-ACK'ed packet;</li> <li>Can send all packets in the current window (from <code>send_base</code> to <code>send_base + window_size</code>);</li> <li>If an ACK for packet <code>send_base</code> is received, then slide the window to start at <code>send_base+1</code>;</li> <li>On a packet timeout, retransmit the single packet.</li> </ul> <p>--- Receiver Side ---</p> <ul> <li>ACK every packet received included in the window;</li> <li>If an out-of-order packet is received, buffer it (by the previous point it must be ACK'ed).</li> </ul>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/TCP/","title":"Transfer Control Protocol","text":"<p>The Transfer Control Protocol is a transport layer protocol based on the Reliable Data Transfer 3.0 protocol. It extends RDT by adding other features such as congestion control, flow control and connections.</p> <p>Unlike RDT, in TCP both hosts of a conversation are both the sender and the receiver of a connection. A TCP connection is opened via a three-way handshake: - \\(A\\) sends a SYN packet with a random sequence number \\(x\\); - \\(B\\) sends an ACK/SYN packet for \\(x+1\\), while setting the packet sequence number to a random \\(y\\); - \\(A\\) sends an ACK packet for \\(y+1\\) with sequence number \\(x+1\\)</p> <p>[!note] Sequence Numbers and ACK</p> <p>Because both hosts are both senders and receivers in the connection, when a receiver is replying to a packet with sequence number \\(x\\), it must send a new packet with its own sequence number \\(y\\) acknowledging packet \\(x+1\\); then, the other party has to send packet \\(x+1\\) acknowledging packet \\(y+1\\).</p> <p>Unlike RDT, hosts in a TCP connection acknowledge the sequence number of the next expected packet instead of the received packet.</p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/TCP/#tcp-segment-structure","title":"TCP Segment Structure","text":"<ol> <li>Source Port: The source port is a 16-bit field that identifies the port number of the sender's application or process.</li> <li>Destination Port: The destination port is also a 16-bit field that specifies the port number of the receiver's application or process.</li> <li>Sequence Number: This 32-bit field indicates the sequence number of the first data octet (byte) in the TCP segment. It helps in ordering and reassembling received data.</li> <li>Acknowledgment Number: In an ACK (acknowledgment) packet, the 32-bit acknowledgment number field indicates the next sequence number the sender expects to receive. It acknowledges the receipt of data up to that sequence number.</li> <li>Data Offset: The 4-bit data offset field specifies the size of the TCP header in 32-bit words. It determines the starting point of the TCP data payload.</li> <li>Flags: The TCP flags consist of several 1-bit fields that control various aspects of the TCP connection. These flags include:<ul> <li>URG (Urgent Pointer): Indicates that urgent data is present in the TCP segment.</li> <li>ACK (Acknowledgment): Indicates that the acknowledgment number field is valid.</li> <li>PSH (Push Function): Requests the receiving application to process the data immediately.</li> <li>RST (Reset Connection): Resets the TCP connection.</li> <li>SYN (Synchronize Sequence Numbers): Initiates a TCP connection setup.</li> <li>FIN (Finish): Closes the TCP connection.</li> </ul> </li> <li>Window Size: This 16-bit field specifies the number of bytes the sender is willing to receive, also known as the receive window. It helps in flow control by indicating the amount of available buffer space on the receiver's side.</li> <li>Checksum: The 16-bit checksum field is used for error detection. It ensures the integrity of the TCP segment by verifying the integrity of the header and payload.</li> <li>Urgent Pointer: If the URG flag is set, this 16-bit field points to the last byte of urgent data in the TCP segment.</li> <li>Options: The options field is optional and used to include additional parameters or functionality. Some commonly used options include Maximum Segment Size (MSS), Selective Acknowledgment (SACK), and Timestamps.</li> <li>TCP Payload: The TCP payload consists of the actual data being transmitted. It can range from zero bytes (in ACK or control packets) to a maximum determined by the maximum segment size (MSS) negotiated during connection setup.</li> </ol>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/TCP/#tcp-retransmission-timeout","title":"TCP Retransmission Timeout","text":"<p>Declaring a correct timeout is a crucial step to ensure the full potential of TCP. The timeout should be set to a value longer than the RTT, but the RTT isn't fixed: - if it is too short then there will be a lot of retransmissions of delayed packets, - if it is too long then retransmission of lost packets will take too much, slowing the whole transmission.</p> <p>A formula for a weighted estimated RTT is called Exponentially Weighted Moving Average (a.k.a. EWMA), which computes a more stable RTT, that fluctuates less w.r.t. the measured one. </p> \\[\\large     \\text{RTT}_i = (1-\\alpha) \\cdot \\text{RTT}_{i-1} + \\alpha \\cdot \\text{rtt}_i \\] <p>[!abstract] Estimated RTT (EWMA) Variables</p> <ul> <li>\\(\\text{rtt}_i\\) is the \\(i\\)-th measured RTT;</li> <li>\\(\\text{RTT}_i\\) is the \\(i\\)-th estimated RTT, assuming \\(\\text{RTT}_0 = 0\\);</li> <li>\\(\\alpha\\) is a parameter that adjusts the average, a standard value is \\(\\alpha = .125\\).</li> </ul> <p>The estimated RTT can be used to compute an adjustable timeout (a.k.a. RTO). Originally double the estimated RTT (i.e. \\(\\text{RTO}_i = 2 \\cdot \\text{RTT}_{i-1}\\)) was a recommended value, but now another formula that uses the mean deviation called Jacobson's Algorithm is recommended.</p> \\[\\large     \\text{RTO}_i = \\text{RTT}_{i-1} + 4 \\cdot \\bar\\sigma_{i-1} \\] <p>[!abstract] Jacobson's Algorithm Variables</p> <ul> <li>\\(\\text{RTO}_i\\) is the \\(i\\)-th recommended timeout;</li> <li>\\(\\bar\\sigma_i\\) is the \\(i\\)-th mean deviation.</li> </ul> <p>The mean deviation gets extremely expensive to compute as more time goes on, so a cheaper-to-compute version of the mean deviation can be computed using EWMA.</p> \\[\\large     \\bar\\sigma_i =         (1-\\rho) \\, \\bar\\sigma_{i-1}         + \\rho \\, |\\text{rtt}_i - \\text{RTT}_{i-1}| \\] <p>[!abstract] Mean Deviance (EWMA) Variables</p> <ul> <li>\\(\\rho\\) is a parameter that adjusts the average, a standard value is \\(\\rho = .25\\).</li> </ul> <p>[!note] Exponential Back-off</p> <p>It could still happen that the retransmission timeout is too short, thus the sender may retransmit duplicates many times. A solution to this problem is exponential back-off: the RTO for a packet is double upon each timeout of the packet, to give more time to delayed packets; the first RTO is still computed using Jacobson's algorithm.</p> <p>This is an initial notion of congestion control.</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/TCP/#tcp-flow-congestion-control","title":"TCP Flow-Congestion Control","text":"<p>The flow and congestion control mechanisms in TCP address different aspects of data transmission to ensure reliable and efficient communication. These mechanisms work together to prevent issues such as buffer overflow, packet loss, and network congestion.</p> <p>[!abstract] Flow and Congestion Control</p> <ul> <li>Flow control prevents receiver buffer overflow, ensuring that data is delivered and processed at a pace that the receiver can handle.</li> <li>Congestion control helps maintain network stability by regulating the sending rate to avoid overwhelming the network and causing congestion-induced performance degradation.</li> </ul> <p>Flow control operates on an end-to-end level, it resolves the issue of the sender transferring data faster than the receiver can buffer and process. The receiver, on each response, sends a feedback to the sender on how much free space it has left to buffer packets, specified in the <code>rwnd</code> header of the segment. This way, the sender knows how much can be sent and the receiver's buffer won't get overflowed.</p> <p>Congestion control, on the other hand, operates at a network level and focuses on preventing congestion within the network. It monitors the state of the network and dynamically adjusts the sending rate of TCP packets (through the congestion window <code>cwnd</code>) to avoid network congestion.</p> <p>The sender monitors the network status by looking for duplicate ACKs and timeouts, which are indicators of network congestion. If ACKs arrive steadily, then the congestion window <code>cwnd</code> can be increased in size; otherwise if duplicate ACKs or timeouts are detected, <code>cwnd</code> must be decreased in size.</p> <p>[!note] Sender's Window</p> <p>The sender's window size is set to be the minimum between the receiver's window and the congestion window. This limits the reliable transmission rate to at most the ratio the window size over the RTT; at most because packet loss or corruption could still happen.</p> <p>The first version of TCP congestion control was named Tahoe TCP. It considered three duplicate ACKs the same as a timeout, the only states it had were slow start and congestion avoidance.</p> <p></p> <p>Then, a better version of TCP congestion control called Reno TCP was developed. It considered three duplicate ACKs as a lighter indication of congestion than a timeout, it also added fast recovery as an additional state to the ones already in Tahoe TCP. </p> <p></p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/TCP/#slow-start","title":"Slow Start","text":"<p>When a TCP connection begins, show start is the first state of the connection. The congestion window <code>cwnd</code> is set to 1 MSS and it's \"doubled\" on every RTT. The window isn't actually doubled on every RTT, but in reality it is incremented by 1 MSS for every ACK received (which, without loss or corruption, is equivalent to doubling the window every RTT).</p> <p>TCP continues in a state of slow start until <code>cwnd</code> becomes greater than a set threshold called <code>sstresh</code>. If doubling <code>cwnd</code> would result in <code>cwnd</code> being greater than <code>ssthresh</code>, then <code>cwnd</code> will be set to the value of <code>ssthresh</code>. After reaching <code>ssthresh</code>, TCP will step out of slow start and get into congestion avoidance (i.e. linear increase).</p> <p>The initial state of <code>ssthresh</code> can be set to a constant, and then it is updated every time a timeout happens. Upon a timeout (or 3dupACKs in Tahoe TCP), TCP will step out of whichever state it is on and get into slow start: <code>ssthresh</code> is set to half the size <code>cwnd</code> was on the timeout, <code>cwnd</code> is set to 1 MSS and <code>cwnd</code> will be increased exponentially (i.e. double every RTT).</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/TCP/#congestion-avoidance","title":"Congestion Avoidance","text":"<p>TK</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/TCP/#fast-recovery","title":"Fast Recovery","text":"<p>TK</p>"},{"location":"Systems%20and%20Networking/Unit%202/Protocols/UDP/","title":"Uniform Datagram Protocol","text":"<p>UDP is the simplest form of transport protocol one could think of. It is almost barebone IP, it adds almost no functionality. It may seem that it is useless comparing it to TCP, but it has its own quirks:</p> <ul> <li>Finer application-level control, when an application tasks the transport layer to send a message, UDP will do it right away. TCP, on the other hand, has a congestion control system, that may slow down the message;</li> <li>No connection, so UDP can (again) send packets right away. TCP, on the other hand (again), has a three-way handshake with the goal to form a connection, which in turns slows down the message;</li> <li>Smaller header, UDP has 8 bytes of headers, versus 20 bytes of TCP headers.</li> </ul>"}]}